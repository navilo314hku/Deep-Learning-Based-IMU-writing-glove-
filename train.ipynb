{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import TensorDataset, DataLoader,Dataset\n",
    "from model_classes.convNet import ConvNet,ConvNet2\n",
    "from torchvision.models import resnet18\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#import torchvision.datasets.ImageFolder \n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "from const import *\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "batch_size=4\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5), (0.5))])\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "from torchvision.io import read_image\n",
    "\n",
    "class CustomImageDataset(Dataset):\n",
    "    def __init__(self, annotations_file, img_dir, transform=None, target_transform=None):\n",
    "        self.img_labels = pd.read_csv(annotations_file)\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        img_path = os.path.join(self.img_dir, self.img_labels.iloc[idx, 0])\n",
    "        #print(f\"image path: {img_path}\")\n",
    "        image = read_image(img_path)\n",
    "        image=image.to(torch.float32)\n",
    "        #print(f\"image type: {type(image)}\")\n",
    "        label = self.img_labels.iloc[idx, 1]\n",
    "        if self.transform:\n",
    "            #print('here')\n",
    "            image = self.transform(image)\n",
    "        if self.target_transform:\n",
    "            label = self.target_transform(label)\n",
    "        return image, label\n",
    "train_dataset= torchvision.datasets.ImageFolder(TRAIN_IMAGE_PATH,transform)\n",
    "test_dataset= torchvision.datasets.ImageFolder(TEST_IMAGE_PATH,transform)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size,shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size,shuffle=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 44, 6])\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset[0][0].size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model,num_epochs=500,learning_rate=0.00001):\n",
    "    print(f\"lr={learning_rate}\")\n",
    "    loss_arr=[]\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    n_total_steps = len(train_loader)\n",
    "    for epoch in range(num_epochs):\n",
    "        for i, (images, labels) in enumerate(train_loader):\n",
    "            # origin shape: [4, 3, 32, 32] = 4, 3, 1024\n",
    "            # input_layer: 3 input channels, 6 output channels, 5 kernel size\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Backward and optimize\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            loss_arr.append(loss)\n",
    "            if (i+1) % 200 == 0:\n",
    "                print (f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{n_total_steps}], Loss: {loss.item():.4f}')\n",
    "    #plt.scatter(np.linspace(1, num_epochs, num_epochs).astype(int),loss_arr)\n",
    "    #plt.show()\n",
    "    print('Finished Training')\n",
    "    PATH = './cnn.pth'\n",
    "    torch.save(model.state_dict(), PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr=0.0005\n",
      "Epoch [1/100], Step [200/735], Loss: 0.2542\n",
      "Epoch [1/100], Step [400/735], Loss: 0.7075\n",
      "Epoch [1/100], Step [600/735], Loss: 0.5104\n",
      "Epoch [2/100], Step [200/735], Loss: 0.3252\n",
      "Epoch [2/100], Step [400/735], Loss: 0.1685\n",
      "Epoch [2/100], Step [600/735], Loss: 0.3808\n",
      "Epoch [3/100], Step [200/735], Loss: 0.3510\n",
      "Epoch [3/100], Step [400/735], Loss: 0.3930\n",
      "Epoch [3/100], Step [600/735], Loss: 0.3185\n",
      "Epoch [4/100], Step [200/735], Loss: 0.1755\n",
      "Epoch [4/100], Step [400/735], Loss: 1.1167\n",
      "Epoch [4/100], Step [600/735], Loss: 0.6098\n",
      "Epoch [5/100], Step [200/735], Loss: 0.4252\n",
      "Epoch [5/100], Step [400/735], Loss: 0.3225\n",
      "Epoch [5/100], Step [600/735], Loss: 0.1936\n",
      "Epoch [6/100], Step [200/735], Loss: 0.0818\n",
      "Epoch [6/100], Step [400/735], Loss: 0.1881\n",
      "Epoch [6/100], Step [600/735], Loss: 0.2775\n",
      "Epoch [7/100], Step [200/735], Loss: 0.3085\n",
      "Epoch [7/100], Step [400/735], Loss: 1.0922\n",
      "Epoch [7/100], Step [600/735], Loss: 0.7534\n",
      "Epoch [8/100], Step [200/735], Loss: 0.1160\n",
      "Epoch [8/100], Step [400/735], Loss: 0.1730\n",
      "Epoch [8/100], Step [600/735], Loss: 0.9738\n",
      "Epoch [9/100], Step [200/735], Loss: 0.2723\n",
      "Epoch [9/100], Step [400/735], Loss: 1.6830\n",
      "Epoch [9/100], Step [600/735], Loss: 0.0245\n",
      "Epoch [10/100], Step [200/735], Loss: 0.6002\n",
      "Epoch [10/100], Step [400/735], Loss: 1.0026\n",
      "Epoch [10/100], Step [600/735], Loss: 0.3260\n",
      "Epoch [11/100], Step [200/735], Loss: 0.3249\n",
      "Epoch [11/100], Step [400/735], Loss: 0.1279\n",
      "Epoch [11/100], Step [600/735], Loss: 1.2311\n",
      "Epoch [12/100], Step [200/735], Loss: 0.2146\n",
      "Epoch [12/100], Step [400/735], Loss: 0.1483\n",
      "Epoch [12/100], Step [600/735], Loss: 0.2466\n",
      "Epoch [13/100], Step [200/735], Loss: 0.4337\n",
      "Epoch [13/100], Step [400/735], Loss: 0.4879\n",
      "Epoch [13/100], Step [600/735], Loss: 0.0827\n",
      "Epoch [14/100], Step [200/735], Loss: 0.6451\n",
      "Epoch [14/100], Step [400/735], Loss: 0.1508\n",
      "Epoch [14/100], Step [600/735], Loss: 0.4242\n",
      "Epoch [15/100], Step [200/735], Loss: 0.4611\n",
      "Epoch [15/100], Step [400/735], Loss: 0.5865\n",
      "Epoch [15/100], Step [600/735], Loss: 1.1205\n",
      "Epoch [16/100], Step [200/735], Loss: 0.3126\n",
      "Epoch [16/100], Step [400/735], Loss: 0.5750\n",
      "Epoch [16/100], Step [600/735], Loss: 1.1376\n",
      "Epoch [17/100], Step [200/735], Loss: 0.3074\n",
      "Epoch [17/100], Step [400/735], Loss: 0.1540\n",
      "Epoch [17/100], Step [600/735], Loss: 0.5707\n",
      "Epoch [18/100], Step [200/735], Loss: 1.1788\n",
      "Epoch [18/100], Step [400/735], Loss: 0.1259\n",
      "Epoch [18/100], Step [600/735], Loss: 0.4310\n",
      "Epoch [19/100], Step [200/735], Loss: 0.1417\n",
      "Epoch [19/100], Step [400/735], Loss: 0.3062\n",
      "Epoch [19/100], Step [600/735], Loss: 0.1648\n",
      "Epoch [20/100], Step [200/735], Loss: 0.2414\n",
      "Epoch [20/100], Step [400/735], Loss: 0.5955\n",
      "Epoch [20/100], Step [600/735], Loss: 0.3165\n",
      "Epoch [21/100], Step [200/735], Loss: 0.5195\n",
      "Epoch [21/100], Step [400/735], Loss: 1.1586\n",
      "Epoch [21/100], Step [600/735], Loss: 0.0784\n",
      "Epoch [22/100], Step [200/735], Loss: 0.4851\n",
      "Epoch [22/100], Step [400/735], Loss: 0.2321\n",
      "Epoch [22/100], Step [600/735], Loss: 0.1052\n",
      "Epoch [23/100], Step [200/735], Loss: 0.4156\n",
      "Epoch [23/100], Step [400/735], Loss: 0.1077\n",
      "Epoch [23/100], Step [600/735], Loss: 0.4152\n",
      "Epoch [24/100], Step [200/735], Loss: 0.7127\n",
      "Epoch [24/100], Step [400/735], Loss: 0.6440\n",
      "Epoch [24/100], Step [600/735], Loss: 0.4339\n",
      "Epoch [25/100], Step [200/735], Loss: 1.2418\n",
      "Epoch [25/100], Step [400/735], Loss: 0.2177\n",
      "Epoch [25/100], Step [600/735], Loss: 0.3275\n",
      "Epoch [26/100], Step [200/735], Loss: 1.2526\n",
      "Epoch [26/100], Step [400/735], Loss: 0.6244\n",
      "Epoch [26/100], Step [600/735], Loss: 0.2364\n",
      "Epoch [27/100], Step [200/735], Loss: 0.3355\n",
      "Epoch [27/100], Step [400/735], Loss: 0.1685\n",
      "Epoch [27/100], Step [600/735], Loss: 0.8734\n",
      "Epoch [28/100], Step [200/735], Loss: 0.5223\n",
      "Epoch [28/100], Step [400/735], Loss: 0.2654\n",
      "Epoch [28/100], Step [600/735], Loss: 0.6260\n",
      "Epoch [29/100], Step [200/735], Loss: 0.2191\n",
      "Epoch [29/100], Step [400/735], Loss: 0.0431\n",
      "Epoch [29/100], Step [600/735], Loss: 0.1039\n",
      "Epoch [30/100], Step [200/735], Loss: 0.2789\n",
      "Epoch [30/100], Step [400/735], Loss: 0.6502\n",
      "Epoch [30/100], Step [600/735], Loss: 0.2606\n",
      "Epoch [31/100], Step [200/735], Loss: 0.6527\n",
      "Epoch [31/100], Step [400/735], Loss: 0.5532\n",
      "Epoch [31/100], Step [600/735], Loss: 0.5918\n",
      "Epoch [32/100], Step [200/735], Loss: 0.3392\n",
      "Epoch [32/100], Step [400/735], Loss: 0.2704\n",
      "Epoch [32/100], Step [600/735], Loss: 1.4529\n",
      "Epoch [33/100], Step [200/735], Loss: 0.3044\n",
      "Epoch [33/100], Step [400/735], Loss: 0.1893\n",
      "Epoch [33/100], Step [600/735], Loss: 0.9003\n",
      "Epoch [34/100], Step [200/735], Loss: 0.9581\n",
      "Epoch [34/100], Step [400/735], Loss: 0.5000\n",
      "Epoch [34/100], Step [600/735], Loss: 0.2709\n",
      "Epoch [35/100], Step [200/735], Loss: 0.5890\n",
      "Epoch [35/100], Step [400/735], Loss: 0.6153\n",
      "Epoch [35/100], Step [600/735], Loss: 0.0914\n",
      "Epoch [36/100], Step [200/735], Loss: 0.2767\n",
      "Epoch [36/100], Step [400/735], Loss: 0.7991\n",
      "Epoch [36/100], Step [600/735], Loss: 0.8873\n",
      "Epoch [37/100], Step [200/735], Loss: 0.3986\n",
      "Epoch [37/100], Step [400/735], Loss: 0.1862\n",
      "Epoch [37/100], Step [600/735], Loss: 0.6943\n",
      "Epoch [38/100], Step [200/735], Loss: 0.5337\n",
      "Epoch [38/100], Step [400/735], Loss: 0.2408\n",
      "Epoch [38/100], Step [600/735], Loss: 0.4017\n",
      "Epoch [39/100], Step [200/735], Loss: 0.3063\n",
      "Epoch [39/100], Step [400/735], Loss: 1.5783\n",
      "Epoch [39/100], Step [600/735], Loss: 1.0331\n",
      "Epoch [40/100], Step [200/735], Loss: 0.4334\n",
      "Epoch [40/100], Step [400/735], Loss: 0.5108\n",
      "Epoch [40/100], Step [600/735], Loss: 0.2677\n",
      "Epoch [41/100], Step [200/735], Loss: 0.7087\n",
      "Epoch [41/100], Step [400/735], Loss: 0.0761\n",
      "Epoch [41/100], Step [600/735], Loss: 0.4810\n",
      "Epoch [42/100], Step [200/735], Loss: 0.5159\n",
      "Epoch [42/100], Step [400/735], Loss: 0.1061\n",
      "Epoch [42/100], Step [600/735], Loss: 0.4702\n",
      "Epoch [43/100], Step [200/735], Loss: 0.0136\n",
      "Epoch [43/100], Step [400/735], Loss: 0.1572\n",
      "Epoch [43/100], Step [600/735], Loss: 0.3216\n",
      "Epoch [44/100], Step [200/735], Loss: 0.7713\n",
      "Epoch [44/100], Step [400/735], Loss: 0.6703\n",
      "Epoch [44/100], Step [600/735], Loss: 0.0960\n",
      "Epoch [45/100], Step [200/735], Loss: 0.1784\n",
      "Epoch [45/100], Step [400/735], Loss: 0.6510\n",
      "Epoch [45/100], Step [600/735], Loss: 0.4633\n",
      "Epoch [46/100], Step [200/735], Loss: 0.6438\n",
      "Epoch [46/100], Step [400/735], Loss: 0.4624\n",
      "Epoch [46/100], Step [600/735], Loss: 0.2191\n",
      "Epoch [47/100], Step [200/735], Loss: 0.1379\n",
      "Epoch [47/100], Step [400/735], Loss: 0.4212\n",
      "Epoch [47/100], Step [600/735], Loss: 0.3770\n",
      "Epoch [48/100], Step [200/735], Loss: 0.2265\n",
      "Epoch [48/100], Step [400/735], Loss: 0.3662\n",
      "Epoch [48/100], Step [600/735], Loss: 0.0544\n",
      "Epoch [49/100], Step [200/735], Loss: 0.0199\n",
      "Epoch [49/100], Step [400/735], Loss: 1.4597\n",
      "Epoch [49/100], Step [600/735], Loss: 0.4869\n",
      "Epoch [50/100], Step [200/735], Loss: 0.2921\n",
      "Epoch [50/100], Step [400/735], Loss: 0.5722\n",
      "Epoch [50/100], Step [600/735], Loss: 0.4231\n",
      "Epoch [51/100], Step [200/735], Loss: 0.1456\n",
      "Epoch [51/100], Step [400/735], Loss: 0.2930\n",
      "Epoch [51/100], Step [600/735], Loss: 0.3630\n",
      "Epoch [52/100], Step [200/735], Loss: 0.2307\n",
      "Epoch [52/100], Step [400/735], Loss: 0.4543\n",
      "Epoch [52/100], Step [600/735], Loss: 1.0348\n",
      "Epoch [53/100], Step [200/735], Loss: 0.4841\n",
      "Epoch [53/100], Step [400/735], Loss: 0.3832\n",
      "Epoch [53/100], Step [600/735], Loss: 0.1715\n",
      "Epoch [54/100], Step [200/735], Loss: 0.5041\n",
      "Epoch [54/100], Step [400/735], Loss: 0.6117\n",
      "Epoch [54/100], Step [600/735], Loss: 0.0748\n",
      "Epoch [55/100], Step [200/735], Loss: 0.1756\n",
      "Epoch [55/100], Step [400/735], Loss: 0.5395\n",
      "Epoch [55/100], Step [600/735], Loss: 0.3793\n",
      "Epoch [56/100], Step [200/735], Loss: 0.1993\n",
      "Epoch [56/100], Step [400/735], Loss: 0.1651\n",
      "Epoch [56/100], Step [600/735], Loss: 0.5433\n",
      "Epoch [57/100], Step [200/735], Loss: 0.7349\n",
      "Epoch [57/100], Step [400/735], Loss: 0.2078\n",
      "Epoch [57/100], Step [600/735], Loss: 0.4332\n",
      "Epoch [58/100], Step [200/735], Loss: 0.2763\n",
      "Epoch [58/100], Step [400/735], Loss: 0.2982\n",
      "Epoch [58/100], Step [600/735], Loss: 0.1520\n",
      "Epoch [59/100], Step [200/735], Loss: 0.0694\n",
      "Epoch [59/100], Step [400/735], Loss: 0.6747\n",
      "Epoch [59/100], Step [600/735], Loss: 0.1637\n",
      "Epoch [60/100], Step [200/735], Loss: 0.5447\n",
      "Epoch [60/100], Step [400/735], Loss: 0.3370\n",
      "Epoch [60/100], Step [600/735], Loss: 0.3505\n",
      "Epoch [61/100], Step [200/735], Loss: 0.1930\n",
      "Epoch [61/100], Step [400/735], Loss: 0.1432\n",
      "Epoch [61/100], Step [600/735], Loss: 0.4103\n",
      "Epoch [62/100], Step [200/735], Loss: 0.2865\n",
      "Epoch [62/100], Step [400/735], Loss: 0.2141\n",
      "Epoch [62/100], Step [600/735], Loss: 0.4508\n",
      "Epoch [63/100], Step [200/735], Loss: 0.2504\n",
      "Epoch [63/100], Step [400/735], Loss: 0.6236\n",
      "Epoch [63/100], Step [600/735], Loss: 0.1167\n",
      "Epoch [64/100], Step [200/735], Loss: 0.0679\n",
      "Epoch [64/100], Step [400/735], Loss: 1.2041\n",
      "Epoch [64/100], Step [600/735], Loss: 0.5999\n",
      "Epoch [65/100], Step [200/735], Loss: 0.0572\n",
      "Epoch [65/100], Step [400/735], Loss: 0.6920\n",
      "Epoch [65/100], Step [600/735], Loss: 0.1654\n",
      "Epoch [66/100], Step [200/735], Loss: 0.2012\n",
      "Epoch [66/100], Step [400/735], Loss: 0.3370\n",
      "Epoch [66/100], Step [600/735], Loss: 0.4032\n",
      "Epoch [67/100], Step [200/735], Loss: 0.0965\n",
      "Epoch [67/100], Step [400/735], Loss: 0.2039\n",
      "Epoch [67/100], Step [600/735], Loss: 0.2744\n",
      "Epoch [68/100], Step [200/735], Loss: 0.0806\n",
      "Epoch [68/100], Step [400/735], Loss: 0.3017\n",
      "Epoch [68/100], Step [600/735], Loss: 0.4126\n",
      "Epoch [69/100], Step [200/735], Loss: 0.0877\n",
      "Epoch [69/100], Step [400/735], Loss: 0.2733\n",
      "Epoch [69/100], Step [600/735], Loss: 0.1552\n",
      "Epoch [70/100], Step [200/735], Loss: 0.0756\n",
      "Epoch [70/100], Step [400/735], Loss: 0.1569\n",
      "Epoch [70/100], Step [600/735], Loss: 0.2676\n",
      "Epoch [71/100], Step [200/735], Loss: 0.4442\n",
      "Epoch [71/100], Step [400/735], Loss: 0.2511\n",
      "Epoch [71/100], Step [600/735], Loss: 0.1654\n",
      "Epoch [72/100], Step [200/735], Loss: 0.2650\n",
      "Epoch [72/100], Step [400/735], Loss: 0.4436\n",
      "Epoch [72/100], Step [600/735], Loss: 0.2311\n",
      "Epoch [73/100], Step [200/735], Loss: 0.3184\n",
      "Epoch [73/100], Step [400/735], Loss: 0.4073\n",
      "Epoch [73/100], Step [600/735], Loss: 0.2133\n",
      "Epoch [74/100], Step [200/735], Loss: 0.1234\n",
      "Epoch [74/100], Step [400/735], Loss: 0.3086\n",
      "Epoch [74/100], Step [600/735], Loss: 0.2992\n",
      "Epoch [75/100], Step [200/735], Loss: 0.0822\n",
      "Epoch [75/100], Step [400/735], Loss: 0.6207\n",
      "Epoch [75/100], Step [600/735], Loss: 0.4156\n",
      "Epoch [76/100], Step [200/735], Loss: 0.3044\n",
      "Epoch [76/100], Step [400/735], Loss: 0.0833\n",
      "Epoch [76/100], Step [600/735], Loss: 0.3311\n",
      "Epoch [77/100], Step [200/735], Loss: 0.1539\n",
      "Epoch [77/100], Step [400/735], Loss: 0.0604\n",
      "Epoch [77/100], Step [600/735], Loss: 0.1663\n",
      "Epoch [78/100], Step [200/735], Loss: 0.7854\n",
      "Epoch [78/100], Step [400/735], Loss: 0.8501\n",
      "Epoch [78/100], Step [600/735], Loss: 0.4515\n",
      "Epoch [79/100], Step [200/735], Loss: 1.4742\n",
      "Epoch [79/100], Step [400/735], Loss: 0.6535\n",
      "Epoch [79/100], Step [600/735], Loss: 0.4787\n",
      "Epoch [80/100], Step [200/735], Loss: 0.2219\n",
      "Epoch [80/100], Step [400/735], Loss: 0.2768\n",
      "Epoch [80/100], Step [600/735], Loss: 0.0321\n",
      "Epoch [81/100], Step [200/735], Loss: 0.2386\n",
      "Epoch [81/100], Step [400/735], Loss: 0.4030\n",
      "Epoch [81/100], Step [600/735], Loss: 0.1985\n",
      "Epoch [82/100], Step [200/735], Loss: 0.3604\n",
      "Epoch [82/100], Step [400/735], Loss: 0.2753\n",
      "Epoch [82/100], Step [600/735], Loss: 0.0212\n",
      "Epoch [83/100], Step [200/735], Loss: 0.1417\n",
      "Epoch [83/100], Step [400/735], Loss: 0.1285\n",
      "Epoch [83/100], Step [600/735], Loss: 0.4814\n",
      "Epoch [84/100], Step [200/735], Loss: 0.1854\n",
      "Epoch [84/100], Step [400/735], Loss: 0.0949\n",
      "Epoch [84/100], Step [600/735], Loss: 0.2983\n",
      "Epoch [85/100], Step [200/735], Loss: 0.1793\n",
      "Epoch [85/100], Step [400/735], Loss: 0.0391\n",
      "Epoch [85/100], Step [600/735], Loss: 0.7487\n",
      "Epoch [86/100], Step [200/735], Loss: 0.0771\n",
      "Epoch [86/100], Step [400/735], Loss: 0.4749\n",
      "Epoch [86/100], Step [600/735], Loss: 0.0713\n",
      "Epoch [87/100], Step [200/735], Loss: 0.2257\n",
      "Epoch [87/100], Step [400/735], Loss: 0.0950\n",
      "Epoch [87/100], Step [600/735], Loss: 0.1741\n",
      "Epoch [88/100], Step [200/735], Loss: 0.1332\n",
      "Epoch [88/100], Step [400/735], Loss: 0.6330\n",
      "Epoch [88/100], Step [600/735], Loss: 0.2025\n",
      "Epoch [89/100], Step [200/735], Loss: 0.4740\n",
      "Epoch [89/100], Step [400/735], Loss: 0.3498\n",
      "Epoch [89/100], Step [600/735], Loss: 0.1664\n",
      "Epoch [90/100], Step [200/735], Loss: 0.5400\n",
      "Epoch [90/100], Step [400/735], Loss: 0.1021\n",
      "Epoch [90/100], Step [600/735], Loss: 0.1291\n",
      "Epoch [91/100], Step [200/735], Loss: 0.6278\n",
      "Epoch [91/100], Step [400/735], Loss: 0.2046\n",
      "Epoch [91/100], Step [600/735], Loss: 0.2100\n",
      "Epoch [92/100], Step [200/735], Loss: 0.5032\n",
      "Epoch [92/100], Step [400/735], Loss: 0.0727\n",
      "Epoch [92/100], Step [600/735], Loss: 0.1323\n",
      "Epoch [93/100], Step [200/735], Loss: 0.4919\n",
      "Epoch [93/100], Step [400/735], Loss: 0.4001\n",
      "Epoch [93/100], Step [600/735], Loss: 0.3200\n",
      "Epoch [94/100], Step [200/735], Loss: 0.2699\n",
      "Epoch [94/100], Step [400/735], Loss: 0.5953\n",
      "Epoch [94/100], Step [600/735], Loss: 0.0152\n",
      "Epoch [95/100], Step [200/735], Loss: 0.0771\n",
      "Epoch [95/100], Step [400/735], Loss: 0.1869\n",
      "Epoch [95/100], Step [600/735], Loss: 1.3439\n",
      "Epoch [96/100], Step [200/735], Loss: 0.0552\n",
      "Epoch [96/100], Step [400/735], Loss: 0.1129\n",
      "Epoch [96/100], Step [600/735], Loss: 0.7301\n",
      "Epoch [97/100], Step [200/735], Loss: 1.3102\n",
      "Epoch [97/100], Step [400/735], Loss: 0.7887\n",
      "Epoch [97/100], Step [600/735], Loss: 0.0553\n",
      "Epoch [98/100], Step [200/735], Loss: 0.8923\n",
      "Epoch [98/100], Step [400/735], Loss: 0.0266\n",
      "Epoch [98/100], Step [600/735], Loss: 0.2024\n",
      "Epoch [99/100], Step [200/735], Loss: 0.0923\n",
      "Epoch [99/100], Step [400/735], Loss: 0.0377\n",
      "Epoch [99/100], Step [600/735], Loss: 0.0173\n",
      "Epoch [100/100], Step [200/735], Loss: 0.6008\n",
      "Epoch [100/100], Step [400/735], Loss: 0.1049\n",
      "Epoch [100/100], Step [600/735], Loss: 0.2302\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "#Load model and train\n",
    "output_size=10\n",
    "model = ConvNet2(output_size=10)\n",
    "MODEL_PATH=os.path.join(\"trained_models\",\"1_1_convNet2_ep=250_lr_0001.pth\")\n",
    "model.load_state_dict(torch.load(MODEL_PATH,map_location=torch.device(device)))\n",
    "model.eval()\n",
    "train(model,num_epochs=100,learning_rate=0.0005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n",
      "lr=1e-05\n",
      "Epoch [1/500], Step [200/379], Loss: 2.2380\n",
      "Epoch [2/500], Step [200/379], Loss: 2.3844\n",
      "Epoch [3/500], Step [200/379], Loss: 2.2777\n",
      "Epoch [4/500], Step [200/379], Loss: 2.2281\n",
      "Epoch [5/500], Step [200/379], Loss: 2.3245\n",
      "Epoch [6/500], Step [200/379], Loss: 2.3272\n",
      "Epoch [7/500], Step [200/379], Loss: 2.3980\n",
      "Epoch [8/500], Step [200/379], Loss: 2.3534\n",
      "Epoch [9/500], Step [200/379], Loss: 2.2705\n",
      "Epoch [10/500], Step [200/379], Loss: 2.2296\n",
      "Epoch [11/500], Step [200/379], Loss: 2.3219\n",
      "Epoch [12/500], Step [200/379], Loss: 2.2391\n",
      "Epoch [13/500], Step [200/379], Loss: 2.3362\n",
      "Epoch [14/500], Step [200/379], Loss: 2.2863\n",
      "Epoch [15/500], Step [200/379], Loss: 2.3198\n",
      "Epoch [16/500], Step [200/379], Loss: 2.2417\n",
      "Epoch [17/500], Step [200/379], Loss: 2.3686\n",
      "Epoch [18/500], Step [200/379], Loss: 2.2492\n",
      "Epoch [19/500], Step [200/379], Loss: 2.2909\n",
      "Epoch [20/500], Step [200/379], Loss: 2.3781\n",
      "Epoch [21/500], Step [200/379], Loss: 2.2828\n",
      "Epoch [22/500], Step [200/379], Loss: 2.3307\n",
      "Epoch [23/500], Step [200/379], Loss: 2.3451\n",
      "Epoch [24/500], Step [200/379], Loss: 2.3044\n",
      "Epoch [25/500], Step [200/379], Loss: 2.3094\n",
      "Epoch [26/500], Step [200/379], Loss: 2.3344\n",
      "Epoch [27/500], Step [200/379], Loss: 2.3474\n",
      "Epoch [28/500], Step [200/379], Loss: 2.3069\n",
      "Epoch [29/500], Step [200/379], Loss: 2.2948\n",
      "Epoch [30/500], Step [200/379], Loss: 2.3088\n",
      "Epoch [31/500], Step [200/379], Loss: 2.2777\n",
      "Epoch [32/500], Step [200/379], Loss: 2.3402\n",
      "Epoch [33/500], Step [200/379], Loss: 2.3669\n",
      "Epoch [34/500], Step [200/379], Loss: 2.2222\n",
      "Epoch [35/500], Step [200/379], Loss: 2.2821\n",
      "Epoch [36/500], Step [200/379], Loss: 2.2927\n",
      "Epoch [37/500], Step [200/379], Loss: 2.2787\n",
      "Epoch [38/500], Step [200/379], Loss: 2.2123\n",
      "Epoch [39/500], Step [200/379], Loss: 2.2809\n",
      "Epoch [40/500], Step [200/379], Loss: 2.2285\n",
      "Epoch [41/500], Step [200/379], Loss: 2.2693\n",
      "Epoch [42/500], Step [200/379], Loss: 2.2462\n",
      "Epoch [43/500], Step [200/379], Loss: 2.3143\n",
      "Epoch [44/500], Step [200/379], Loss: 2.3179\n",
      "Epoch [45/500], Step [200/379], Loss: 2.2789\n",
      "Epoch [46/500], Step [200/379], Loss: 2.2873\n",
      "Epoch [47/500], Step [200/379], Loss: 2.2428\n",
      "Epoch [48/500], Step [200/379], Loss: 2.2690\n",
      "Epoch [49/500], Step [200/379], Loss: 2.3146\n",
      "Epoch [50/500], Step [200/379], Loss: 2.2343\n",
      "Epoch [51/500], Step [200/379], Loss: 2.2782\n",
      "Epoch [52/500], Step [200/379], Loss: 2.3883\n",
      "Epoch [53/500], Step [200/379], Loss: 2.2422\n",
      "Epoch [54/500], Step [200/379], Loss: 2.3632\n",
      "Epoch [55/500], Step [200/379], Loss: 2.2585\n",
      "Epoch [56/500], Step [200/379], Loss: 2.2558\n",
      "Epoch [57/500], Step [200/379], Loss: 2.3499\n",
      "Epoch [58/500], Step [200/379], Loss: 2.3728\n",
      "Epoch [59/500], Step [200/379], Loss: 2.2884\n",
      "Epoch [60/500], Step [200/379], Loss: 2.3235\n",
      "Epoch [61/500], Step [200/379], Loss: 2.3229\n",
      "Epoch [62/500], Step [200/379], Loss: 2.3225\n",
      "Epoch [63/500], Step [200/379], Loss: 2.3227\n",
      "Epoch [64/500], Step [200/379], Loss: 2.2612\n",
      "Epoch [65/500], Step [200/379], Loss: 2.3121\n",
      "Epoch [66/500], Step [200/379], Loss: 2.2754\n",
      "Epoch [67/500], Step [200/379], Loss: 2.2708\n",
      "Epoch [68/500], Step [200/379], Loss: 2.2712\n",
      "Epoch [69/500], Step [200/379], Loss: 2.3164\n",
      "Epoch [70/500], Step [200/379], Loss: 2.2941\n",
      "Epoch [71/500], Step [200/379], Loss: 2.2702\n",
      "Epoch [72/500], Step [200/379], Loss: 2.3811\n",
      "Epoch [73/500], Step [200/379], Loss: 2.2713\n",
      "Epoch [74/500], Step [200/379], Loss: 2.2614\n",
      "Epoch [75/500], Step [200/379], Loss: 2.2655\n",
      "Epoch [76/500], Step [200/379], Loss: 2.3032\n",
      "Epoch [77/500], Step [200/379], Loss: 2.3545\n",
      "Epoch [78/500], Step [200/379], Loss: 2.3033\n",
      "Epoch [79/500], Step [200/379], Loss: 2.2137\n",
      "Epoch [80/500], Step [200/379], Loss: 2.3150\n",
      "Epoch [81/500], Step [200/379], Loss: 2.2926\n",
      "Epoch [82/500], Step [200/379], Loss: 2.2926\n",
      "Epoch [83/500], Step [200/379], Loss: 2.2919\n",
      "Epoch [84/500], Step [200/379], Loss: 2.3797\n",
      "Epoch [85/500], Step [200/379], Loss: 2.1683\n",
      "Epoch [86/500], Step [200/379], Loss: 2.2987\n",
      "Epoch [87/500], Step [200/379], Loss: 2.3018\n",
      "Epoch [88/500], Step [200/379], Loss: 2.3070\n",
      "Epoch [89/500], Step [200/379], Loss: 2.2692\n",
      "Epoch [90/500], Step [200/379], Loss: 2.2580\n",
      "Epoch [91/500], Step [200/379], Loss: 2.3108\n",
      "Epoch [92/500], Step [200/379], Loss: 2.2786\n",
      "Epoch [93/500], Step [200/379], Loss: 2.2485\n",
      "Epoch [94/500], Step [200/379], Loss: 2.2300\n",
      "Epoch [95/500], Step [200/379], Loss: 2.2393\n",
      "Epoch [96/500], Step [200/379], Loss: 2.2181\n",
      "Epoch [97/500], Step [200/379], Loss: 2.3004\n",
      "Epoch [98/500], Step [200/379], Loss: 2.3006\n",
      "Epoch [99/500], Step [200/379], Loss: 2.2271\n",
      "Epoch [100/500], Step [200/379], Loss: 2.2489\n",
      "Epoch [101/500], Step [200/379], Loss: 2.2954\n",
      "Epoch [102/500], Step [200/379], Loss: 2.2504\n",
      "Epoch [103/500], Step [200/379], Loss: 2.2669\n",
      "Epoch [104/500], Step [200/379], Loss: 2.3195\n",
      "Epoch [105/500], Step [200/379], Loss: 2.1871\n",
      "Epoch [106/500], Step [200/379], Loss: 2.3900\n",
      "Epoch [107/500], Step [200/379], Loss: 2.2304\n",
      "Epoch [108/500], Step [200/379], Loss: 2.2945\n",
      "Epoch [109/500], Step [200/379], Loss: 2.2392\n",
      "Epoch [110/500], Step [200/379], Loss: 2.2282\n",
      "Epoch [111/500], Step [200/379], Loss: 2.3519\n",
      "Epoch [112/500], Step [200/379], Loss: 2.3050\n",
      "Epoch [113/500], Step [200/379], Loss: 2.2971\n",
      "Epoch [114/500], Step [200/379], Loss: 2.2090\n",
      "Epoch [115/500], Step [200/379], Loss: 2.3666\n",
      "Epoch [116/500], Step [200/379], Loss: 2.2675\n",
      "Epoch [117/500], Step [200/379], Loss: 2.2651\n",
      "Epoch [118/500], Step [200/379], Loss: 2.2144\n",
      "Epoch [119/500], Step [200/379], Loss: 2.2086\n",
      "Epoch [120/500], Step [200/379], Loss: 2.3221\n",
      "Epoch [121/500], Step [200/379], Loss: 2.2939\n",
      "Epoch [122/500], Step [200/379], Loss: 2.2361\n",
      "Epoch [123/500], Step [200/379], Loss: 2.2220\n",
      "Epoch [124/500], Step [200/379], Loss: 2.2460\n",
      "Epoch [125/500], Step [200/379], Loss: 2.2924\n",
      "Epoch [126/500], Step [200/379], Loss: 2.2890\n",
      "Epoch [127/500], Step [200/379], Loss: 2.2598\n",
      "Epoch [128/500], Step [200/379], Loss: 2.2304\n",
      "Epoch [129/500], Step [200/379], Loss: 2.1729\n",
      "Epoch [130/500], Step [200/379], Loss: 2.2412\n",
      "Epoch [131/500], Step [200/379], Loss: 2.3332\n",
      "Epoch [132/500], Step [200/379], Loss: 2.2666\n",
      "Epoch [133/500], Step [200/379], Loss: 2.3494\n",
      "Epoch [134/500], Step [200/379], Loss: 2.2400\n",
      "Epoch [135/500], Step [200/379], Loss: 2.1610\n",
      "Epoch [136/500], Step [200/379], Loss: 2.2151\n",
      "Epoch [137/500], Step [200/379], Loss: 2.3207\n",
      "Epoch [138/500], Step [200/379], Loss: 2.2407\n",
      "Epoch [139/500], Step [200/379], Loss: 2.2404\n",
      "Epoch [140/500], Step [200/379], Loss: 2.3113\n",
      "Epoch [141/500], Step [200/379], Loss: 2.4012\n",
      "Epoch [142/500], Step [200/379], Loss: 2.2893\n",
      "Epoch [143/500], Step [200/379], Loss: 2.2888\n",
      "Epoch [144/500], Step [200/379], Loss: 2.3325\n",
      "Epoch [145/500], Step [200/379], Loss: 2.3173\n",
      "Epoch [146/500], Step [200/379], Loss: 2.2876\n",
      "Epoch [147/500], Step [200/379], Loss: 2.2714\n",
      "Epoch [148/500], Step [200/379], Loss: 2.2368\n",
      "Epoch [149/500], Step [200/379], Loss: 2.2545\n",
      "Epoch [150/500], Step [200/379], Loss: 2.2710\n",
      "Epoch [151/500], Step [200/379], Loss: 2.2411\n",
      "Epoch [152/500], Step [200/379], Loss: 2.1583\n",
      "Epoch [153/500], Step [200/379], Loss: 2.2125\n",
      "Epoch [154/500], Step [200/379], Loss: 2.2409\n",
      "Epoch [155/500], Step [200/379], Loss: 2.3769\n",
      "Epoch [156/500], Step [200/379], Loss: 2.2963\n",
      "Epoch [157/500], Step [200/379], Loss: 2.3473\n",
      "Epoch [158/500], Step [200/379], Loss: 2.3181\n",
      "Epoch [159/500], Step [200/379], Loss: 2.2325\n",
      "Epoch [160/500], Step [200/379], Loss: 2.2614\n",
      "Epoch [161/500], Step [200/379], Loss: 2.3130\n",
      "Epoch [162/500], Step [200/379], Loss: 2.2143\n",
      "Epoch [163/500], Step [200/379], Loss: 2.3253\n",
      "Epoch [164/500], Step [200/379], Loss: 2.2505\n",
      "Epoch [165/500], Step [200/379], Loss: 2.3128\n",
      "Epoch [166/500], Step [200/379], Loss: 2.2821\n",
      "Epoch [167/500], Step [200/379], Loss: 2.3626\n",
      "Epoch [168/500], Step [200/379], Loss: 2.2973\n",
      "Epoch [169/500], Step [200/379], Loss: 2.3755\n",
      "Epoch [170/500], Step [200/379], Loss: 2.3245\n",
      "Epoch [171/500], Step [200/379], Loss: 2.3344\n",
      "Epoch [172/500], Step [200/379], Loss: 2.2697\n",
      "Epoch [173/500], Step [200/379], Loss: 2.3323\n",
      "Epoch [174/500], Step [200/379], Loss: 2.2906\n",
      "Epoch [175/500], Step [200/379], Loss: 2.2420\n",
      "Epoch [176/500], Step [200/379], Loss: 2.1422\n",
      "Epoch [177/500], Step [200/379], Loss: 2.2597\n",
      "Epoch [178/500], Step [200/379], Loss: 2.2572\n",
      "Epoch [179/500], Step [200/379], Loss: 2.2282\n",
      "Epoch [180/500], Step [200/379], Loss: 2.2894\n",
      "Epoch [181/500], Step [200/379], Loss: 2.2516\n",
      "Epoch [182/500], Step [200/379], Loss: 2.2045\n",
      "Epoch [183/500], Step [200/379], Loss: 2.1865\n",
      "Epoch [184/500], Step [200/379], Loss: 2.2037\n",
      "Epoch [185/500], Step [200/379], Loss: 2.2282\n",
      "Epoch [186/500], Step [200/379], Loss: 2.2636\n",
      "Epoch [187/500], Step [200/379], Loss: 2.2392\n",
      "Epoch [188/500], Step [200/379], Loss: 2.3112\n",
      "Epoch [189/500], Step [200/379], Loss: 2.2518\n",
      "Epoch [190/500], Step [200/379], Loss: 2.1827\n",
      "Epoch [191/500], Step [200/379], Loss: 2.2917\n",
      "Epoch [192/500], Step [200/379], Loss: 2.2666\n",
      "Epoch [193/500], Step [200/379], Loss: 2.3270\n",
      "Epoch [194/500], Step [200/379], Loss: 2.2947\n",
      "Epoch [195/500], Step [200/379], Loss: 2.1949\n",
      "Epoch [196/500], Step [200/379], Loss: 2.2179\n",
      "Epoch [197/500], Step [200/379], Loss: 2.3234\n",
      "Epoch [198/500], Step [200/379], Loss: 2.2596\n",
      "Epoch [199/500], Step [200/379], Loss: 2.2566\n",
      "Epoch [200/500], Step [200/379], Loss: 2.2823\n",
      "Epoch [201/500], Step [200/379], Loss: 2.3171\n",
      "Epoch [202/500], Step [200/379], Loss: 2.3036\n",
      "Epoch [203/500], Step [200/379], Loss: 2.2773\n",
      "Epoch [204/500], Step [200/379], Loss: 2.3608\n",
      "Epoch [205/500], Step [200/379], Loss: 2.2757\n",
      "Epoch [206/500], Step [200/379], Loss: 2.2191\n",
      "Epoch [207/500], Step [200/379], Loss: 2.2930\n",
      "Epoch [208/500], Step [200/379], Loss: 2.3157\n",
      "Epoch [209/500], Step [200/379], Loss: 2.3325\n",
      "Epoch [210/500], Step [200/379], Loss: 2.1080\n",
      "Epoch [211/500], Step [200/379], Loss: 2.2432\n",
      "Epoch [212/500], Step [200/379], Loss: 2.2676\n",
      "Epoch [213/500], Step [200/379], Loss: 2.1343\n",
      "Epoch [214/500], Step [200/379], Loss: 2.2593\n",
      "Epoch [215/500], Step [200/379], Loss: 2.2372\n",
      "Epoch [216/500], Step [200/379], Loss: 2.2933\n",
      "Epoch [217/500], Step [200/379], Loss: 2.3528\n",
      "Epoch [218/500], Step [200/379], Loss: 2.2855\n",
      "Epoch [219/500], Step [200/379], Loss: 2.2061\n",
      "Epoch [220/500], Step [200/379], Loss: 2.2944\n",
      "Epoch [221/500], Step [200/379], Loss: 2.3481\n",
      "Epoch [222/500], Step [200/379], Loss: 2.1998\n",
      "Epoch [223/500], Step [200/379], Loss: 2.1786\n",
      "Epoch [224/500], Step [200/379], Loss: 2.2023\n",
      "Epoch [225/500], Step [200/379], Loss: 2.3242\n",
      "Epoch [226/500], Step [200/379], Loss: 2.2205\n",
      "Epoch [227/500], Step [200/379], Loss: 2.3599\n",
      "Epoch [228/500], Step [200/379], Loss: 2.2069\n",
      "Epoch [229/500], Step [200/379], Loss: 2.3008\n",
      "Epoch [230/500], Step [200/379], Loss: 2.2820\n",
      "Epoch [231/500], Step [200/379], Loss: 2.2918\n",
      "Epoch [232/500], Step [200/379], Loss: 2.2863\n",
      "Epoch [233/500], Step [200/379], Loss: 2.3258\n",
      "Epoch [234/500], Step [200/379], Loss: 2.2730\n",
      "Epoch [235/500], Step [200/379], Loss: 2.2863\n",
      "Epoch [236/500], Step [200/379], Loss: 2.2011\n",
      "Epoch [237/500], Step [200/379], Loss: 2.2780\n",
      "Epoch [238/500], Step [200/379], Loss: 2.2405\n",
      "Epoch [239/500], Step [200/379], Loss: 2.2087\n",
      "Epoch [240/500], Step [200/379], Loss: 2.3545\n",
      "Epoch [241/500], Step [200/379], Loss: 2.2751\n",
      "Epoch [242/500], Step [200/379], Loss: 2.3000\n",
      "Epoch [243/500], Step [200/379], Loss: 2.2221\n",
      "Epoch [244/500], Step [200/379], Loss: 2.1748\n",
      "Epoch [245/500], Step [200/379], Loss: 2.3469\n",
      "Epoch [246/500], Step [200/379], Loss: 2.2532\n",
      "Epoch [247/500], Step [200/379], Loss: 2.3266\n",
      "Epoch [248/500], Step [200/379], Loss: 2.3216\n",
      "Epoch [249/500], Step [200/379], Loss: 2.3281\n",
      "Epoch [250/500], Step [200/379], Loss: 2.3766\n",
      "Epoch [251/500], Step [200/379], Loss: 2.2641\n",
      "Epoch [252/500], Step [200/379], Loss: 2.1561\n",
      "Epoch [253/500], Step [200/379], Loss: 2.2100\n",
      "Epoch [254/500], Step [200/379], Loss: 2.2958\n",
      "Epoch [255/500], Step [200/379], Loss: 2.1937\n",
      "Epoch [256/500], Step [200/379], Loss: 2.3445\n",
      "Epoch [257/500], Step [200/379], Loss: 2.2811\n",
      "Epoch [258/500], Step [200/379], Loss: 2.2322\n",
      "Epoch [259/500], Step [200/379], Loss: 2.3330\n",
      "Epoch [260/500], Step [200/379], Loss: 2.1129\n",
      "Epoch [261/500], Step [200/379], Loss: 2.1372\n",
      "Epoch [262/500], Step [200/379], Loss: 2.1277\n",
      "Epoch [263/500], Step [200/379], Loss: 2.2283\n",
      "Epoch [264/500], Step [200/379], Loss: 2.3264\n",
      "Epoch [265/500], Step [200/379], Loss: 2.1424\n",
      "Epoch [266/500], Step [200/379], Loss: 2.2230\n",
      "Epoch [267/500], Step [200/379], Loss: 2.2048\n",
      "Epoch [268/500], Step [200/379], Loss: 2.2424\n",
      "Epoch [269/500], Step [200/379], Loss: 2.2709\n",
      "Epoch [270/500], Step [200/379], Loss: 2.3570\n",
      "Epoch [271/500], Step [200/379], Loss: 2.3238\n",
      "Epoch [272/500], Step [200/379], Loss: 2.2669\n",
      "Epoch [273/500], Step [200/379], Loss: 2.2334\n",
      "Epoch [274/500], Step [200/379], Loss: 2.2039\n",
      "Epoch [275/500], Step [200/379], Loss: 2.2724\n",
      "Epoch [276/500], Step [200/379], Loss: 2.3149\n",
      "Epoch [277/500], Step [200/379], Loss: 2.2198\n",
      "Epoch [278/500], Step [200/379], Loss: 2.2238\n",
      "Epoch [279/500], Step [200/379], Loss: 2.3043\n",
      "Epoch [280/500], Step [200/379], Loss: 2.3115\n",
      "Epoch [281/500], Step [200/379], Loss: 2.3208\n",
      "Epoch [282/500], Step [200/379], Loss: 2.2024\n",
      "Epoch [283/500], Step [200/379], Loss: 2.2082\n",
      "Epoch [284/500], Step [200/379], Loss: 2.3056\n",
      "Epoch [285/500], Step [200/379], Loss: 2.1767\n",
      "Epoch [286/500], Step [200/379], Loss: 2.2031\n",
      "Epoch [287/500], Step [200/379], Loss: 2.3636\n",
      "Epoch [288/500], Step [200/379], Loss: 2.0804\n",
      "Epoch [289/500], Step [200/379], Loss: 2.1848\n",
      "Epoch [290/500], Step [200/379], Loss: 2.1620\n",
      "Epoch [291/500], Step [200/379], Loss: 2.2633\n",
      "Epoch [292/500], Step [200/379], Loss: 2.2833\n",
      "Epoch [293/500], Step [200/379], Loss: 2.2772\n",
      "Epoch [294/500], Step [200/379], Loss: 2.2490\n",
      "Epoch [295/500], Step [200/379], Loss: 2.2645\n",
      "Epoch [296/500], Step [200/379], Loss: 2.2825\n",
      "Epoch [297/500], Step [200/379], Loss: 2.2707\n",
      "Epoch [298/500], Step [200/379], Loss: 2.1114\n",
      "Epoch [299/500], Step [200/379], Loss: 2.2809\n",
      "Epoch [300/500], Step [200/379], Loss: 2.2358\n",
      "Epoch [301/500], Step [200/379], Loss: 2.2585\n",
      "Epoch [302/500], Step [200/379], Loss: 2.3385\n",
      "Epoch [303/500], Step [200/379], Loss: 2.2616\n",
      "Epoch [304/500], Step [200/379], Loss: 2.1273\n",
      "Epoch [305/500], Step [200/379], Loss: 2.2982\n",
      "Epoch [306/500], Step [200/379], Loss: 2.1196\n",
      "Epoch [307/500], Step [200/379], Loss: 2.3183\n",
      "Epoch [308/500], Step [200/379], Loss: 2.2684\n",
      "Epoch [309/500], Step [200/379], Loss: 2.2624\n",
      "Epoch [310/500], Step [200/379], Loss: 2.2137\n",
      "Epoch [311/500], Step [200/379], Loss: 2.1820\n",
      "Epoch [312/500], Step [200/379], Loss: 2.2774\n",
      "Epoch [313/500], Step [200/379], Loss: 2.1404\n",
      "Epoch [314/500], Step [200/379], Loss: 2.3380\n",
      "Epoch [315/500], Step [200/379], Loss: 2.1862\n",
      "Epoch [316/500], Step [200/379], Loss: 2.3681\n",
      "Epoch [317/500], Step [200/379], Loss: 2.1793\n",
      "Epoch [318/500], Step [200/379], Loss: 2.1579\n",
      "Epoch [319/500], Step [200/379], Loss: 2.2899\n",
      "Epoch [320/500], Step [200/379], Loss: 2.1795\n",
      "Epoch [321/500], Step [200/379], Loss: 2.1315\n",
      "Epoch [322/500], Step [200/379], Loss: 2.2736\n",
      "Epoch [323/500], Step [200/379], Loss: 2.3221\n",
      "Epoch [324/500], Step [200/379], Loss: 2.2772\n",
      "Epoch [325/500], Step [200/379], Loss: 2.3365\n",
      "Epoch [326/500], Step [200/379], Loss: 2.2602\n",
      "Epoch [327/500], Step [200/379], Loss: 2.0995\n",
      "Epoch [328/500], Step [200/379], Loss: 2.2881\n",
      "Epoch [329/500], Step [200/379], Loss: 2.3152\n",
      "Epoch [330/500], Step [200/379], Loss: 2.1283\n",
      "Epoch [331/500], Step [200/379], Loss: 2.1747\n",
      "Epoch [332/500], Step [200/379], Loss: 2.2464\n",
      "Epoch [333/500], Step [200/379], Loss: 2.2843\n",
      "Epoch [334/500], Step [200/379], Loss: 2.2963\n",
      "Epoch [335/500], Step [200/379], Loss: 2.1471\n",
      "Epoch [336/500], Step [200/379], Loss: 2.2164\n",
      "Epoch [337/500], Step [200/379], Loss: 2.2644\n",
      "Epoch [338/500], Step [200/379], Loss: 2.3034\n",
      "Epoch [339/500], Step [200/379], Loss: 2.2770\n",
      "Epoch [340/500], Step [200/379], Loss: 2.1382\n",
      "Epoch [341/500], Step [200/379], Loss: 2.1449\n",
      "Epoch [342/500], Step [200/379], Loss: 2.2604\n",
      "Epoch [343/500], Step [200/379], Loss: 2.2076\n",
      "Epoch [344/500], Step [200/379], Loss: 2.2104\n",
      "Epoch [345/500], Step [200/379], Loss: 2.3416\n",
      "Epoch [346/500], Step [200/379], Loss: 2.2486\n",
      "Epoch [347/500], Step [200/379], Loss: 2.1498\n",
      "Epoch [348/500], Step [200/379], Loss: 2.1439\n",
      "Epoch [349/500], Step [200/379], Loss: 2.2472\n",
      "Epoch [350/500], Step [200/379], Loss: 2.1709\n",
      "Epoch [351/500], Step [200/379], Loss: 2.3230\n",
      "Epoch [352/500], Step [200/379], Loss: 2.1828\n",
      "Epoch [353/500], Step [200/379], Loss: 2.1504\n",
      "Epoch [354/500], Step [200/379], Loss: 2.2645\n",
      "Epoch [355/500], Step [200/379], Loss: 2.3334\n",
      "Epoch [356/500], Step [200/379], Loss: 2.2882\n",
      "Epoch [357/500], Step [200/379], Loss: 2.2581\n",
      "Epoch [358/500], Step [200/379], Loss: 2.2249\n",
      "Epoch [359/500], Step [200/379], Loss: 2.1290\n",
      "Epoch [360/500], Step [200/379], Loss: 2.2746\n",
      "Epoch [361/500], Step [200/379], Loss: 2.2194\n",
      "Epoch [362/500], Step [200/379], Loss: 2.1684\n",
      "Epoch [363/500], Step [200/379], Loss: 2.3068\n",
      "Epoch [364/500], Step [200/379], Loss: 2.3840\n",
      "Epoch [365/500], Step [200/379], Loss: 2.1759\n",
      "Epoch [366/500], Step [200/379], Loss: 2.1550\n",
      "Epoch [367/500], Step [200/379], Loss: 2.2754\n",
      "Epoch [368/500], Step [200/379], Loss: 2.1437\n",
      "Epoch [369/500], Step [200/379], Loss: 2.0796\n",
      "Epoch [370/500], Step [200/379], Loss: 2.2006\n",
      "Epoch [371/500], Step [200/379], Loss: 2.0583\n",
      "Epoch [372/500], Step [200/379], Loss: 2.1838\n",
      "Epoch [373/500], Step [200/379], Loss: 2.3063\n",
      "Epoch [374/500], Step [200/379], Loss: 2.1072\n",
      "Epoch [375/500], Step [200/379], Loss: 2.1703\n",
      "Epoch [376/500], Step [200/379], Loss: 2.2044\n",
      "Epoch [377/500], Step [200/379], Loss: 2.2393\n",
      "Epoch [378/500], Step [200/379], Loss: 2.3494\n",
      "Epoch [379/500], Step [200/379], Loss: 2.3374\n",
      "Epoch [380/500], Step [200/379], Loss: 2.1928\n",
      "Epoch [381/500], Step [200/379], Loss: 2.1817\n",
      "Epoch [382/500], Step [200/379], Loss: 2.1453\n",
      "Epoch [383/500], Step [200/379], Loss: 2.1811\n",
      "Epoch [384/500], Step [200/379], Loss: 2.3321\n",
      "Epoch [385/500], Step [200/379], Loss: 2.0428\n",
      "Epoch [386/500], Step [200/379], Loss: 2.3439\n",
      "Epoch [387/500], Step [200/379], Loss: 2.2103\n",
      "Epoch [388/500], Step [200/379], Loss: 2.3072\n",
      "Epoch [389/500], Step [200/379], Loss: 2.2770\n",
      "Epoch [390/500], Step [200/379], Loss: 2.0615\n",
      "Epoch [391/500], Step [200/379], Loss: 2.3175\n",
      "Epoch [392/500], Step [200/379], Loss: 2.1536\n",
      "Epoch [393/500], Step [200/379], Loss: 2.2556\n",
      "Epoch [394/500], Step [200/379], Loss: 2.2695\n",
      "Epoch [395/500], Step [200/379], Loss: 2.1730\n",
      "Epoch [396/500], Step [200/379], Loss: 2.1524\n",
      "Epoch [397/500], Step [200/379], Loss: 2.3664\n",
      "Epoch [398/500], Step [200/379], Loss: 2.3599\n",
      "Epoch [399/500], Step [200/379], Loss: 2.1200\n",
      "Epoch [400/500], Step [200/379], Loss: 2.2431\n",
      "Epoch [401/500], Step [200/379], Loss: 2.0358\n",
      "Epoch [402/500], Step [200/379], Loss: 2.2399\n",
      "Epoch [403/500], Step [200/379], Loss: 2.0682\n",
      "Epoch [404/500], Step [200/379], Loss: 2.2549\n",
      "Epoch [405/500], Step [200/379], Loss: 2.1549\n",
      "Epoch [406/500], Step [200/379], Loss: 2.1773\n",
      "Epoch [407/500], Step [200/379], Loss: 2.1610\n",
      "Epoch [408/500], Step [200/379], Loss: 2.1549\n",
      "Epoch [409/500], Step [200/379], Loss: 2.1264\n",
      "Epoch [410/500], Step [200/379], Loss: 2.2007\n",
      "Epoch [411/500], Step [200/379], Loss: 2.3725\n",
      "Epoch [412/500], Step [200/379], Loss: 2.1605\n",
      "Epoch [413/500], Step [200/379], Loss: 2.2810\n",
      "Epoch [414/500], Step [200/379], Loss: 2.0829\n",
      "Epoch [415/500], Step [200/379], Loss: 2.1855\n",
      "Epoch [416/500], Step [200/379], Loss: 2.2140\n",
      "Epoch [417/500], Step [200/379], Loss: 2.2099\n",
      "Epoch [418/500], Step [200/379], Loss: 2.2899\n",
      "Epoch [419/500], Step [200/379], Loss: 2.3199\n",
      "Epoch [420/500], Step [200/379], Loss: 2.2579\n",
      "Epoch [421/500], Step [200/379], Loss: 2.3224\n",
      "Epoch [422/500], Step [200/379], Loss: 2.2506\n",
      "Epoch [423/500], Step [200/379], Loss: 2.3284\n",
      "Epoch [424/500], Step [200/379], Loss: 2.0479\n",
      "Epoch [425/500], Step [200/379], Loss: 2.1667\n",
      "Epoch [426/500], Step [200/379], Loss: 2.0528\n",
      "Epoch [427/500], Step [200/379], Loss: 2.2959\n",
      "Epoch [428/500], Step [200/379], Loss: 2.2386\n",
      "Epoch [429/500], Step [200/379], Loss: 2.2781\n",
      "Epoch [430/500], Step [200/379], Loss: 2.2611\n",
      "Epoch [431/500], Step [200/379], Loss: 2.1942\n",
      "Epoch [432/500], Step [200/379], Loss: 2.1501\n",
      "Epoch [433/500], Step [200/379], Loss: 2.3717\n",
      "Epoch [434/500], Step [200/379], Loss: 2.2893\n",
      "Epoch [435/500], Step [200/379], Loss: 2.1072\n",
      "Epoch [436/500], Step [200/379], Loss: 2.1709\n",
      "Epoch [437/500], Step [200/379], Loss: 2.0437\n",
      "Epoch [438/500], Step [200/379], Loss: 2.2549\n",
      "Epoch [439/500], Step [200/379], Loss: 2.2764\n",
      "Epoch [440/500], Step [200/379], Loss: 2.2059\n",
      "Epoch [441/500], Step [200/379], Loss: 2.3967\n",
      "Epoch [442/500], Step [200/379], Loss: 2.1522\n",
      "Epoch [443/500], Step [200/379], Loss: 2.2521\n",
      "Epoch [444/500], Step [200/379], Loss: 2.2658\n",
      "Epoch [445/500], Step [200/379], Loss: 2.3106\n",
      "Epoch [446/500], Step [200/379], Loss: 2.2161\n",
      "Epoch [447/500], Step [200/379], Loss: 2.1252\n",
      "Epoch [448/500], Step [200/379], Loss: 2.2659\n",
      "Epoch [449/500], Step [200/379], Loss: 2.3422\n",
      "Epoch [450/500], Step [200/379], Loss: 2.2856\n",
      "Epoch [451/500], Step [200/379], Loss: 2.1621\n",
      "Epoch [452/500], Step [200/379], Loss: 2.2018\n",
      "Epoch [453/500], Step [200/379], Loss: 2.1244\n",
      "Epoch [454/500], Step [200/379], Loss: 2.2034\n",
      "Epoch [455/500], Step [200/379], Loss: 2.2498\n",
      "Epoch [456/500], Step [200/379], Loss: 2.2106\n",
      "Epoch [457/500], Step [200/379], Loss: 2.3300\n",
      "Epoch [458/500], Step [200/379], Loss: 2.2298\n",
      "Epoch [459/500], Step [200/379], Loss: 2.2628\n",
      "Epoch [460/500], Step [200/379], Loss: 2.1115\n",
      "Epoch [461/500], Step [200/379], Loss: 2.3572\n",
      "Epoch [462/500], Step [200/379], Loss: 2.0483\n",
      "Epoch [463/500], Step [200/379], Loss: 2.0687\n",
      "Epoch [464/500], Step [200/379], Loss: 2.1423\n",
      "Epoch [465/500], Step [200/379], Loss: 2.0767\n",
      "Epoch [466/500], Step [200/379], Loss: 2.2079\n",
      "Epoch [467/500], Step [200/379], Loss: 2.2279\n",
      "Epoch [468/500], Step [200/379], Loss: 2.0251\n",
      "Epoch [469/500], Step [200/379], Loss: 2.2486\n",
      "Epoch [470/500], Step [200/379], Loss: 2.1959\n",
      "Epoch [471/500], Step [200/379], Loss: 2.2293\n",
      "Epoch [472/500], Step [200/379], Loss: 2.1961\n",
      "Epoch [473/500], Step [200/379], Loss: 2.2647\n",
      "Epoch [474/500], Step [200/379], Loss: 2.0654\n",
      "Epoch [475/500], Step [200/379], Loss: 2.4498\n",
      "Epoch [476/500], Step [200/379], Loss: 2.0843\n",
      "Epoch [477/500], Step [200/379], Loss: 2.2146\n",
      "Epoch [478/500], Step [200/379], Loss: 2.4383\n",
      "Epoch [479/500], Step [200/379], Loss: 2.1777\n",
      "Epoch [480/500], Step [200/379], Loss: 2.2826\n",
      "Epoch [481/500], Step [200/379], Loss: 2.1703\n",
      "Epoch [482/500], Step [200/379], Loss: 2.2836\n",
      "Epoch [483/500], Step [200/379], Loss: 2.3171\n",
      "Epoch [484/500], Step [200/379], Loss: 2.0600\n",
      "Epoch [485/500], Step [200/379], Loss: 2.1092\n",
      "Epoch [486/500], Step [200/379], Loss: 2.2542\n",
      "Epoch [487/500], Step [200/379], Loss: 2.1508\n",
      "Epoch [488/500], Step [200/379], Loss: 2.3507\n",
      "Epoch [489/500], Step [200/379], Loss: 2.2434\n",
      "Epoch [490/500], Step [200/379], Loss: 2.1984\n",
      "Epoch [491/500], Step [200/379], Loss: 2.2547\n",
      "Epoch [492/500], Step [200/379], Loss: 2.0674\n",
      "Epoch [493/500], Step [200/379], Loss: 2.4765\n",
      "Epoch [494/500], Step [200/379], Loss: 2.2552\n",
      "Epoch [495/500], Step [200/379], Loss: 2.1745\n",
      "Epoch [496/500], Step [200/379], Loss: 2.1653\n",
      "Epoch [497/500], Step [200/379], Loss: 2.1560\n",
      "Epoch [498/500], Step [200/379], Loss: 2.1171\n",
      "Epoch [499/500], Step [200/379], Loss: 2.2612\n",
      "Epoch [500/500], Step [200/379], Loss: 2.2922\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Can't call numpy() on Tensor that requires grad. Use tensor.detach().numpy() instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/2l/b3b8rt8j60b1bqtxxf00654c0000gn/T/ipykernel_36961/1044437881.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#model= resnet18().to(device)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/var/folders/2l/b3b8rt8j60b1bqtxxf00654c0000gn/T/ipykernel_36961/1400332675.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, num_epochs, learning_rate)\u001b[0m\n\u001b[1;32m     24\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m200\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m                 \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34mf'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{n_total_steps}], Loss: {loss.item():.4f}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscatter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinspace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mloss_arr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Finished Training'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/matplotlib/pyplot.py\u001b[0m in \u001b[0;36mscatter\u001b[0;34m(x, y, s, c, marker, cmap, norm, vmin, vmax, alpha, linewidths, edgecolors, plotnonfinite, data, **kwargs)\u001b[0m\n\u001b[1;32m   3066\u001b[0m         \u001b[0mvmin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvmax\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlinewidths\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3067\u001b[0m         edgecolors=None, plotnonfinite=False, data=None, **kwargs):\n\u001b[0;32m-> 3068\u001b[0;31m     __ret = gca().scatter(\n\u001b[0m\u001b[1;32m   3069\u001b[0m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmarker\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmarker\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcmap\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcmap\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3070\u001b[0m         \u001b[0mvmin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvmin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvmax\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvmax\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlinewidths\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlinewidths\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/matplotlib/__init__.py\u001b[0m in \u001b[0;36minner\u001b[0;34m(ax, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1359\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1360\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1361\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msanitize_sequence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1362\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1363\u001b[0m         \u001b[0mbound\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_sig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/matplotlib/axes/_axes.py\u001b[0m in \u001b[0;36mscatter\u001b[0;34m(self, x, y, s, c, marker, cmap, norm, vmin, vmax, alpha, linewidths, edgecolors, plotnonfinite, **kwargs)\u001b[0m\n\u001b[1;32m   4494\u001b[0m         \u001b[0;31m# unless its argument is a masked array.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4495\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4496\u001b[0;31m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4497\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4498\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"x and y must be the same size\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/numpy/ma/core.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, a, *args, **params)\u001b[0m\n\u001b[1;32m   6765\u001b[0m             \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6766\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6767\u001b[0;31m         \u001b[0mmarr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0masanyarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6768\u001b[0m         \u001b[0mmethod_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6769\u001b[0m         \u001b[0mmethod\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmarr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/numpy/ma/core.py\u001b[0m in \u001b[0;36masanyarray\u001b[0;34m(a, dtype)\u001b[0m\n\u001b[1;32m   7998\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMaskedArray\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   7999\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 8000\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mmasked_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeep_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubok\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   8001\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   8002\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/numpy/ma/core.py\u001b[0m in \u001b[0;36m__new__\u001b[0;34m(cls, data, mask, dtype, copy, subok, ndmin, fill_value, keep_mask, hard_mask, shrink, order)\u001b[0m\n\u001b[1;32m   2823\u001b[0m         \"\"\"\n\u001b[1;32m   2824\u001b[0m         \u001b[0;31m# Process data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2825\u001b[0;31m         _data = np.array(data, dtype=dtype, copy=copy,\n\u001b[0m\u001b[1;32m   2826\u001b[0m                          order=order, subok=True, ndmin=ndmin)\n\u001b[1;32m   2827\u001b[0m         \u001b[0m_baseclass\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'_baseclass'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36m__array__\u001b[0;34m(self, dtype)\u001b[0m\n\u001b[1;32m    755\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__array__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    756\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 757\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    758\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    759\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Can't call numpy() on Tensor that requires grad. Use tensor.detach().numpy() instead."
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
