{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9d3a9447-36a4-4dd7-b3d6-ff41c01b5d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import TensorDataset, DataLoader,Dataset\n",
    "from model_classes.convNet import *\n",
    "from model_classes.DNN import *\n",
    "from torchvision.models import resnet18\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "from test import report_accuracies\n",
    "from const import *\n",
    "from utils import *\n",
    "\n",
    "# Device configuration\n",
    "save=False\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "batch_size=4\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "from torchvision.io import read_image\n",
    "\n",
    "class CustomImageDataset(Dataset):\n",
    "    def __init__(self, annotations_file, img_dir, transform=None, target_transform=None):\n",
    "        self.img_labels = pd.read_csv(annotations_file)\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        img_path = os.path.join(self.img_dir, self.img_labels.iloc[idx, 0])\n",
    "        #print(f\"image path: {img_path}\")\n",
    "        image = read_image(img_path)\n",
    "        image=image.to(torch.float32)\n",
    "        #print(f\"image type: {type(image)}\")\n",
    "        label = self.img_labels.iloc[idx, 1]\n",
    "        if self.transform:\n",
    "            #print('here')\n",
    "            image = self.transform(image)\n",
    "        if self.target_transform:\n",
    "            label = self.target_transform(label)\n",
    "        return image, label\n",
    "\n",
    "def train(model,num_epochs=500,learning_rate=0.00001):\n",
    "#    MODEL_CHECKPOINT\n",
    "    print(f\"lr={learning_rate}\")\n",
    "    loss_arr=[]\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    n_total_steps = len(train_loader)\n",
    "    with open(LOSS_LOG_PATH, 'a') as f:\n",
    "        f.writelines(f\"{model.model_name}_ep{num_epochs}\\n\")\n",
    "        f.writelines(f\"training time:\\n\")\n",
    "        f.writelines(datetime.now().strftime(\"%Y%m%d_%H%M%S\"))\n",
    "        f.write('\\n')\n",
    "        for epoch in range(num_epochs):\n",
    "            for i, (images, labels) in enumerate(train_loader):\n",
    "                # origin shape: [4, 3, 32, 32] = 4, 3, 1024\n",
    "                # input_layer: 3 input channels, 6 output channels, 5 kernel size\n",
    "                images = images.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # Forward pass\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                # Backward and optimize\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                loss_arr.append(loss)\n",
    "                if (i+1) % 200 == 0:\n",
    "                    print (f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{n_total_steps}], Loss: {loss.item():.4f}')\n",
    "            if epoch% int(num_epochs/10)==0 and save:\n",
    "                print(\"saving model checkpoint\")\n",
    "                savePath=f\"{model.model_name}_ep{epoch}.pth\"\n",
    "                savePath=os.path.join(MODEL_CHECKPOINT,savePath)\n",
    "                torch.save(model.state_dict(),'./cnn.pth')\n",
    "                \n",
    "            f.writelines(f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{n_total_steps}], Loss: {loss.item():.4f}\\n')\n",
    "\n",
    "        #plt.scatter(np.linspace(1, num_epochs, num_epochs).astype(int),loss_arr)\n",
    "        #plt.show()\n",
    "        print('Finished Training')\n",
    "        f.write(\"Finished Training\")\n",
    "        training_name=model.model_name+\"_\"+get_datetime()\n",
    "        PATH=os.path.join(TRAINED_MODELS_PATH,training_name)\n",
    "        torch.save(model.state_dict(), PATH)\n",
    "        torch.save(model.state_dict(), './cnn.pth')\n",
    "    report_accuracies(model,batch_size=batch_size,logFile=ACC_LOG_PATH)\n",
    "#Load model and train\n",
    "def train_from_load(model_object,modelPath,num_epochs,learning_rate):\n",
    "    \n",
    "   \n",
    "    #MODEL_PATH=os.path.join(\"trained_models\",\"1_1_convNet2_ep=250_lr_0001.pth\")\n",
    "    model.load_state_dict(torch.load(modelPath,map_location=torch.device(device)),strict=False)\n",
    "    model.eval()\n",
    "    train(model,num_epochs=num_epochs,learning_rate=learning_rate)\n",
    "\n",
    "def train_previous(model_object,num_epochs,learning_rate):\n",
    "    model.load_state_dict(torch.load('./cnn.pth',map_location=torch.device(device)),strict=False)\n",
    "    model.eval()\n",
    "    train(model,num_epochs=num_epochs,learning_rate=learning_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ae099a01-f6e8-497c-90b5-68cb9a005f6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n",
      "torch.Size([1, 44, 6])\n",
      "lr=0.001\n",
      "Epoch [1/300], Step [200/735], Loss: 2.3532\n",
      "Epoch [1/300], Step [400/735], Loss: 2.3143\n",
      "Epoch [1/300], Step [600/735], Loss: 2.3421\n",
      "Epoch [2/300], Step [200/735], Loss: 2.2943\n",
      "Epoch [2/300], Step [400/735], Loss: 2.2839\n",
      "Epoch [2/300], Step [600/735], Loss: 2.2766\n",
      "Epoch [3/300], Step [200/735], Loss: 2.3402\n",
      "Epoch [3/300], Step [400/735], Loss: 2.3085\n",
      "Epoch [3/300], Step [600/735], Loss: 2.3092\n",
      "Epoch [4/300], Step [200/735], Loss: 2.3328\n",
      "Epoch [4/300], Step [400/735], Loss: 2.3040\n",
      "Epoch [4/300], Step [600/735], Loss: 2.2967\n",
      "Epoch [5/300], Step [200/735], Loss: 2.2884\n",
      "Epoch [5/300], Step [400/735], Loss: 2.3093\n",
      "Epoch [5/300], Step [600/735], Loss: 2.3147\n",
      "Epoch [6/300], Step [200/735], Loss: 2.3071\n",
      "Epoch [6/300], Step [400/735], Loss: 2.3230\n",
      "Epoch [6/300], Step [600/735], Loss: 2.3098\n",
      "Epoch [7/300], Step [200/735], Loss: 2.3054\n",
      "Epoch [7/300], Step [400/735], Loss: 2.3140\n",
      "Epoch [7/300], Step [600/735], Loss: 2.3138\n",
      "Epoch [8/300], Step [200/735], Loss: 2.3116\n",
      "Epoch [8/300], Step [400/735], Loss: 2.3235\n",
      "Epoch [8/300], Step [600/735], Loss: 2.3000\n",
      "Epoch [9/300], Step [200/735], Loss: 2.2994\n",
      "Epoch [9/300], Step [400/735], Loss: 2.2847\n",
      "Epoch [9/300], Step [600/735], Loss: 2.3053\n",
      "Epoch [10/300], Step [200/735], Loss: 2.3028\n",
      "Epoch [10/300], Step [400/735], Loss: 2.3101\n",
      "Epoch [10/300], Step [600/735], Loss: 2.3061\n",
      "Epoch [11/300], Step [200/735], Loss: 2.3018\n",
      "Epoch [11/300], Step [400/735], Loss: 2.2857\n",
      "Epoch [11/300], Step [600/735], Loss: 2.3150\n",
      "Epoch [12/300], Step [200/735], Loss: 2.2991\n",
      "Epoch [12/300], Step [400/735], Loss: 2.3164\n",
      "Epoch [12/300], Step [600/735], Loss: 2.2957\n",
      "Epoch [13/300], Step [200/735], Loss: 2.3026\n",
      "Epoch [13/300], Step [400/735], Loss: 2.3176\n",
      "Epoch [13/300], Step [600/735], Loss: 2.3008\n",
      "Epoch [14/300], Step [200/735], Loss: 2.3030\n",
      "Epoch [14/300], Step [400/735], Loss: 2.2984\n",
      "Epoch [14/300], Step [600/735], Loss: 2.3116\n",
      "Epoch [15/300], Step [200/735], Loss: 2.3086\n",
      "Epoch [15/300], Step [400/735], Loss: 2.3044\n",
      "Epoch [15/300], Step [600/735], Loss: 2.3065\n",
      "Epoch [16/300], Step [200/735], Loss: 2.2880\n",
      "Epoch [16/300], Step [400/735], Loss: 2.3065\n",
      "Epoch [16/300], Step [600/735], Loss: 2.3075\n",
      "Epoch [17/300], Step [200/735], Loss: 2.3089\n",
      "Epoch [17/300], Step [400/735], Loss: 2.2994\n",
      "Epoch [17/300], Step [600/735], Loss: 2.3097\n",
      "Epoch [18/300], Step [200/735], Loss: 2.3074\n",
      "Epoch [18/300], Step [400/735], Loss: 2.3048\n",
      "Epoch [18/300], Step [600/735], Loss: 2.3135\n",
      "Epoch [19/300], Step [200/735], Loss: 2.2993\n",
      "Epoch [19/300], Step [400/735], Loss: 2.3093\n",
      "Epoch [19/300], Step [600/735], Loss: 2.2976\n",
      "Epoch [20/300], Step [200/735], Loss: 2.2881\n",
      "Epoch [20/300], Step [400/735], Loss: 2.3213\n",
      "Epoch [20/300], Step [600/735], Loss: 2.3054\n",
      "Epoch [21/300], Step [200/735], Loss: 2.3231\n",
      "Epoch [21/300], Step [400/735], Loss: 2.2993\n",
      "Epoch [21/300], Step [600/735], Loss: 2.3043\n",
      "Epoch [22/300], Step [200/735], Loss: 2.2964\n",
      "Epoch [22/300], Step [400/735], Loss: 2.3057\n",
      "Epoch [22/300], Step [600/735], Loss: 2.3018\n",
      "Epoch [23/300], Step [200/735], Loss: 2.3164\n",
      "Epoch [23/300], Step [400/735], Loss: 2.3163\n",
      "Epoch [23/300], Step [600/735], Loss: 2.3086\n",
      "Epoch [24/300], Step [200/735], Loss: 2.3061\n",
      "Epoch [24/300], Step [400/735], Loss: 2.2959\n",
      "Epoch [24/300], Step [600/735], Loss: 2.3100\n",
      "Epoch [25/300], Step [200/735], Loss: 2.2934\n",
      "Epoch [25/300], Step [400/735], Loss: 2.2826\n",
      "Epoch [25/300], Step [600/735], Loss: 2.3074\n",
      "Epoch [26/300], Step [200/735], Loss: 2.3080\n",
      "Epoch [26/300], Step [400/735], Loss: 2.2965\n",
      "Epoch [26/300], Step [600/735], Loss: 2.3071\n",
      "Epoch [27/300], Step [200/735], Loss: 2.3021\n",
      "Epoch [27/300], Step [400/735], Loss: 2.2801\n",
      "Epoch [27/300], Step [600/735], Loss: 2.2974\n",
      "Epoch [28/300], Step [200/735], Loss: 2.3188\n",
      "Epoch [28/300], Step [400/735], Loss: 2.2941\n",
      "Epoch [28/300], Step [600/735], Loss: 2.3193\n",
      "Epoch [29/300], Step [200/735], Loss: 2.3048\n",
      "Epoch [29/300], Step [400/735], Loss: 2.2989\n",
      "Epoch [29/300], Step [600/735], Loss: 2.2949\n",
      "Epoch [30/300], Step [200/735], Loss: 2.3024\n",
      "Epoch [30/300], Step [400/735], Loss: 2.3070\n",
      "Epoch [30/300], Step [600/735], Loss: 2.3019\n",
      "Epoch [31/300], Step [200/735], Loss: 2.3063\n",
      "Epoch [31/300], Step [400/735], Loss: 2.3178\n",
      "Epoch [31/300], Step [600/735], Loss: 2.2970\n",
      "Epoch [32/300], Step [200/735], Loss: 2.3080\n",
      "Epoch [32/300], Step [400/735], Loss: 2.2896\n",
      "Epoch [32/300], Step [600/735], Loss: 2.2855\n",
      "Epoch [33/300], Step [200/735], Loss: 2.2864\n",
      "Epoch [33/300], Step [400/735], Loss: 2.3186\n",
      "Epoch [33/300], Step [600/735], Loss: 2.2866\n",
      "Epoch [34/300], Step [200/735], Loss: 2.3033\n",
      "Epoch [34/300], Step [400/735], Loss: 2.3018\n",
      "Epoch [34/300], Step [600/735], Loss: 2.3078\n",
      "Epoch [35/300], Step [200/735], Loss: 2.3031\n",
      "Epoch [35/300], Step [400/735], Loss: 2.2881\n",
      "Epoch [35/300], Step [600/735], Loss: 2.3016\n",
      "Epoch [36/300], Step [200/735], Loss: 2.3130\n",
      "Epoch [36/300], Step [400/735], Loss: 2.2998\n",
      "Epoch [36/300], Step [600/735], Loss: 2.3178\n",
      "Epoch [37/300], Step [200/735], Loss: 2.3242\n",
      "Epoch [37/300], Step [400/735], Loss: 2.3087\n",
      "Epoch [37/300], Step [600/735], Loss: 2.3312\n",
      "Epoch [38/300], Step [200/735], Loss: 2.3042\n",
      "Epoch [38/300], Step [400/735], Loss: 2.3151\n",
      "Epoch [38/300], Step [600/735], Loss: 2.3115\n",
      "Epoch [39/300], Step [200/735], Loss: 2.2956\n",
      "Epoch [39/300], Step [400/735], Loss: 2.3058\n",
      "Epoch [39/300], Step [600/735], Loss: 2.3037\n",
      "Epoch [40/300], Step [200/735], Loss: 2.2965\n",
      "Epoch [40/300], Step [400/735], Loss: 2.3293\n",
      "Epoch [40/300], Step [600/735], Loss: 2.3045\n",
      "Epoch [41/300], Step [200/735], Loss: 2.2807\n",
      "Epoch [41/300], Step [400/735], Loss: 2.2873\n",
      "Epoch [41/300], Step [600/735], Loss: 2.3052\n",
      "Epoch [42/300], Step [200/735], Loss: 2.3205\n",
      "Epoch [42/300], Step [400/735], Loss: 2.3095\n",
      "Epoch [42/300], Step [600/735], Loss: 2.3020\n",
      "Epoch [43/300], Step [200/735], Loss: 2.3076\n",
      "Epoch [43/300], Step [400/735], Loss: 2.2955\n",
      "Epoch [43/300], Step [600/735], Loss: 2.3077\n",
      "Epoch [44/300], Step [200/735], Loss: 2.3204\n",
      "Epoch [44/300], Step [400/735], Loss: 2.3009\n",
      "Epoch [44/300], Step [600/735], Loss: 2.3065\n",
      "Epoch [45/300], Step [200/735], Loss: 2.2718\n",
      "Epoch [45/300], Step [400/735], Loss: 2.2807\n",
      "Epoch [45/300], Step [600/735], Loss: 2.3235\n",
      "Epoch [46/300], Step [200/735], Loss: 2.2637\n",
      "Epoch [46/300], Step [400/735], Loss: 2.3258\n",
      "Epoch [46/300], Step [600/735], Loss: 2.3138\n",
      "Epoch [47/300], Step [200/735], Loss: 2.3151\n",
      "Epoch [47/300], Step [400/735], Loss: 2.2879\n",
      "Epoch [47/300], Step [600/735], Loss: 2.2760\n",
      "Epoch [48/300], Step [200/735], Loss: 2.2787\n",
      "Epoch [48/300], Step [400/735], Loss: 2.3196\n",
      "Epoch [48/300], Step [600/735], Loss: 2.3110\n",
      "Epoch [49/300], Step [200/735], Loss: 2.2900\n",
      "Epoch [49/300], Step [400/735], Loss: 2.3015\n",
      "Epoch [49/300], Step [600/735], Loss: 2.3348\n",
      "Epoch [50/300], Step [200/735], Loss: 2.3350\n",
      "Epoch [50/300], Step [400/735], Loss: 2.2895\n",
      "Epoch [50/300], Step [600/735], Loss: 2.3092\n",
      "Epoch [51/300], Step [200/735], Loss: 2.2986\n",
      "Epoch [51/300], Step [400/735], Loss: 2.3013\n",
      "Epoch [51/300], Step [600/735], Loss: 2.3100\n",
      "Epoch [52/300], Step [200/735], Loss: 2.2980\n",
      "Epoch [52/300], Step [400/735], Loss: 2.3267\n",
      "Epoch [52/300], Step [600/735], Loss: 2.3012\n",
      "Epoch [53/300], Step [200/735], Loss: 2.2966\n",
      "Epoch [53/300], Step [400/735], Loss: 2.2842\n",
      "Epoch [53/300], Step [600/735], Loss: 2.2932\n",
      "Epoch [54/300], Step [200/735], Loss: 2.3102\n",
      "Epoch [54/300], Step [400/735], Loss: 2.3101\n",
      "Epoch [54/300], Step [600/735], Loss: 2.2880\n",
      "Epoch [55/300], Step [200/735], Loss: 2.2920\n",
      "Epoch [55/300], Step [400/735], Loss: 2.3189\n",
      "Epoch [55/300], Step [600/735], Loss: 2.3040\n",
      "Epoch [56/300], Step [200/735], Loss: 2.3141\n",
      "Epoch [56/300], Step [400/735], Loss: 2.3052\n",
      "Epoch [56/300], Step [600/735], Loss: 2.3091\n",
      "Epoch [57/300], Step [200/735], Loss: 2.3265\n",
      "Epoch [57/300], Step [400/735], Loss: 2.2823\n",
      "Epoch [57/300], Step [600/735], Loss: 2.3146\n",
      "Epoch [58/300], Step [200/735], Loss: 2.2835\n",
      "Epoch [58/300], Step [400/735], Loss: 2.2982\n",
      "Epoch [58/300], Step [600/735], Loss: 2.3121\n",
      "Epoch [59/300], Step [200/735], Loss: 2.2825\n",
      "Epoch [59/300], Step [400/735], Loss: 2.2802\n",
      "Epoch [59/300], Step [600/735], Loss: 2.3195\n",
      "Epoch [60/300], Step [200/735], Loss: 2.3133\n",
      "Epoch [60/300], Step [400/735], Loss: 2.2975\n",
      "Epoch [60/300], Step [600/735], Loss: 2.3218\n",
      "Epoch [61/300], Step [200/735], Loss: 2.2982\n",
      "Epoch [61/300], Step [400/735], Loss: 2.3232\n",
      "Epoch [61/300], Step [600/735], Loss: 2.2822\n",
      "Epoch [62/300], Step [200/735], Loss: 2.3073\n",
      "Epoch [62/300], Step [400/735], Loss: 2.2833\n",
      "Epoch [62/300], Step [600/735], Loss: 2.3300\n",
      "Epoch [63/300], Step [200/735], Loss: 2.3022\n",
      "Epoch [63/300], Step [400/735], Loss: 2.2965\n",
      "Epoch [63/300], Step [600/735], Loss: 2.3136\n",
      "Epoch [64/300], Step [200/735], Loss: 2.2778\n",
      "Epoch [64/300], Step [400/735], Loss: 2.2784\n",
      "Epoch [64/300], Step [600/735], Loss: 2.3165\n",
      "Epoch [65/300], Step [200/735], Loss: 2.2783\n",
      "Epoch [65/300], Step [400/735], Loss: 2.3134\n",
      "Epoch [65/300], Step [600/735], Loss: 2.3191\n",
      "Epoch [66/300], Step [200/735], Loss: 2.3083\n",
      "Epoch [66/300], Step [400/735], Loss: 2.2787\n",
      "Epoch [66/300], Step [600/735], Loss: 2.2845\n",
      "Epoch [67/300], Step [200/735], Loss: 2.2974\n",
      "Epoch [67/300], Step [400/735], Loss: 2.2970\n",
      "Epoch [67/300], Step [600/735], Loss: 2.2968\n",
      "Epoch [68/300], Step [200/735], Loss: 2.3074\n",
      "Epoch [68/300], Step [400/735], Loss: 2.3082\n",
      "Epoch [68/300], Step [600/735], Loss: 2.3063\n",
      "Epoch [69/300], Step [200/735], Loss: 2.3096\n",
      "Epoch [69/300], Step [400/735], Loss: 2.2728\n",
      "Epoch [69/300], Step [600/735], Loss: 2.2944\n",
      "Epoch [70/300], Step [200/735], Loss: 2.2866\n",
      "Epoch [70/300], Step [400/735], Loss: 2.3186\n",
      "Epoch [70/300], Step [600/735], Loss: 2.2851\n",
      "Epoch [71/300], Step [200/735], Loss: 2.2912\n",
      "Epoch [71/300], Step [400/735], Loss: 2.2891\n",
      "Epoch [71/300], Step [600/735], Loss: 2.3001\n",
      "Epoch [72/300], Step [200/735], Loss: 2.2973\n",
      "Epoch [72/300], Step [400/735], Loss: 2.3096\n",
      "Epoch [72/300], Step [600/735], Loss: 2.2991\n",
      "Epoch [73/300], Step [200/735], Loss: 2.3181\n",
      "Epoch [73/300], Step [400/735], Loss: 2.2888\n",
      "Epoch [73/300], Step [600/735], Loss: 2.3084\n",
      "Epoch [74/300], Step [200/735], Loss: 2.2824\n",
      "Epoch [74/300], Step [400/735], Loss: 2.3185\n",
      "Epoch [74/300], Step [600/735], Loss: 2.3037\n",
      "Epoch [75/300], Step [200/735], Loss: 2.3082\n",
      "Epoch [75/300], Step [400/735], Loss: 2.2690\n",
      "Epoch [75/300], Step [600/735], Loss: 2.2906\n",
      "Epoch [76/300], Step [200/735], Loss: 2.2734\n",
      "Epoch [76/300], Step [400/735], Loss: 2.3073\n",
      "Epoch [76/300], Step [600/735], Loss: 2.3057\n",
      "Epoch [77/300], Step [200/735], Loss: 2.3195\n",
      "Epoch [77/300], Step [400/735], Loss: 2.2788\n",
      "Epoch [77/300], Step [600/735], Loss: 2.3290\n",
      "Epoch [78/300], Step [200/735], Loss: 2.3241\n",
      "Epoch [78/300], Step [400/735], Loss: 2.2780\n",
      "Epoch [78/300], Step [600/735], Loss: 2.3229\n",
      "Epoch [79/300], Step [200/735], Loss: 2.3168\n",
      "Epoch [79/300], Step [400/735], Loss: 2.2923\n",
      "Epoch [79/300], Step [600/735], Loss: 2.3007\n",
      "Epoch [80/300], Step [200/735], Loss: 2.3002\n",
      "Epoch [80/300], Step [400/735], Loss: 2.3077\n",
      "Epoch [80/300], Step [600/735], Loss: 2.2974\n",
      "Epoch [81/300], Step [200/735], Loss: 2.3059\n",
      "Epoch [81/300], Step [400/735], Loss: 2.3034\n",
      "Epoch [81/300], Step [600/735], Loss: 2.3120\n",
      "Epoch [82/300], Step [200/735], Loss: 2.3168\n",
      "Epoch [82/300], Step [400/735], Loss: 2.2874\n",
      "Epoch [82/300], Step [600/735], Loss: 2.2942\n",
      "Epoch [83/300], Step [200/735], Loss: 2.3012\n",
      "Epoch [83/300], Step [400/735], Loss: 2.2911\n",
      "Epoch [83/300], Step [600/735], Loss: 2.2980\n",
      "Epoch [84/300], Step [200/735], Loss: 2.2809\n",
      "Epoch [84/300], Step [400/735], Loss: 2.3121\n",
      "Epoch [84/300], Step [600/735], Loss: 2.2927\n",
      "Epoch [85/300], Step [200/735], Loss: 2.3084\n",
      "Epoch [85/300], Step [400/735], Loss: 2.3079\n",
      "Epoch [85/300], Step [600/735], Loss: 2.3319\n",
      "Epoch [86/300], Step [200/735], Loss: 2.2997\n",
      "Epoch [86/300], Step [400/735], Loss: 2.3000\n",
      "Epoch [86/300], Step [600/735], Loss: 2.2818\n",
      "Epoch [87/300], Step [200/735], Loss: 2.2918\n",
      "Epoch [87/300], Step [400/735], Loss: 2.3141\n",
      "Epoch [87/300], Step [600/735], Loss: 2.2873\n",
      "Epoch [88/300], Step [200/735], Loss: 2.3079\n",
      "Epoch [88/300], Step [400/735], Loss: 2.3218\n",
      "Epoch [88/300], Step [600/735], Loss: 2.3043\n",
      "Epoch [89/300], Step [200/735], Loss: 2.2689\n",
      "Epoch [89/300], Step [400/735], Loss: 2.2743\n",
      "Epoch [89/300], Step [600/735], Loss: 2.3024\n",
      "Epoch [90/300], Step [200/735], Loss: 2.3061\n",
      "Epoch [90/300], Step [400/735], Loss: 2.3173\n",
      "Epoch [90/300], Step [600/735], Loss: 2.3186\n",
      "Epoch [91/300], Step [200/735], Loss: 2.3142\n",
      "Epoch [91/300], Step [400/735], Loss: 2.2927\n",
      "Epoch [91/300], Step [600/735], Loss: 2.3004\n",
      "Epoch [92/300], Step [200/735], Loss: 2.3246\n",
      "Epoch [92/300], Step [400/735], Loss: 2.3165\n",
      "Epoch [92/300], Step [600/735], Loss: 2.2793\n",
      "Epoch [93/300], Step [200/735], Loss: 2.3137\n",
      "Epoch [93/300], Step [400/735], Loss: 2.3002\n",
      "Epoch [93/300], Step [600/735], Loss: 2.3249\n",
      "Epoch [94/300], Step [200/735], Loss: 2.2961\n",
      "Epoch [94/300], Step [400/735], Loss: 2.2972\n",
      "Epoch [94/300], Step [600/735], Loss: 2.3054\n",
      "Epoch [95/300], Step [200/735], Loss: 2.3091\n",
      "Epoch [95/300], Step [400/735], Loss: 2.2969\n",
      "Epoch [95/300], Step [600/735], Loss: 2.3120\n",
      "Epoch [96/300], Step [200/735], Loss: 2.3107\n",
      "Epoch [96/300], Step [400/735], Loss: 2.2976\n",
      "Epoch [96/300], Step [600/735], Loss: 2.2747\n",
      "Epoch [97/300], Step [200/735], Loss: 2.3081\n",
      "Epoch [97/300], Step [400/735], Loss: 2.3040\n",
      "Epoch [97/300], Step [600/735], Loss: 2.2891\n",
      "Epoch [98/300], Step [200/735], Loss: 2.2824\n",
      "Epoch [98/300], Step [400/735], Loss: 2.3157\n",
      "Epoch [98/300], Step [600/735], Loss: 2.2989\n",
      "Epoch [99/300], Step [200/735], Loss: 2.3058\n",
      "Epoch [99/300], Step [400/735], Loss: 2.2865\n",
      "Epoch [99/300], Step [600/735], Loss: 2.2983\n",
      "Epoch [100/300], Step [200/735], Loss: 2.3353\n",
      "Epoch [100/300], Step [400/735], Loss: 2.3093\n",
      "Epoch [100/300], Step [600/735], Loss: 2.3157\n",
      "Epoch [101/300], Step [200/735], Loss: 2.2908\n",
      "Epoch [101/300], Step [400/735], Loss: 2.3055\n",
      "Epoch [101/300], Step [600/735], Loss: 2.3076\n",
      "Epoch [102/300], Step [200/735], Loss: 2.3176\n",
      "Epoch [102/300], Step [400/735], Loss: 2.3009\n",
      "Epoch [102/300], Step [600/735], Loss: 2.2970\n",
      "Epoch [103/300], Step [200/735], Loss: 2.2869\n",
      "Epoch [103/300], Step [400/735], Loss: 2.2876\n",
      "Epoch [103/300], Step [600/735], Loss: 2.2894\n",
      "Epoch [104/300], Step [200/735], Loss: 2.3199\n",
      "Epoch [104/300], Step [400/735], Loss: 2.2980\n",
      "Epoch [104/300], Step [600/735], Loss: 2.2947\n",
      "Epoch [105/300], Step [200/735], Loss: 2.2705\n",
      "Epoch [105/300], Step [400/735], Loss: 2.3019\n",
      "Epoch [105/300], Step [600/735], Loss: 2.2985\n",
      "Epoch [106/300], Step [200/735], Loss: 2.2995\n",
      "Epoch [106/300], Step [400/735], Loss: 2.3213\n",
      "Epoch [106/300], Step [600/735], Loss: 2.3107\n",
      "Epoch [107/300], Step [200/735], Loss: 2.3180\n",
      "Epoch [107/300], Step [400/735], Loss: 2.3148\n",
      "Epoch [107/300], Step [600/735], Loss: 2.2898\n",
      "Epoch [108/300], Step [200/735], Loss: 2.3117\n",
      "Epoch [108/300], Step [400/735], Loss: 2.2919\n",
      "Epoch [108/300], Step [600/735], Loss: 2.2909\n",
      "Epoch [109/300], Step [200/735], Loss: 2.2759\n",
      "Epoch [109/300], Step [400/735], Loss: 2.3127\n",
      "Epoch [109/300], Step [600/735], Loss: 2.3094\n",
      "Epoch [110/300], Step [200/735], Loss: 2.3291\n",
      "Epoch [110/300], Step [400/735], Loss: 2.3015\n",
      "Epoch [110/300], Step [600/735], Loss: 2.3056\n",
      "Epoch [111/300], Step [200/735], Loss: 2.2988\n",
      "Epoch [111/300], Step [400/735], Loss: 2.3036\n",
      "Epoch [111/300], Step [600/735], Loss: 2.2959\n",
      "Epoch [112/300], Step [200/735], Loss: 2.2875\n",
      "Epoch [112/300], Step [400/735], Loss: 2.3183\n",
      "Epoch [112/300], Step [600/735], Loss: 2.2869\n",
      "Epoch [113/300], Step [200/735], Loss: 2.3085\n",
      "Epoch [113/300], Step [400/735], Loss: 2.2851\n",
      "Epoch [113/300], Step [600/735], Loss: 2.3254\n",
      "Epoch [114/300], Step [200/735], Loss: 2.3085\n",
      "Epoch [114/300], Step [400/735], Loss: 2.3039\n",
      "Epoch [114/300], Step [600/735], Loss: 2.2852\n",
      "Epoch [115/300], Step [200/735], Loss: 2.2842\n",
      "Epoch [115/300], Step [400/735], Loss: 2.2932\n",
      "Epoch [115/300], Step [600/735], Loss: 2.2996\n",
      "Epoch [116/300], Step [200/735], Loss: 2.3117\n",
      "Epoch [116/300], Step [400/735], Loss: 2.3174\n",
      "Epoch [116/300], Step [600/735], Loss: 2.2921\n",
      "Epoch [117/300], Step [200/735], Loss: 2.3022\n",
      "Epoch [117/300], Step [400/735], Loss: 2.2846\n",
      "Epoch [117/300], Step [600/735], Loss: 2.2773\n",
      "Epoch [118/300], Step [200/735], Loss: 2.2896\n",
      "Epoch [118/300], Step [400/735], Loss: 2.2798\n",
      "Epoch [118/300], Step [600/735], Loss: 2.3314\n",
      "Epoch [119/300], Step [200/735], Loss: 2.3115\n",
      "Epoch [119/300], Step [400/735], Loss: 2.3254\n",
      "Epoch [119/300], Step [600/735], Loss: 2.2825\n",
      "Epoch [120/300], Step [200/735], Loss: 2.3086\n",
      "Epoch [120/300], Step [400/735], Loss: 2.2582\n",
      "Epoch [120/300], Step [600/735], Loss: 2.2821\n",
      "Epoch [121/300], Step [200/735], Loss: 2.2889\n",
      "Epoch [121/300], Step [400/735], Loss: 2.3018\n",
      "Epoch [121/300], Step [600/735], Loss: 2.2866\n",
      "Epoch [122/300], Step [200/735], Loss: 2.2881\n",
      "Epoch [122/300], Step [400/735], Loss: 2.2918\n",
      "Epoch [122/300], Step [600/735], Loss: 2.2985\n",
      "Epoch [123/300], Step [200/735], Loss: 2.3303\n",
      "Epoch [123/300], Step [400/735], Loss: 2.3139\n",
      "Epoch [123/300], Step [600/735], Loss: 2.3245\n",
      "Epoch [124/300], Step [200/735], Loss: 2.2877\n",
      "Epoch [124/300], Step [400/735], Loss: 2.3080\n",
      "Epoch [124/300], Step [600/735], Loss: 2.2872\n",
      "Epoch [125/300], Step [200/735], Loss: 2.3280\n",
      "Epoch [125/300], Step [400/735], Loss: 2.3279\n",
      "Epoch [125/300], Step [600/735], Loss: 2.3058\n",
      "Epoch [126/300], Step [200/735], Loss: 2.3012\n",
      "Epoch [126/300], Step [400/735], Loss: 2.2843\n",
      "Epoch [126/300], Step [600/735], Loss: 2.2871\n",
      "Epoch [127/300], Step [200/735], Loss: 2.2835\n",
      "Epoch [127/300], Step [400/735], Loss: 2.3034\n",
      "Epoch [127/300], Step [600/735], Loss: 2.2829\n",
      "Epoch [128/300], Step [200/735], Loss: 2.3010\n",
      "Epoch [128/300], Step [400/735], Loss: 2.3172\n",
      "Epoch [128/300], Step [600/735], Loss: 2.3120\n",
      "Epoch [129/300], Step [200/735], Loss: 2.3189\n",
      "Epoch [129/300], Step [400/735], Loss: 2.3245\n",
      "Epoch [129/300], Step [600/735], Loss: 2.2990\n",
      "Epoch [130/300], Step [200/735], Loss: 2.2880\n",
      "Epoch [130/300], Step [400/735], Loss: 2.2859\n",
      "Epoch [130/300], Step [600/735], Loss: 2.3168\n",
      "Epoch [131/300], Step [200/735], Loss: 2.2934\n",
      "Epoch [131/300], Step [400/735], Loss: 2.2939\n",
      "Epoch [131/300], Step [600/735], Loss: 2.2808\n",
      "Epoch [132/300], Step [200/735], Loss: 2.3051\n",
      "Epoch [132/300], Step [400/735], Loss: 2.2942\n",
      "Epoch [132/300], Step [600/735], Loss: 2.3055\n",
      "Epoch [133/300], Step [200/735], Loss: 2.2827\n",
      "Epoch [133/300], Step [400/735], Loss: 2.3194\n",
      "Epoch [133/300], Step [600/735], Loss: 2.3187\n",
      "Epoch [134/300], Step [200/735], Loss: 2.2956\n",
      "Epoch [134/300], Step [400/735], Loss: 2.3266\n",
      "Epoch [134/300], Step [600/735], Loss: 2.3147\n",
      "Epoch [135/300], Step [200/735], Loss: 2.2945\n",
      "Epoch [135/300], Step [400/735], Loss: 2.2758\n",
      "Epoch [135/300], Step [600/735], Loss: 2.3170\n",
      "Epoch [136/300], Step [200/735], Loss: 2.3100\n",
      "Epoch [136/300], Step [400/735], Loss: 2.2982\n",
      "Epoch [136/300], Step [600/735], Loss: 2.3147\n",
      "Epoch [137/300], Step [200/735], Loss: 2.2915\n",
      "Epoch [137/300], Step [400/735], Loss: 2.2834\n",
      "Epoch [137/300], Step [600/735], Loss: 2.2988\n",
      "Epoch [138/300], Step [200/735], Loss: 2.2748\n",
      "Epoch [138/300], Step [400/735], Loss: 2.2687\n",
      "Epoch [138/300], Step [600/735], Loss: 2.3155\n",
      "Epoch [139/300], Step [200/735], Loss: 2.3050\n",
      "Epoch [139/300], Step [400/735], Loss: 2.2983\n",
      "Epoch [139/300], Step [600/735], Loss: 2.3154\n",
      "Epoch [140/300], Step [200/735], Loss: 2.3222\n",
      "Epoch [140/300], Step [400/735], Loss: 2.2975\n",
      "Epoch [140/300], Step [600/735], Loss: 2.3028\n",
      "Epoch [141/300], Step [200/735], Loss: 2.2870\n",
      "Epoch [141/300], Step [400/735], Loss: 2.2945\n",
      "Epoch [141/300], Step [600/735], Loss: 2.2940\n",
      "Epoch [142/300], Step [200/735], Loss: 2.3107\n",
      "Epoch [142/300], Step [400/735], Loss: 2.2879\n",
      "Epoch [142/300], Step [600/735], Loss: 2.2985\n",
      "Epoch [143/300], Step [200/735], Loss: 2.3145\n",
      "Epoch [143/300], Step [400/735], Loss: 2.2840\n",
      "Epoch [143/300], Step [600/735], Loss: 2.2990\n",
      "Epoch [144/300], Step [200/735], Loss: 2.2977\n",
      "Epoch [144/300], Step [400/735], Loss: 2.2965\n",
      "Epoch [144/300], Step [600/735], Loss: 2.2865\n",
      "Epoch [145/300], Step [200/735], Loss: 2.3072\n",
      "Epoch [145/300], Step [400/735], Loss: 2.2754\n",
      "Epoch [145/300], Step [600/735], Loss: 2.2797\n",
      "Epoch [146/300], Step [200/735], Loss: 2.3024\n",
      "Epoch [146/300], Step [400/735], Loss: 2.3106\n",
      "Epoch [146/300], Step [600/735], Loss: 2.3245\n",
      "Epoch [147/300], Step [200/735], Loss: 2.2758\n",
      "Epoch [147/300], Step [400/735], Loss: 2.3073\n",
      "Epoch [147/300], Step [600/735], Loss: 2.2973\n",
      "Epoch [148/300], Step [200/735], Loss: 2.3039\n",
      "Epoch [148/300], Step [400/735], Loss: 2.2841\n",
      "Epoch [148/300], Step [600/735], Loss: 2.3110\n",
      "Epoch [149/300], Step [200/735], Loss: 2.3029\n",
      "Epoch [149/300], Step [400/735], Loss: 2.2466\n",
      "Epoch [149/300], Step [600/735], Loss: 2.2623\n",
      "Epoch [150/300], Step [200/735], Loss: 2.2948\n",
      "Epoch [150/300], Step [400/735], Loss: 2.3090\n",
      "Epoch [150/300], Step [600/735], Loss: 2.2582\n",
      "Epoch [151/300], Step [200/735], Loss: 2.2757\n",
      "Epoch [151/300], Step [400/735], Loss: 2.3034\n",
      "Epoch [151/300], Step [600/735], Loss: 2.2943\n",
      "Epoch [152/300], Step [200/735], Loss: 2.2993\n",
      "Epoch [152/300], Step [400/735], Loss: 2.2349\n",
      "Epoch [152/300], Step [600/735], Loss: 2.2912\n",
      "Epoch [153/300], Step [200/735], Loss: 2.3307\n",
      "Epoch [153/300], Step [400/735], Loss: 2.2515\n",
      "Epoch [153/300], Step [600/735], Loss: 2.2663\n",
      "Epoch [154/300], Step [200/735], Loss: 2.3063\n",
      "Epoch [154/300], Step [400/735], Loss: 2.2668\n",
      "Epoch [154/300], Step [600/735], Loss: 2.2243\n",
      "Epoch [155/300], Step [200/735], Loss: 2.2781\n",
      "Epoch [155/300], Step [400/735], Loss: 2.2941\n",
      "Epoch [155/300], Step [600/735], Loss: 2.2127\n",
      "Epoch [156/300], Step [200/735], Loss: 2.2608\n",
      "Epoch [156/300], Step [400/735], Loss: 2.2170\n",
      "Epoch [156/300], Step [600/735], Loss: 2.2810\n",
      "Epoch [157/300], Step [200/735], Loss: 2.2865\n",
      "Epoch [157/300], Step [400/735], Loss: 2.2828\n",
      "Epoch [157/300], Step [600/735], Loss: 2.2964\n",
      "Epoch [158/300], Step [200/735], Loss: 2.2320\n",
      "Epoch [158/300], Step [400/735], Loss: 2.2383\n",
      "Epoch [158/300], Step [600/735], Loss: 2.2901\n",
      "Epoch [159/300], Step [200/735], Loss: 2.2764\n",
      "Epoch [159/300], Step [400/735], Loss: 2.3036\n",
      "Epoch [159/300], Step [600/735], Loss: 2.2546\n",
      "Epoch [160/300], Step [200/735], Loss: 2.3043\n",
      "Epoch [160/300], Step [400/735], Loss: 2.2136\n",
      "Epoch [160/300], Step [600/735], Loss: 2.2601\n",
      "Epoch [161/300], Step [200/735], Loss: 2.2132\n",
      "Epoch [161/300], Step [400/735], Loss: 2.3407\n",
      "Epoch [161/300], Step [600/735], Loss: 2.1822\n",
      "Epoch [162/300], Step [200/735], Loss: 2.3331\n",
      "Epoch [162/300], Step [400/735], Loss: 2.2637\n",
      "Epoch [162/300], Step [600/735], Loss: 2.2204\n",
      "Epoch [163/300], Step [200/735], Loss: 2.3291\n",
      "Epoch [163/300], Step [400/735], Loss: 2.2496\n",
      "Epoch [163/300], Step [600/735], Loss: 2.2411\n",
      "Epoch [164/300], Step [200/735], Loss: 2.2343\n",
      "Epoch [164/300], Step [400/735], Loss: 2.2943\n",
      "Epoch [164/300], Step [600/735], Loss: 2.2510\n",
      "Epoch [165/300], Step [200/735], Loss: 2.1799\n",
      "Epoch [165/300], Step [400/735], Loss: 2.2383\n",
      "Epoch [165/300], Step [600/735], Loss: 2.0841\n",
      "Epoch [166/300], Step [200/735], Loss: 2.2671\n",
      "Epoch [166/300], Step [400/735], Loss: 2.0555\n",
      "Epoch [166/300], Step [600/735], Loss: 2.2338\n",
      "Epoch [167/300], Step [200/735], Loss: 2.2687\n",
      "Epoch [167/300], Step [400/735], Loss: 2.2537\n",
      "Epoch [167/300], Step [600/735], Loss: 2.4209\n",
      "Epoch [168/300], Step [200/735], Loss: 2.1168\n",
      "Epoch [168/300], Step [400/735], Loss: 2.1766\n",
      "Epoch [168/300], Step [600/735], Loss: 1.9757\n",
      "Epoch [169/300], Step [200/735], Loss: 1.6632\n",
      "Epoch [169/300], Step [400/735], Loss: 2.1306\n",
      "Epoch [169/300], Step [600/735], Loss: 2.1082\n",
      "Epoch [170/300], Step [200/735], Loss: 2.2406\n",
      "Epoch [170/300], Step [400/735], Loss: 1.8689\n",
      "Epoch [170/300], Step [600/735], Loss: 1.7142\n",
      "Epoch [171/300], Step [200/735], Loss: 2.2599\n",
      "Epoch [171/300], Step [400/735], Loss: 1.9049\n",
      "Epoch [171/300], Step [600/735], Loss: 2.0062\n",
      "Epoch [172/300], Step [200/735], Loss: 2.2278\n",
      "Epoch [172/300], Step [400/735], Loss: 1.8174\n",
      "Epoch [172/300], Step [600/735], Loss: 2.3141\n",
      "Epoch [173/300], Step [200/735], Loss: 1.9017\n",
      "Epoch [173/300], Step [400/735], Loss: 2.0280\n",
      "Epoch [173/300], Step [600/735], Loss: 2.0242\n",
      "Epoch [174/300], Step [200/735], Loss: 2.0374\n",
      "Epoch [174/300], Step [400/735], Loss: 1.9155\n",
      "Epoch [174/300], Step [600/735], Loss: 1.9170\n",
      "Epoch [175/300], Step [200/735], Loss: 2.1729\n",
      "Epoch [175/300], Step [400/735], Loss: 2.1929\n",
      "Epoch [175/300], Step [600/735], Loss: 1.5420\n",
      "Epoch [176/300], Step [200/735], Loss: 1.8495\n",
      "Epoch [176/300], Step [400/735], Loss: 1.9203\n",
      "Epoch [176/300], Step [600/735], Loss: 1.4792\n",
      "Epoch [177/300], Step [200/735], Loss: 2.1943\n",
      "Epoch [177/300], Step [400/735], Loss: 1.5058\n",
      "Epoch [177/300], Step [600/735], Loss: 1.7061\n",
      "Epoch [178/300], Step [200/735], Loss: 1.1912\n",
      "Epoch [178/300], Step [400/735], Loss: 1.0881\n",
      "Epoch [178/300], Step [600/735], Loss: 1.4811\n",
      "Epoch [179/300], Step [200/735], Loss: 2.4527\n",
      "Epoch [179/300], Step [400/735], Loss: 2.0135\n",
      "Epoch [179/300], Step [600/735], Loss: 1.9594\n",
      "Epoch [180/300], Step [200/735], Loss: 1.5952\n",
      "Epoch [180/300], Step [400/735], Loss: 1.7872\n",
      "Epoch [180/300], Step [600/735], Loss: 1.8424\n",
      "Epoch [181/300], Step [200/735], Loss: 2.0605\n",
      "Epoch [181/300], Step [400/735], Loss: 1.7116\n",
      "Epoch [181/300], Step [600/735], Loss: 1.3164\n",
      "Epoch [182/300], Step [200/735], Loss: 1.2850\n",
      "Epoch [182/300], Step [400/735], Loss: 1.4506\n",
      "Epoch [182/300], Step [600/735], Loss: 1.7990\n",
      "Epoch [183/300], Step [200/735], Loss: 1.1839\n",
      "Epoch [183/300], Step [400/735], Loss: 1.7688\n",
      "Epoch [183/300], Step [600/735], Loss: 1.4893\n",
      "Epoch [184/300], Step [200/735], Loss: 1.1023\n",
      "Epoch [184/300], Step [400/735], Loss: 1.5255\n",
      "Epoch [184/300], Step [600/735], Loss: 2.1302\n",
      "Epoch [185/300], Step [200/735], Loss: 1.8122\n",
      "Epoch [185/300], Step [400/735], Loss: 1.4842\n",
      "Epoch [185/300], Step [600/735], Loss: 1.1391\n",
      "Epoch [186/300], Step [200/735], Loss: 1.6501\n",
      "Epoch [186/300], Step [400/735], Loss: 1.6019\n",
      "Epoch [186/300], Step [600/735], Loss: 1.6613\n",
      "Epoch [187/300], Step [200/735], Loss: 1.3470\n",
      "Epoch [187/300], Step [400/735], Loss: 1.9650\n",
      "Epoch [187/300], Step [600/735], Loss: 1.1769\n",
      "Epoch [188/300], Step [200/735], Loss: 1.7522\n",
      "Epoch [188/300], Step [400/735], Loss: 0.8749\n",
      "Epoch [188/300], Step [600/735], Loss: 1.9638\n",
      "Epoch [189/300], Step [200/735], Loss: 2.1327\n",
      "Epoch [189/300], Step [400/735], Loss: 1.5086\n",
      "Epoch [189/300], Step [600/735], Loss: 2.5660\n",
      "Epoch [190/300], Step [200/735], Loss: 2.2459\n",
      "Epoch [190/300], Step [400/735], Loss: 1.3620\n",
      "Epoch [190/300], Step [600/735], Loss: 1.0274\n",
      "Epoch [191/300], Step [200/735], Loss: 1.4330\n",
      "Epoch [191/300], Step [400/735], Loss: 1.8549\n",
      "Epoch [191/300], Step [600/735], Loss: 0.8539\n",
      "Epoch [192/300], Step [200/735], Loss: 1.0688\n",
      "Epoch [192/300], Step [400/735], Loss: 1.4194\n",
      "Epoch [192/300], Step [600/735], Loss: 1.6822\n",
      "Epoch [193/300], Step [200/735], Loss: 1.3003\n",
      "Epoch [193/300], Step [400/735], Loss: 1.3669\n",
      "Epoch [193/300], Step [600/735], Loss: 1.3753\n",
      "Epoch [194/300], Step [200/735], Loss: 1.0952\n",
      "Epoch [194/300], Step [400/735], Loss: 0.9169\n",
      "Epoch [194/300], Step [600/735], Loss: 1.5964\n",
      "Epoch [195/300], Step [200/735], Loss: 1.7939\n",
      "Epoch [195/300], Step [400/735], Loss: 1.6377\n",
      "Epoch [195/300], Step [600/735], Loss: 1.0954\n",
      "Epoch [196/300], Step [200/735], Loss: 1.4509\n",
      "Epoch [196/300], Step [400/735], Loss: 1.1043\n",
      "Epoch [196/300], Step [600/735], Loss: 1.6095\n",
      "Epoch [197/300], Step [200/735], Loss: 1.3124\n",
      "Epoch [197/300], Step [400/735], Loss: 1.2589\n",
      "Epoch [197/300], Step [600/735], Loss: 2.5387\n",
      "Epoch [198/300], Step [200/735], Loss: 1.9233\n",
      "Epoch [198/300], Step [400/735], Loss: 1.9366\n",
      "Epoch [198/300], Step [600/735], Loss: 2.0765\n",
      "Epoch [199/300], Step [200/735], Loss: 1.2440\n",
      "Epoch [199/300], Step [400/735], Loss: 1.0254\n",
      "Epoch [199/300], Step [600/735], Loss: 0.9761\n",
      "Epoch [200/300], Step [200/735], Loss: 1.3182\n",
      "Epoch [200/300], Step [400/735], Loss: 1.8636\n",
      "Epoch [200/300], Step [600/735], Loss: 2.0338\n",
      "Epoch [201/300], Step [200/735], Loss: 1.4831\n",
      "Epoch [201/300], Step [400/735], Loss: 1.4971\n",
      "Epoch [201/300], Step [600/735], Loss: 1.6339\n",
      "Epoch [202/300], Step [200/735], Loss: 2.1174\n",
      "Epoch [202/300], Step [400/735], Loss: 1.1514\n",
      "Epoch [202/300], Step [600/735], Loss: 1.5086\n",
      "Epoch [203/300], Step [200/735], Loss: 1.4661\n",
      "Epoch [203/300], Step [400/735], Loss: 1.1135\n",
      "Epoch [203/300], Step [600/735], Loss: 1.1904\n",
      "Epoch [204/300], Step [200/735], Loss: 1.6947\n",
      "Epoch [204/300], Step [400/735], Loss: 1.2556\n",
      "Epoch [204/300], Step [600/735], Loss: 0.4645\n",
      "Epoch [205/300], Step [200/735], Loss: 0.8895\n",
      "Epoch [205/300], Step [400/735], Loss: 1.6933\n",
      "Epoch [205/300], Step [600/735], Loss: 0.7998\n",
      "Epoch [206/300], Step [200/735], Loss: 1.3511\n",
      "Epoch [206/300], Step [400/735], Loss: 2.0143\n",
      "Epoch [206/300], Step [600/735], Loss: 0.3601\n",
      "Epoch [207/300], Step [200/735], Loss: 2.6990\n",
      "Epoch [207/300], Step [400/735], Loss: 0.5703\n",
      "Epoch [207/300], Step [600/735], Loss: 0.6185\n",
      "Epoch [208/300], Step [200/735], Loss: 1.4187\n",
      "Epoch [208/300], Step [400/735], Loss: 0.8905\n",
      "Epoch [208/300], Step [600/735], Loss: 1.1026\n",
      "Epoch [209/300], Step [200/735], Loss: 1.4433\n",
      "Epoch [209/300], Step [400/735], Loss: 1.2213\n",
      "Epoch [209/300], Step [600/735], Loss: 1.6764\n",
      "Epoch [210/300], Step [200/735], Loss: 0.7244\n",
      "Epoch [210/300], Step [400/735], Loss: 1.1962\n",
      "Epoch [210/300], Step [600/735], Loss: 1.3185\n",
      "Epoch [211/300], Step [200/735], Loss: 2.0514\n",
      "Epoch [211/300], Step [400/735], Loss: 0.7518\n",
      "Epoch [211/300], Step [600/735], Loss: 0.6749\n",
      "Epoch [212/300], Step [200/735], Loss: 0.9378\n",
      "Epoch [212/300], Step [400/735], Loss: 1.1441\n",
      "Epoch [212/300], Step [600/735], Loss: 1.0214\n",
      "Epoch [213/300], Step [200/735], Loss: 1.5750\n",
      "Epoch [213/300], Step [400/735], Loss: 1.2086\n",
      "Epoch [213/300], Step [600/735], Loss: 2.2652\n",
      "Epoch [214/300], Step [200/735], Loss: 0.3628\n",
      "Epoch [214/300], Step [400/735], Loss: 0.2760\n",
      "Epoch [214/300], Step [600/735], Loss: 0.8304\n",
      "Epoch [215/300], Step [200/735], Loss: 1.0583\n",
      "Epoch [215/300], Step [400/735], Loss: 0.5501\n",
      "Epoch [215/300], Step [600/735], Loss: 1.2705\n",
      "Epoch [216/300], Step [200/735], Loss: 0.8817\n",
      "Epoch [216/300], Step [400/735], Loss: 1.3758\n",
      "Epoch [216/300], Step [600/735], Loss: 1.5938\n",
      "Epoch [217/300], Step [200/735], Loss: 0.5918\n",
      "Epoch [217/300], Step [400/735], Loss: 1.5214\n",
      "Epoch [217/300], Step [600/735], Loss: 0.3054\n",
      "Epoch [218/300], Step [200/735], Loss: 0.7980\n",
      "Epoch [218/300], Step [400/735], Loss: 1.0972\n",
      "Epoch [218/300], Step [600/735], Loss: 0.3727\n",
      "Epoch [219/300], Step [200/735], Loss: 0.6875\n",
      "Epoch [219/300], Step [400/735], Loss: 1.0109\n",
      "Epoch [219/300], Step [600/735], Loss: 1.3725\n",
      "Epoch [220/300], Step [200/735], Loss: 1.1551\n",
      "Epoch [220/300], Step [400/735], Loss: 0.7854\n",
      "Epoch [220/300], Step [600/735], Loss: 1.3884\n",
      "Epoch [221/300], Step [200/735], Loss: 0.8461\n",
      "Epoch [221/300], Step [400/735], Loss: 0.8152\n",
      "Epoch [221/300], Step [600/735], Loss: 0.6858\n",
      "Epoch [222/300], Step [200/735], Loss: 1.1803\n",
      "Epoch [222/300], Step [400/735], Loss: 1.6985\n",
      "Epoch [222/300], Step [600/735], Loss: 0.8141\n",
      "Epoch [223/300], Step [200/735], Loss: 0.4555\n",
      "Epoch [223/300], Step [400/735], Loss: 1.3947\n",
      "Epoch [223/300], Step [600/735], Loss: 0.4673\n",
      "Epoch [224/300], Step [200/735], Loss: 1.1547\n",
      "Epoch [224/300], Step [400/735], Loss: 0.6428\n",
      "Epoch [224/300], Step [600/735], Loss: 0.9935\n",
      "Epoch [225/300], Step [200/735], Loss: 0.7122\n",
      "Epoch [225/300], Step [400/735], Loss: 0.7043\n",
      "Epoch [225/300], Step [600/735], Loss: 1.1344\n",
      "Epoch [226/300], Step [200/735], Loss: 1.2091\n",
      "Epoch [226/300], Step [400/735], Loss: 3.2983\n",
      "Epoch [226/300], Step [600/735], Loss: 0.7535\n",
      "Epoch [227/300], Step [200/735], Loss: 2.2495\n",
      "Epoch [227/300], Step [400/735], Loss: 1.4712\n",
      "Epoch [227/300], Step [600/735], Loss: 1.0260\n",
      "Epoch [228/300], Step [200/735], Loss: 1.1019\n",
      "Epoch [228/300], Step [400/735], Loss: 1.1478\n",
      "Epoch [228/300], Step [600/735], Loss: 0.9333\n",
      "Epoch [229/300], Step [200/735], Loss: 4.4980\n",
      "Epoch [229/300], Step [400/735], Loss: 3.4310\n",
      "Epoch [229/300], Step [600/735], Loss: 0.5967\n",
      "Epoch [230/300], Step [200/735], Loss: 1.2404\n",
      "Epoch [230/300], Step [400/735], Loss: 1.3089\n",
      "Epoch [230/300], Step [600/735], Loss: 0.7597\n",
      "Epoch [231/300], Step [200/735], Loss: 0.8297\n",
      "Epoch [231/300], Step [400/735], Loss: 0.7301\n",
      "Epoch [231/300], Step [600/735], Loss: 0.6628\n",
      "Epoch [232/300], Step [200/735], Loss: 0.7347\n",
      "Epoch [232/300], Step [400/735], Loss: 1.2631\n",
      "Epoch [232/300], Step [600/735], Loss: 0.7534\n",
      "Epoch [233/300], Step [200/735], Loss: 1.2784\n",
      "Epoch [233/300], Step [400/735], Loss: 0.6897\n",
      "Epoch [233/300], Step [600/735], Loss: 0.4111\n",
      "Epoch [234/300], Step [200/735], Loss: 0.3332\n",
      "Epoch [234/300], Step [400/735], Loss: 1.0292\n",
      "Epoch [234/300], Step [600/735], Loss: 1.1554\n",
      "Epoch [235/300], Step [200/735], Loss: 0.9348\n",
      "Epoch [235/300], Step [400/735], Loss: 0.6903\n",
      "Epoch [235/300], Step [600/735], Loss: 1.2690\n",
      "Epoch [236/300], Step [200/735], Loss: 1.9985\n",
      "Epoch [236/300], Step [400/735], Loss: 0.7791\n",
      "Epoch [236/300], Step [600/735], Loss: 1.3857\n",
      "Epoch [237/300], Step [200/735], Loss: 3.1456\n",
      "Epoch [237/300], Step [400/735], Loss: 1.1625\n",
      "Epoch [237/300], Step [600/735], Loss: 1.2208\n",
      "Epoch [238/300], Step [200/735], Loss: 1.7225\n",
      "Epoch [238/300], Step [400/735], Loss: 1.1048\n",
      "Epoch [238/300], Step [600/735], Loss: 0.3550\n",
      "Epoch [239/300], Step [200/735], Loss: 0.9161\n",
      "Epoch [239/300], Step [400/735], Loss: 0.8382\n",
      "Epoch [239/300], Step [600/735], Loss: 0.7703\n",
      "Epoch [240/300], Step [200/735], Loss: 2.6647\n",
      "Epoch [240/300], Step [400/735], Loss: 0.2870\n",
      "Epoch [240/300], Step [600/735], Loss: 0.3167\n",
      "Epoch [241/300], Step [200/735], Loss: 0.6147\n",
      "Epoch [241/300], Step [400/735], Loss: 0.5890\n",
      "Epoch [241/300], Step [600/735], Loss: 0.8284\n",
      "Epoch [242/300], Step [200/735], Loss: 1.3236\n",
      "Epoch [242/300], Step [400/735], Loss: 1.0688\n",
      "Epoch [242/300], Step [600/735], Loss: 0.3547\n",
      "Epoch [243/300], Step [200/735], Loss: 0.7777\n",
      "Epoch [243/300], Step [400/735], Loss: 1.9011\n",
      "Epoch [243/300], Step [600/735], Loss: 0.5776\n",
      "Epoch [244/300], Step [200/735], Loss: 0.6957\n",
      "Epoch [244/300], Step [400/735], Loss: 0.8849\n",
      "Epoch [244/300], Step [600/735], Loss: 0.9351\n",
      "Epoch [245/300], Step [200/735], Loss: 1.0406\n",
      "Epoch [245/300], Step [400/735], Loss: 1.1557\n",
      "Epoch [245/300], Step [600/735], Loss: 0.1237\n",
      "Epoch [246/300], Step [200/735], Loss: 1.4045\n",
      "Epoch [246/300], Step [400/735], Loss: 1.0809\n",
      "Epoch [246/300], Step [600/735], Loss: 0.7481\n",
      "Epoch [247/300], Step [200/735], Loss: 1.2204\n",
      "Epoch [247/300], Step [400/735], Loss: 0.8778\n",
      "Epoch [247/300], Step [600/735], Loss: 2.3452\n",
      "Epoch [248/300], Step [200/735], Loss: 0.5307\n",
      "Epoch [248/300], Step [400/735], Loss: 0.2242\n",
      "Epoch [248/300], Step [600/735], Loss: 0.6604\n",
      "Epoch [249/300], Step [200/735], Loss: 1.2593\n",
      "Epoch [249/300], Step [400/735], Loss: 0.7643\n",
      "Epoch [249/300], Step [600/735], Loss: 0.8925\n",
      "Epoch [250/300], Step [200/735], Loss: 0.5915\n",
      "Epoch [250/300], Step [400/735], Loss: 1.1205\n",
      "Epoch [250/300], Step [600/735], Loss: 0.6692\n",
      "Epoch [251/300], Step [200/735], Loss: 1.0290\n",
      "Epoch [251/300], Step [400/735], Loss: 0.8220\n",
      "Epoch [251/300], Step [600/735], Loss: 0.1957\n",
      "Epoch [252/300], Step [200/735], Loss: 0.6444\n",
      "Epoch [252/300], Step [400/735], Loss: 0.5698\n",
      "Epoch [252/300], Step [600/735], Loss: 0.7686\n",
      "Epoch [253/300], Step [200/735], Loss: 1.8648\n",
      "Epoch [253/300], Step [400/735], Loss: 0.3736\n",
      "Epoch [253/300], Step [600/735], Loss: 0.9330\n",
      "Epoch [254/300], Step [200/735], Loss: 0.7327\n",
      "Epoch [254/300], Step [400/735], Loss: 0.2658\n",
      "Epoch [254/300], Step [600/735], Loss: 1.0295\n",
      "Epoch [255/300], Step [200/735], Loss: 0.5081\n",
      "Epoch [255/300], Step [400/735], Loss: 0.3199\n",
      "Epoch [255/300], Step [600/735], Loss: 1.8335\n",
      "Epoch [256/300], Step [200/735], Loss: 0.7232\n",
      "Epoch [256/300], Step [400/735], Loss: 1.2195\n",
      "Epoch [256/300], Step [600/735], Loss: 0.2169\n",
      "Epoch [257/300], Step [200/735], Loss: 1.2024\n",
      "Epoch [257/300], Step [400/735], Loss: 0.8689\n",
      "Epoch [257/300], Step [600/735], Loss: 0.8086\n",
      "Epoch [258/300], Step [200/735], Loss: 1.2703\n",
      "Epoch [258/300], Step [400/735], Loss: 0.4274\n",
      "Epoch [258/300], Step [600/735], Loss: 0.4769\n",
      "Epoch [259/300], Step [200/735], Loss: 0.6076\n",
      "Epoch [259/300], Step [400/735], Loss: 0.7739\n",
      "Epoch [259/300], Step [600/735], Loss: 0.5889\n",
      "Epoch [260/300], Step [200/735], Loss: 0.2859\n",
      "Epoch [260/300], Step [400/735], Loss: 1.0561\n",
      "Epoch [260/300], Step [600/735], Loss: 1.1741\n",
      "Epoch [261/300], Step [200/735], Loss: 0.6488\n",
      "Epoch [261/300], Step [400/735], Loss: 0.5384\n",
      "Epoch [261/300], Step [600/735], Loss: 1.0051\n",
      "Epoch [262/300], Step [200/735], Loss: 0.5839\n",
      "Epoch [262/300], Step [400/735], Loss: 0.3772\n",
      "Epoch [262/300], Step [600/735], Loss: 0.4040\n",
      "Epoch [263/300], Step [200/735], Loss: 1.5278\n",
      "Epoch [263/300], Step [400/735], Loss: 0.5177\n",
      "Epoch [263/300], Step [600/735], Loss: 0.4228\n",
      "Epoch [264/300], Step [200/735], Loss: 0.3686\n",
      "Epoch [264/300], Step [400/735], Loss: 0.1557\n",
      "Epoch [264/300], Step [600/735], Loss: 0.6677\n",
      "Epoch [265/300], Step [200/735], Loss: 0.7515\n",
      "Epoch [265/300], Step [400/735], Loss: 0.4365\n",
      "Epoch [265/300], Step [600/735], Loss: 0.4654\n",
      "Epoch [266/300], Step [200/735], Loss: 0.7144\n",
      "Epoch [266/300], Step [400/735], Loss: 1.1496\n",
      "Epoch [266/300], Step [600/735], Loss: 0.4421\n",
      "Epoch [267/300], Step [200/735], Loss: 0.7291\n",
      "Epoch [267/300], Step [400/735], Loss: 0.7931\n",
      "Epoch [267/300], Step [600/735], Loss: 0.4741\n",
      "Epoch [268/300], Step [200/735], Loss: 0.3719\n",
      "Epoch [268/300], Step [400/735], Loss: 2.0745\n",
      "Epoch [268/300], Step [600/735], Loss: 2.1016\n",
      "Epoch [269/300], Step [200/735], Loss: 1.0109\n",
      "Epoch [269/300], Step [400/735], Loss: 0.6883\n",
      "Epoch [269/300], Step [600/735], Loss: 1.4203\n",
      "Epoch [270/300], Step [200/735], Loss: 0.8846\n",
      "Epoch [270/300], Step [400/735], Loss: 0.6035\n",
      "Epoch [270/300], Step [600/735], Loss: 1.6613\n",
      "Epoch [271/300], Step [200/735], Loss: 1.1837\n",
      "Epoch [271/300], Step [400/735], Loss: 2.7184\n",
      "Epoch [271/300], Step [600/735], Loss: 1.0384\n",
      "Epoch [272/300], Step [200/735], Loss: 0.9186\n",
      "Epoch [272/300], Step [400/735], Loss: 0.9977\n",
      "Epoch [272/300], Step [600/735], Loss: 1.0977\n",
      "Epoch [273/300], Step [200/735], Loss: 0.4774\n",
      "Epoch [273/300], Step [400/735], Loss: 0.5402\n",
      "Epoch [273/300], Step [600/735], Loss: 0.4157\n",
      "Epoch [274/300], Step [200/735], Loss: 2.3930\n",
      "Epoch [274/300], Step [400/735], Loss: 0.5758\n",
      "Epoch [274/300], Step [600/735], Loss: 0.4595\n",
      "Epoch [275/300], Step [200/735], Loss: 1.4079\n",
      "Epoch [275/300], Step [400/735], Loss: 0.5414\n",
      "Epoch [275/300], Step [600/735], Loss: 0.3288\n",
      "Epoch [276/300], Step [200/735], Loss: 0.8685\n",
      "Epoch [276/300], Step [400/735], Loss: 0.2274\n",
      "Epoch [276/300], Step [600/735], Loss: 0.9289\n",
      "Epoch [277/300], Step [200/735], Loss: 0.4455\n",
      "Epoch [277/300], Step [400/735], Loss: 0.4250\n",
      "Epoch [277/300], Step [600/735], Loss: 0.2321\n",
      "Epoch [278/300], Step [200/735], Loss: 0.3023\n",
      "Epoch [278/300], Step [400/735], Loss: 0.1993\n",
      "Epoch [278/300], Step [600/735], Loss: 0.3225\n",
      "Epoch [279/300], Step [200/735], Loss: 0.4141\n",
      "Epoch [279/300], Step [400/735], Loss: 1.0004\n",
      "Epoch [279/300], Step [600/735], Loss: 0.8833\n",
      "Epoch [280/300], Step [200/735], Loss: 1.4387\n",
      "Epoch [280/300], Step [400/735], Loss: 2.0973\n",
      "Epoch [280/300], Step [600/735], Loss: 0.2327\n",
      "Epoch [281/300], Step [200/735], Loss: 0.7318\n",
      "Epoch [281/300], Step [400/735], Loss: 0.1585\n",
      "Epoch [281/300], Step [600/735], Loss: 0.6219\n",
      "Epoch [282/300], Step [200/735], Loss: 0.5307\n",
      "Epoch [282/300], Step [400/735], Loss: 0.6998\n",
      "Epoch [282/300], Step [600/735], Loss: 0.5787\n",
      "Epoch [283/300], Step [200/735], Loss: 1.1832\n",
      "Epoch [283/300], Step [400/735], Loss: 0.8640\n",
      "Epoch [283/300], Step [600/735], Loss: 0.8308\n",
      "Epoch [284/300], Step [200/735], Loss: 0.6794\n",
      "Epoch [284/300], Step [400/735], Loss: 0.1487\n",
      "Epoch [284/300], Step [600/735], Loss: 0.5750\n",
      "Epoch [285/300], Step [200/735], Loss: 0.4839\n",
      "Epoch [285/300], Step [400/735], Loss: 2.9939\n",
      "Epoch [285/300], Step [600/735], Loss: 0.2362\n",
      "Epoch [286/300], Step [200/735], Loss: 0.3672\n",
      "Epoch [286/300], Step [400/735], Loss: 0.5261\n",
      "Epoch [286/300], Step [600/735], Loss: 0.8852\n",
      "Epoch [287/300], Step [200/735], Loss: 0.1225\n",
      "Epoch [287/300], Step [400/735], Loss: 0.9010\n",
      "Epoch [287/300], Step [600/735], Loss: 0.6403\n",
      "Epoch [288/300], Step [200/735], Loss: 0.3847\n",
      "Epoch [288/300], Step [400/735], Loss: 1.6536\n",
      "Epoch [288/300], Step [600/735], Loss: 0.8462\n",
      "Epoch [289/300], Step [200/735], Loss: 1.0883\n",
      "Epoch [289/300], Step [400/735], Loss: 0.1842\n",
      "Epoch [289/300], Step [600/735], Loss: 0.5111\n",
      "Epoch [290/300], Step [200/735], Loss: 1.2111\n",
      "Epoch [290/300], Step [400/735], Loss: 0.4153\n",
      "Epoch [290/300], Step [600/735], Loss: 0.3473\n",
      "Epoch [291/300], Step [200/735], Loss: 0.4156\n",
      "Epoch [291/300], Step [400/735], Loss: 0.4233\n",
      "Epoch [291/300], Step [600/735], Loss: 0.7946\n",
      "Epoch [292/300], Step [200/735], Loss: 0.6808\n",
      "Epoch [292/300], Step [400/735], Loss: 0.6714\n",
      "Epoch [292/300], Step [600/735], Loss: 0.5832\n",
      "Epoch [293/300], Step [200/735], Loss: 0.4705\n",
      "Epoch [293/300], Step [400/735], Loss: 1.0874\n",
      "Epoch [293/300], Step [600/735], Loss: 2.6831\n",
      "Epoch [294/300], Step [200/735], Loss: 1.0352\n",
      "Epoch [294/300], Step [400/735], Loss: 0.7373\n",
      "Epoch [294/300], Step [600/735], Loss: 1.8598\n",
      "Epoch [295/300], Step [200/735], Loss: 0.7669\n",
      "Epoch [295/300], Step [400/735], Loss: 1.1442\n",
      "Epoch [295/300], Step [600/735], Loss: 0.6160\n",
      "Epoch [296/300], Step [200/735], Loss: 0.2306\n",
      "Epoch [296/300], Step [400/735], Loss: 0.5626\n",
      "Epoch [296/300], Step [600/735], Loss: 1.0603\n",
      "Epoch [297/300], Step [200/735], Loss: 1.5205\n",
      "Epoch [297/300], Step [400/735], Loss: 0.2644\n",
      "Epoch [297/300], Step [600/735], Loss: 0.8853\n",
      "Epoch [298/300], Step [200/735], Loss: 0.4619\n",
      "Epoch [298/300], Step [400/735], Loss: 0.3647\n",
      "Epoch [298/300], Step [600/735], Loss: 0.6921\n",
      "Epoch [299/300], Step [200/735], Loss: 1.1971\n",
      "Epoch [299/300], Step [400/735], Loss: 0.5287\n",
      "Epoch [299/300], Step [600/735], Loss: 0.0903\n",
      "Epoch [300/300], Step [200/735], Loss: 0.6655\n",
      "Epoch [300/300], Step [400/735], Loss: 0.6797\n",
      "Epoch [300/300], Step [600/735], Loss: 0.3978\n",
      "Finished Training\n",
      "Test accuracy\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Given groups=1, weight of size [128, 1, 3, 3], expected input[4, 3, 44, 6] to have 1 channels, but got 3 channels instead",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/2l/b3b8rt8j60b1bqtxxf00654c0000gn/T/ipykernel_21816/1858976084.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mOptimConvNet2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;31m#train_previous(model,100,0.001)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m300\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0.001\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0;31m#MODEL_PATH=os.path.join(\"checkpoint\",\"ConvNetFlex_ep90.pth\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;31m#train_from_load(model,\"cnn.pth\",200,0.001)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/2l/b3b8rt8j60b1bqtxxf00654c0000gn/T/ipykernel_21816/3536773406.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, num_epochs, learning_rate)\u001b[0m\n\u001b[1;32m     98\u001b[0m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPATH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'./cnn.pth'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m     \u001b[0mreport_accuracies\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlogFile\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mACC_LOG_PATH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    101\u001b[0m \u001b[0;31m#Load model and train\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtrain_from_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_object\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodelPath\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/FYP/code/main/test.py\u001b[0m in \u001b[0;36mreport_accuracies\u001b[0;34m(model, batch_size, logFile)\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[0mtrainloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Test accuracy\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m     \u001b[0mreport\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtestloader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'test'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mACC_LOG_PATH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Train accuracy\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m     \u001b[0mreport\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainloader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mACC_LOG_PATH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/FYP/code/main/test.py\u001b[0m in \u001b[0;36mreport\u001b[0;34m(dataloader, mode, logFile)\u001b[0m\n\u001b[1;32m     27\u001b[0m                 \u001b[0mimages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m                 \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m                 \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m                 \u001b[0;31m#print(outputs)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m                 \u001b[0;31m# max returns (value ,index)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/FYP/code/main/model_classes/convNet.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m         \u001b[0;31m# -> n, 3, 32, 32\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpool1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# -> n, 6, 14, 14\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    101\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpool2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# -> n, 16, 5, 5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m560\u001b[0m\u001b[0;34m)\u001b[0m            \u001b[0;31m# -> n, 400\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 457\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    458\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    451\u001b[0m                             \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    452\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[0;32m--> 453\u001b[0;31m         return F.conv2d(input, weight, bias, self.stride,\n\u001b[0m\u001b[1;32m    454\u001b[0m                         self.padding, self.dilation, self.groups)\n\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Given groups=1, weight of size [128, 1, 3, 3], expected input[4, 3, 44, 6] to have 1 channels, but got 3 channels instead"
     ]
    }
   ],
   "source": [
    "from customFunctions import *\n",
    "if __name__=='__main__':\n",
    "    print(device)\n",
    "    \n",
    "    train_dataset= torchvision.datasets.ImageFolder(TRAIN_IMAGE_PATH,transform=basicTransform,loader=custom_pil_loader)\n",
    "    test_dataset= torchvision.datasets.ImageFolder(TEST_IMAGE_PATH,transform=basicTransform)\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size,shuffle=True)\n",
    "    test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size,shuffle=True)\n",
    "    x=train_dataset[0][0].size()\n",
    "    print(x)\n",
    "    #model = ConvNet4(output_size=len(train_dataset.classes))\n",
    "    model=OptimConvNet2(output_size=10)\n",
    "    #train_previous(model,100,0.001)\n",
    "    train(model,300,0.001)\n",
    "    #MODEL_PATH=os.path.join(\"checkpoint\",\"ConvNetFlex_ep90.pth\")\n",
    "    #train_from_load(model,\"cnn.pth\",200,0.001)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78ccbd1c-528e-467e-b207-8d27e415d6de",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
