{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import TensorDataset, DataLoader,Dataset\n",
    "#import torchvision.datasets.ImageFolder \n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "batch_size=4\n",
    "\n",
    "\n",
    "# dataset has PILImage images of range [0, 1]. \n",
    "# We transform them to Tensors of normalized range [-1, 1]\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5), (0.5))])\n",
    "import os\n",
    "import pandas as pd\n",
    "from torchvision.io import read_image\n",
    "\n",
    "class CustomImageDataset(Dataset):\n",
    "    def __init__(self, annotations_file, img_dir, transform=None, target_transform=None):\n",
    "        self.img_labels = pd.read_csv(annotations_file)\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        img_path = os.path.join(self.img_dir, self.img_labels.iloc[idx, 0])\n",
    "        #print(f\"image path: {img_path}\")\n",
    "        image = read_image(img_path)\n",
    "        image=image.to(torch.float32)\n",
    "        #print(f\"image type: {type(image)}\")\n",
    "        label = self.img_labels.iloc[idx, 1]\n",
    "        if self.transform:\n",
    "            #print('here')\n",
    "            image = self.transform(image)\n",
    "        if self.target_transform:\n",
    "            label = self.target_transform(label)\n",
    "        return image, label\n",
    "train_dataset= torchvision.datasets.ImageFolder(\"images\",transform)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size,shuffle=True)\n",
    "#                                           shuffle=True)# # CIFAR10: 60000 32x32 color images in 10 classes, with 6000 images per class\n",
    "# train_dataset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "#                                         download=True, transform=transform)\n",
    "\n",
    "# test_dataset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
    "#                                        download=True, transform=transform)\n",
    "\n",
    "# train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size,\n",
    "#                                           shuffle=True)\n",
    "\n",
    "# test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size,\n",
    "#                                          shuffle=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ConvNet(nn.Module):\n",
    "    def __init__(self,output_size):\n",
    "        super(ConvNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 3)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 2)\n",
    "        self.fc1 = nn.Linear(20*1*16, 120)\n",
    "        self.fc2 = nn.Linear(120, 50)\n",
    "        self.fc3 = nn.Linear(50, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # -> n, 3, 32, 32\n",
    "        x = self.pool(F.relu(self.conv1(x)))  # -> n, 6, 14, 14\n",
    "        x = F.relu(self.conv2(x))  # -> n, 16, 5, 5\n",
    "        x = x.view(-1, 20*1*16)            # -> n, 400\n",
    "        x = F.relu(self.fc1(x))               # -> n, 120\n",
    "        x = F.relu(self.fc2(x))               # -> n, 84\n",
    "        x = self.fc3(x)                       # -> n, 10\n",
    "        return x\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ConvNet(\n",
       "  (conv1): Conv2d(3, 6, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (conv2): Conv2d(6, 16, kernel_size=(2, 2), stride=(1, 1))\n",
       "  (fc1): Linear(in_features=320, out_features=120, bias=True)\n",
       "  (fc2): Linear(in_features=120, out_features=50, bias=True)\n",
       "  (fc3): Linear(in_features=50, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_size=10\n",
    "model = ConvNet(output_size=10)\n",
    "PATH=os.path.join(\"trained_models\",\"28_12.pth\")\n",
    "model.load_state_dict(torch.load(PATH,map_location=torch.device('cpu')))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network: 95.89201877934272 %\n"
     ]
    }
   ],
   "source": [
    "#testing loaded model\n",
    "with torch.no_grad():\n",
    "    n_correct = 0\n",
    "    n_samples = 0\n",
    "    n_class_correct = [0 for i in range(output_size)]\n",
    "    n_class_samples = [0 for i in range(output_size)]\n",
    "    for images, labels in train_loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(images)\n",
    "        # max returns (value ,index)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        n_samples += labels.size(0)\n",
    "        n_correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        for i in range(batch_size):\n",
    "            label = labels[i]\n",
    "            pred = predicted[i]\n",
    "            if (label == pred):\n",
    "                n_class_correct[label] += 1\n",
    "            n_class_samples[label] += 1\n",
    "\n",
    "    acc = 100.0 * n_correct / n_samples\n",
    "    print(f'Accuracy of the network: {acc} %')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Hyper-parameters \n",
    "output_size=10\n",
    "print(device)\n",
    "num_epochs = 1000\n",
    "learning_rate = 0.001\n",
    "model = ConvNet(output_size=output_size).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "n_total_steps = len(train_loader)\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        # origin shape: [4, 3, 32, 32] = 4, 3, 1024\n",
    "        # input_layer: 3 input channels, 6 output channels, 5 kernel size\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if (i+1) % 100 == 0:\n",
    "            print (f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{n_total_steps}], Loss: {loss.item():.4f}')\n",
    "\n",
    "print('Finished Training')\n",
    "PATH = './cnn.pth'\n",
    "torch.save(model.state_dict(), PATH)\n",
    "\n",
    "with torch.no_grad():\n",
    "    n_correct = 0\n",
    "    n_samples = 0\n",
    "    n_class_correct = [0 for i in range(output_size)]\n",
    "    n_class_samples = [0 for i in range(output_size)]\n",
    "    for images, labels in train_loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(images)\n",
    "        # max returns (value ,index)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        n_samples += labels.size(0)\n",
    "        n_correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        for i in range(batch_size):\n",
    "            label = labels[i]\n",
    "            pred = predicted[i]\n",
    "            if (label == pred):\n",
    "                n_class_correct[label] += 1\n",
    "            n_class_samples[label] += 1\n",
    "\n",
    "    acc = 100.0 * n_correct / n_samples\n",
    "    print(f'Accuracy of the network: {acc} %')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
