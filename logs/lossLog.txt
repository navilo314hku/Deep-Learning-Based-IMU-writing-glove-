OptimConvNet2_ep100
training time:
20230113_201513
Epoch [1/100], Step [735/735], Loss: 0.2122
OptimConvNet2_ep100
training time:
20230113_201548
Epoch [1/100], Step [735/735], Loss: 0.9111
Epoch [2/100], Step [735/735], Loss: 0.0908
Epoch [3/100], Step [735/735], Loss: 0.4910
Epoch [4/100], Step [735/735], Loss: 0.2910
Epoch [5/100], Step [735/735], Loss: 0.3006
Epoch [6/100], Step [735/735], Loss: 0.1016
Epoch [7/100], Step [735/735], Loss: 0.1851
Epoch [8/100], Step [735/735], Loss: 0.1706
Epoch [9/100], Step [735/735], Loss: 0.0809
Epoch [10/100], Step [735/735], Loss: 0.1482
Epoch [11/100], Step [735/735], Loss: 0.4418
Epoch [12/100], Step [735/735], Loss: 0.5812
Epoch [13/100], Step [735/735], Loss: 0.4311
Epoch [14/100], Step [735/735], Loss: 0.1392
Epoch [15/100], Step [735/735], Loss: 0.2850
Epoch [16/100], Step [735/735], Loss: 1.1687
Epoch [17/100], Step [735/735], Loss: 0.5282
Epoch [18/100], Step [735/735], Loss: 1.4390
Epoch [19/100], Step [735/735], Loss: 0.0989
Epoch [20/100], Step [735/735], Loss: 0.0307
Epoch [21/100], Step [735/735], Loss: 0.7349
Epoch [22/100], Step [735/735], Loss: 0.0369
Epoch [23/100], Step [735/735], Loss: 0.7449
Epoch [24/100], Step [735/735], Loss: 0.5812
Epoch [25/100], Step [735/735], Loss: 0.2114
Epoch [26/100], Step [735/735], Loss: 0.0261
Epoch [27/100], Step [735/735], Loss: 0.1520
Epoch [28/100], Step [735/735], Loss: 0.6917
Epoch [29/100], Step [735/735], Loss: 0.2702
Epoch [30/100], Step [735/735], Loss: 0.0267
Epoch [31/100], Step [735/735], Loss: 0.3066
Epoch [32/100], Step [735/735], Loss: 0.3660
Epoch [33/100], Step [735/735], Loss: 0.2152
Epoch [34/100], Step [735/735], Loss: 0.3072
Epoch [35/100], Step [735/735], Loss: 0.3940
Epoch [36/100], Step [735/735], Loss: 0.7061
Epoch [37/100], Step [735/735], Loss: 0.2191
Epoch [38/100], Step [735/735], Loss: 0.1147
Epoch [39/100], Step [735/735], Loss: 0.2833
Epoch [40/100], Step [735/735], Loss: 0.4279
Epoch [41/100], Step [735/735], Loss: 0.2703
Epoch [42/100], Step [735/735], Loss: 0.3669
Epoch [43/100], Step [735/735], Loss: 0.4005
Epoch [44/100], Step [735/735], Loss: 0.6106
Epoch [45/100], Step [735/735], Loss: 0.5499
Epoch [46/100], Step [735/735], Loss: 0.1017
Epoch [47/100], Step [735/735], Loss: 0.7757
Epoch [48/100], Step [735/735], Loss: 0.0079
Epoch [49/100], Step [735/735], Loss: 0.1927
Epoch [50/100], Step [735/735], Loss: 0.1513
Epoch [51/100], Step [735/735], Loss: 0.0401
Epoch [52/100], Step [735/735], Loss: 0.4429
Epoch [53/100], Step [735/735], Loss: 0.0278
Epoch [54/100], Step [735/735], Loss: 0.0134
Epoch [55/100], Step [735/735], Loss: 0.1271
Epoch [56/100], Step [735/735], Loss: 0.1677
Epoch [57/100], Step [735/735], Loss: 0.0872
Epoch [58/100], Step [735/735], Loss: 0.1009
Epoch [59/100], Step [735/735], Loss: 0.1537
Epoch [60/100], Step [735/735], Loss: 0.0045
Epoch [61/100], Step [735/735], Loss: 0.2906
Epoch [62/100], Step [735/735], Loss: 0.0184
Epoch [63/100], Step [735/735], Loss: 0.0142
Epoch [64/100], Step [735/735], Loss: 0.0662
Epoch [65/100], Step [735/735], Loss: 1.0281
Epoch [66/100], Step [735/735], Loss: 0.5604
Epoch [67/100], Step [735/735], Loss: 0.0323
Epoch [68/100], Step [735/735], Loss: 0.2084
Epoch [69/100], Step [735/735], Loss: 0.2117
Epoch [70/100], Step [735/735], Loss: 2.0436
Epoch [71/100], Step [735/735], Loss: 0.1422
Epoch [72/100], Step [735/735], Loss: 0.8927
Epoch [73/100], Step [735/735], Loss: 0.3323
Epoch [74/100], Step [735/735], Loss: 0.3102
Epoch [75/100], Step [735/735], Loss: 1.5401
Epoch [76/100], Step [735/735], Loss: 0.6329
Epoch [77/100], Step [735/735], Loss: 0.0918
Epoch [78/100], Step [735/735], Loss: 0.8547
Epoch [79/100], Step [735/735], Loss: 0.3153
Epoch [80/100], Step [735/735], Loss: 0.1782
Epoch [81/100], Step [735/735], Loss: 0.2225
Epoch [82/100], Step [735/735], Loss: 0.2706
Epoch [83/100], Step [735/735], Loss: 0.6788
Epoch [84/100], Step [735/735], Loss: 0.0998
Epoch [85/100], Step [735/735], Loss: 0.0630
Epoch [86/100], Step [735/735], Loss: 0.0456
Epoch [87/100], Step [735/735], Loss: 0.0886
Epoch [88/100], Step [735/735], Loss: 0.0484
Epoch [89/100], Step [735/735], Loss: 0.4790
Epoch [90/100], Step [735/735], Loss: 0.4921
Epoch [91/100], Step [735/735], Loss: 0.1291
Epoch [92/100], Step [735/735], Loss: 0.3671
Epoch [93/100], Step [735/735], Loss: 3.3339
Epoch [94/100], Step [735/735], Loss: 0.0692
Epoch [95/100], Step [735/735], Loss: 0.0520
Epoch [96/100], Step [735/735], Loss: 0.0578
Epoch [97/100], Step [735/735], Loss: 0.0050
Epoch [98/100], Step [735/735], Loss: 0.3232
Epoch [99/100], Step [735/735], Loss: 0.1603
Epoch [100/100], Step [735/735], Loss: 0.9661
Finished TrainingRNN_ep100
training time:
20230130_141326
OptimConvNet2_ep100
training time:
20230130_141615
OptimConvNet2_ep100
training time:
20230130_151908
RNN_ep100
training time:
20230130_151921
RNN_ep100
training time:
20230130_152046
RNN_ep100
training time:
20230130_152313
RNN_ep100
training time:
20230130_152458
Epoch [1/100], Step [735/735], Loss: 2.3680
Epoch [2/100], Step [735/735], Loss: 2.3254
Epoch [3/100], Step [735/735], Loss: 2.3506
Epoch [4/100], Step [735/735], Loss: 2.2832
Epoch [5/100], Step [735/735], Loss: 2.3519
Epoch [6/100], Step [735/735], Loss: 2.3298
RNN_ep100
training time:
20230130_152621
Epoch [1/100], Step [735/735], Loss: 2.3491
Epoch [2/100], Step [735/735], Loss: 2.2936
Epoch [3/100], Step [735/735], Loss: 2.3437
Epoch [4/100], Step [735/735], Loss: 2.2951
Epoch [5/100], Step [735/735], Loss: 2.3712
Epoch [6/100], Step [735/735], Loss: 2.2815
Epoch [7/100], Step [735/735], Loss: 2.3525
Epoch [8/100], Step [735/735], Loss: 2.2878
Epoch [9/100], Step [735/735], Loss: 2.3126
Epoch [10/100], Step [735/735], Loss: 2.3039
Epoch [11/100], Step [735/735], Loss: 2.2914
Epoch [12/100], Step [735/735], Loss: 2.2211
Epoch [13/100], Step [735/735], Loss: 2.2558
Epoch [14/100], Step [735/735], Loss: 2.2769
Epoch [15/100], Step [735/735], Loss: 2.3124
Epoch [16/100], Step [735/735], Loss: 2.3200
Epoch [17/100], Step [735/735], Loss: 2.2582
Epoch [18/100], Step [735/735], Loss: 2.3006
Epoch [19/100], Step [735/735], Loss: 2.3180
Epoch [20/100], Step [735/735], Loss: 2.2428
Epoch [21/100], Step [735/735], Loss: 2.2908
Epoch [22/100], Step [735/735], Loss: 2.2452
Epoch [23/100], Step [735/735], Loss: 2.2067
Epoch [24/100], Step [735/735], Loss: 2.2897
Epoch [25/100], Step [735/735], Loss: 2.5136
Epoch [26/100], Step [735/735], Loss: 2.2306
Epoch [27/100], Step [735/735], Loss: 2.5830
Epoch [28/100], Step [735/735], Loss: 2.0897
Epoch [29/100], Step [735/735], Loss: 1.8605
Epoch [30/100], Step [735/735], Loss: 0.8162
Epoch [31/100], Step [735/735], Loss: 1.2708
Epoch [32/100], Step [735/735], Loss: 2.8634
Epoch [33/100], Step [735/735], Loss: 1.6606
Epoch [34/100], Step [735/735], Loss: 1.9653
Epoch [35/100], Step [735/735], Loss: 0.6917
Epoch [36/100], Step [735/735], Loss: 2.4972
Epoch [37/100], Step [735/735], Loss: 1.3133
Epoch [38/100], Step [735/735], Loss: 1.5776
Epoch [39/100], Step [735/735], Loss: 1.5299
Epoch [40/100], Step [735/735], Loss: 1.4271
Epoch [41/100], Step [735/735], Loss: 2.4110
Epoch [42/100], Step [735/735], Loss: 0.8210
Epoch [43/100], Step [735/735], Loss: 2.0251
Epoch [44/100], Step [735/735], Loss: 1.7477
Epoch [45/100], Step [735/735], Loss: 1.2366
Epoch [46/100], Step [735/735], Loss: 0.8024
Epoch [47/100], Step [735/735], Loss: 0.6971
Epoch [48/100], Step [735/735], Loss: 1.4656
Epoch [49/100], Step [735/735], Loss: 0.9737
Epoch [50/100], Step [735/735], Loss: 1.7508
Epoch [51/100], Step [735/735], Loss: 0.5578
Epoch [52/100], Step [735/735], Loss: 2.0422
Epoch [53/100], Step [735/735], Loss: 1.4357
Epoch [54/100], Step [735/735], Loss: 0.7596
Epoch [55/100], Step [735/735], Loss: 0.9469
Epoch [56/100], Step [735/735], Loss: 0.2959
Epoch [57/100], Step [735/735], Loss: 1.0509
Epoch [58/100], Step [735/735], Loss: 0.0899
Epoch [59/100], Step [735/735], Loss: 1.1464
Epoch [60/100], Step [735/735], Loss: 0.3317
Epoch [61/100], Step [735/735], Loss: 0.4407
Epoch [62/100], Step [735/735], Loss: 0.1234
Epoch [63/100], Step [735/735], Loss: 0.5293
Epoch [64/100], Step [735/735], Loss: 0.2410
Epoch [65/100], Step [735/735], Loss: 0.5247
Epoch [66/100], Step [735/735], Loss: 1.6201
Epoch [67/100], Step [735/735], Loss: 0.6911
Epoch [68/100], Step [735/735], Loss: 0.5114
Epoch [69/100], Step [735/735], Loss: 2.1005
Epoch [70/100], Step [735/735], Loss: 0.3613
Epoch [71/100], Step [735/735], Loss: 0.6173
Epoch [72/100], Step [735/735], Loss: 1.2132
Epoch [73/100], Step [735/735], Loss: 1.6028
Epoch [74/100], Step [735/735], Loss: 0.4446
Epoch [75/100], Step [735/735], Loss: 0.0385
Epoch [76/100], Step [735/735], Loss: 0.4334
Epoch [77/100], Step [735/735], Loss: 0.7288
Epoch [78/100], Step [735/735], Loss: 0.3108
Epoch [79/100], Step [735/735], Loss: 0.0414
Epoch [80/100], Step [735/735], Loss: 0.9710
Epoch [81/100], Step [735/735], Loss: 1.6496
Epoch [82/100], Step [735/735], Loss: 0.2286
Epoch [83/100], Step [735/735], Loss: 0.3446
Epoch [84/100], Step [735/735], Loss: 1.5905
Epoch [85/100], Step [735/735], Loss: 0.0646
Epoch [86/100], Step [735/735], Loss: 0.3458
Epoch [87/100], Step [735/735], Loss: 0.5336
Epoch [88/100], Step [735/735], Loss: 0.6830
Epoch [89/100], Step [735/735], Loss: 0.2243
Epoch [90/100], Step [735/735], Loss: 0.2306
Epoch [91/100], Step [735/735], Loss: 0.3867
Epoch [92/100], Step [735/735], Loss: 1.0494
Epoch [93/100], Step [735/735], Loss: 0.4044
Epoch [94/100], Step [735/735], Loss: 0.0223
Epoch [95/100], Step [735/735], Loss: 0.4681
Epoch [96/100], Step [735/735], Loss: 0.6258
Epoch [97/100], Step [735/735], Loss: 0.3917
Epoch [98/100], Step [735/735], Loss: 0.1265
Epoch [99/100], Step [735/735], Loss: 0.0049
Epoch [100/100], Step [735/735], Loss: 0.1304
Finished TrainingRNN_LSTM_ep100
training time:
20230130_161124
RNN_LSTM_ep100
training time:
20230130_161301
Epoch [1/100], Step [735/735], Loss: 2.3204
RNN_LSTM_ep50
training time:
20230130_161353
Epoch [1/50], Step [735/735], Loss: 2.3016
Epoch [2/50], Step [735/735], Loss: 2.3156
Epoch [3/50], Step [735/735], Loss: 2.3010
Epoch [4/50], Step [735/735], Loss: 2.3083
Epoch [5/50], Step [735/735], Loss: 2.3171
Epoch [6/50], Step [735/735], Loss: 2.3195
Epoch [7/50], Step [735/735], Loss: 2.3142
Epoch [8/50], Step [735/735], Loss: 2.3057
Epoch [9/50], Step [735/735], Loss: 2.2926
Epoch [10/50], Step [735/735], Loss: 2.3191
Epoch [11/50], Step [735/735], Loss: 2.2891
Epoch [12/50], Step [735/735], Loss: 2.2871
Epoch [13/50], Step [735/735], Loss: 2.2934
Epoch [14/50], Step [735/735], Loss: 2.2972
Epoch [15/50], Step [735/735], Loss: 2.2840
Epoch [16/50], Step [735/735], Loss: 2.2875
Epoch [17/50], Step [735/735], Loss: 2.3084
Epoch [18/50], Step [735/735], Loss: 2.3233
Epoch [19/50], Step [735/735], Loss: 2.3050
Epoch [20/50], Step [735/735], Loss: 2.2943
Epoch [21/50], Step [735/735], Loss: 2.2781
Epoch [22/50], Step [735/735], Loss: 2.3114
Epoch [23/50], Step [735/735], Loss: 2.2796
Epoch [24/50], Step [735/735], Loss: 2.2849
Epoch [25/50], Step [735/735], Loss: 2.2807
Epoch [26/50], Step [735/735], Loss: 2.2739
Epoch [27/50], Step [735/735], Loss: 2.2690
Epoch [28/50], Step [735/735], Loss: 2.3213
Epoch [29/50], Step [735/735], Loss: 2.3133
Epoch [30/50], Step [735/735], Loss: 2.2976
Epoch [31/50], Step [735/735], Loss: 2.2867
Epoch [32/50], Step [735/735], Loss: 2.3031
Epoch [33/50], Step [735/735], Loss: 2.2873
Epoch [34/50], Step [735/735], Loss: 2.2860
Epoch [35/50], Step [735/735], Loss: 2.3340
Epoch [36/50], Step [735/735], Loss: 2.3054
Epoch [37/50], Step [735/735], Loss: 2.2649
Epoch [38/50], Step [735/735], Loss: 2.3074
Epoch [39/50], Step [735/735], Loss: 2.3030
Epoch [40/50], Step [735/735], Loss: 2.2816
Epoch [41/50], Step [735/735], Loss: 2.3120
Epoch [42/50], Step [735/735], Loss: 2.2734
Epoch [43/50], Step [735/735], Loss: 2.3117
Epoch [44/50], Step [735/735], Loss: 2.2745
Epoch [45/50], Step [735/735], Loss: 2.2846
Epoch [46/50], Step [735/735], Loss: 2.3171
Epoch [47/50], Step [735/735], Loss: 2.2970
Epoch [48/50], Step [735/735], Loss: 2.2695
Epoch [49/50], Step [735/735], Loss: 2.2979
Epoch [50/50], Step [735/735], Loss: 2.2808
Finished TrainingRNN_LSTM_ep50
training time:
20230202_125740
RNN_LSTM_ep50
training time:
20230208_171029
RNN_LSTM_ep50
training time:
20230210_100551
RNN2_ep50
training time:
20230210_132213
RNN2_ep50
training time:
20230210_132233
RNN2_ep50
training time:
20230210_132312
RNN2_ep50
training time:
20230210_132531
RNN2_ep50
training time:
20230210_132650
RNN2_ep50
training time:
20230210_132800
RNN_ep50
training time:
20230210_133432
RNN_ep50
training time:
20230210_133616
RNN_ep50
training time:
20230210_133653
RNN_ep50
training time:
20230210_133721
RNN_LSTM_ep50
training time:
20230210_133959
RNN_test_ep50
training time:
20230210_153032
RNN_test_ep50
training time:
20230210_153148
RNN_ep50
training time:
20230210_153303
RNN_test_ep50
training time:
20230210_153746
