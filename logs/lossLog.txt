OptimConvNet2_ep100
training time:
20230113_201513
Epoch [1/100], Step [735/735], Loss: 0.2122
OptimConvNet2_ep100
training time:
20230113_201548
Epoch [1/100], Step [735/735], Loss: 0.9111
Epoch [2/100], Step [735/735], Loss: 0.0908
Epoch [3/100], Step [735/735], Loss: 0.4910
Epoch [4/100], Step [735/735], Loss: 0.2910
Epoch [5/100], Step [735/735], Loss: 0.3006
Epoch [6/100], Step [735/735], Loss: 0.1016
Epoch [7/100], Step [735/735], Loss: 0.1851
Epoch [8/100], Step [735/735], Loss: 0.1706
Epoch [9/100], Step [735/735], Loss: 0.0809
Epoch [10/100], Step [735/735], Loss: 0.1482
Epoch [11/100], Step [735/735], Loss: 0.4418
Epoch [12/100], Step [735/735], Loss: 0.5812
Epoch [13/100], Step [735/735], Loss: 0.4311
Epoch [14/100], Step [735/735], Loss: 0.1392
Epoch [15/100], Step [735/735], Loss: 0.2850
Epoch [16/100], Step [735/735], Loss: 1.1687
Epoch [17/100], Step [735/735], Loss: 0.5282
Epoch [18/100], Step [735/735], Loss: 1.4390
Epoch [19/100], Step [735/735], Loss: 0.0989
Epoch [20/100], Step [735/735], Loss: 0.0307
Epoch [21/100], Step [735/735], Loss: 0.7349
Epoch [22/100], Step [735/735], Loss: 0.0369
Epoch [23/100], Step [735/735], Loss: 0.7449
Epoch [24/100], Step [735/735], Loss: 0.5812
Epoch [25/100], Step [735/735], Loss: 0.2114
Epoch [26/100], Step [735/735], Loss: 0.0261
Epoch [27/100], Step [735/735], Loss: 0.1520
Epoch [28/100], Step [735/735], Loss: 0.6917
Epoch [29/100], Step [735/735], Loss: 0.2702
Epoch [30/100], Step [735/735], Loss: 0.0267
Epoch [31/100], Step [735/735], Loss: 0.3066
Epoch [32/100], Step [735/735], Loss: 0.3660
Epoch [33/100], Step [735/735], Loss: 0.2152
Epoch [34/100], Step [735/735], Loss: 0.3072
Epoch [35/100], Step [735/735], Loss: 0.3940
Epoch [36/100], Step [735/735], Loss: 0.7061
Epoch [37/100], Step [735/735], Loss: 0.2191
Epoch [38/100], Step [735/735], Loss: 0.1147
Epoch [39/100], Step [735/735], Loss: 0.2833
Epoch [40/100], Step [735/735], Loss: 0.4279
Epoch [41/100], Step [735/735], Loss: 0.2703
Epoch [42/100], Step [735/735], Loss: 0.3669
Epoch [43/100], Step [735/735], Loss: 0.4005
Epoch [44/100], Step [735/735], Loss: 0.6106
Epoch [45/100], Step [735/735], Loss: 0.5499
Epoch [46/100], Step [735/735], Loss: 0.1017
Epoch [47/100], Step [735/735], Loss: 0.7757
Epoch [48/100], Step [735/735], Loss: 0.0079
Epoch [49/100], Step [735/735], Loss: 0.1927
Epoch [50/100], Step [735/735], Loss: 0.1513
Epoch [51/100], Step [735/735], Loss: 0.0401
Epoch [52/100], Step [735/735], Loss: 0.4429
Epoch [53/100], Step [735/735], Loss: 0.0278
Epoch [54/100], Step [735/735], Loss: 0.0134
Epoch [55/100], Step [735/735], Loss: 0.1271
Epoch [56/100], Step [735/735], Loss: 0.1677
Epoch [57/100], Step [735/735], Loss: 0.0872
Epoch [58/100], Step [735/735], Loss: 0.1009
Epoch [59/100], Step [735/735], Loss: 0.1537
Epoch [60/100], Step [735/735], Loss: 0.0045
Epoch [61/100], Step [735/735], Loss: 0.2906
Epoch [62/100], Step [735/735], Loss: 0.0184
Epoch [63/100], Step [735/735], Loss: 0.0142
Epoch [64/100], Step [735/735], Loss: 0.0662
Epoch [65/100], Step [735/735], Loss: 1.0281
Epoch [66/100], Step [735/735], Loss: 0.5604
Epoch [67/100], Step [735/735], Loss: 0.0323
Epoch [68/100], Step [735/735], Loss: 0.2084
Epoch [69/100], Step [735/735], Loss: 0.2117
Epoch [70/100], Step [735/735], Loss: 2.0436
Epoch [71/100], Step [735/735], Loss: 0.1422
Epoch [72/100], Step [735/735], Loss: 0.8927
Epoch [73/100], Step [735/735], Loss: 0.3323
Epoch [74/100], Step [735/735], Loss: 0.3102
Epoch [75/100], Step [735/735], Loss: 1.5401
Epoch [76/100], Step [735/735], Loss: 0.6329
Epoch [77/100], Step [735/735], Loss: 0.0918
Epoch [78/100], Step [735/735], Loss: 0.8547
Epoch [79/100], Step [735/735], Loss: 0.3153
Epoch [80/100], Step [735/735], Loss: 0.1782
Epoch [81/100], Step [735/735], Loss: 0.2225
Epoch [82/100], Step [735/735], Loss: 0.2706
Epoch [83/100], Step [735/735], Loss: 0.6788
Epoch [84/100], Step [735/735], Loss: 0.0998
Epoch [85/100], Step [735/735], Loss: 0.0630
Epoch [86/100], Step [735/735], Loss: 0.0456
Epoch [87/100], Step [735/735], Loss: 0.0886
Epoch [88/100], Step [735/735], Loss: 0.0484
Epoch [89/100], Step [735/735], Loss: 0.4790
Epoch [90/100], Step [735/735], Loss: 0.4921
Epoch [91/100], Step [735/735], Loss: 0.1291
Epoch [92/100], Step [735/735], Loss: 0.3671
Epoch [93/100], Step [735/735], Loss: 3.3339
Epoch [94/100], Step [735/735], Loss: 0.0692
Epoch [95/100], Step [735/735], Loss: 0.0520
Epoch [96/100], Step [735/735], Loss: 0.0578
Epoch [97/100], Step [735/735], Loss: 0.0050
Epoch [98/100], Step [735/735], Loss: 0.3232
Epoch [99/100], Step [735/735], Loss: 0.1603
Epoch [100/100], Step [735/735], Loss: 0.9661
Finished TrainingRNN_ep100
training time:
20230130_141326
OptimConvNet2_ep100
training time:
20230130_141615
OptimConvNet2_ep100
training time:
20230130_151908
RNN_ep100
training time:
20230130_151921
RNN_ep100
training time:
20230130_152046
RNN_ep100
training time:
20230130_152313
RNN_ep100
training time:
20230130_152458
Epoch [1/100], Step [735/735], Loss: 2.3680
Epoch [2/100], Step [735/735], Loss: 2.3254
Epoch [3/100], Step [735/735], Loss: 2.3506
Epoch [4/100], Step [735/735], Loss: 2.2832
Epoch [5/100], Step [735/735], Loss: 2.3519
Epoch [6/100], Step [735/735], Loss: 2.3298
RNN_ep100
training time:
20230130_152621
Epoch [1/100], Step [735/735], Loss: 2.3491
Epoch [2/100], Step [735/735], Loss: 2.2936
Epoch [3/100], Step [735/735], Loss: 2.3437
Epoch [4/100], Step [735/735], Loss: 2.2951
Epoch [5/100], Step [735/735], Loss: 2.3712
Epoch [6/100], Step [735/735], Loss: 2.2815
Epoch [7/100], Step [735/735], Loss: 2.3525
Epoch [8/100], Step [735/735], Loss: 2.2878
Epoch [9/100], Step [735/735], Loss: 2.3126
Epoch [10/100], Step [735/735], Loss: 2.3039
Epoch [11/100], Step [735/735], Loss: 2.2914
Epoch [12/100], Step [735/735], Loss: 2.2211
Epoch [13/100], Step [735/735], Loss: 2.2558
Epoch [14/100], Step [735/735], Loss: 2.2769
Epoch [15/100], Step [735/735], Loss: 2.3124
Epoch [16/100], Step [735/735], Loss: 2.3200
Epoch [17/100], Step [735/735], Loss: 2.2582
Epoch [18/100], Step [735/735], Loss: 2.3006
Epoch [19/100], Step [735/735], Loss: 2.3180
Epoch [20/100], Step [735/735], Loss: 2.2428
Epoch [21/100], Step [735/735], Loss: 2.2908
Epoch [22/100], Step [735/735], Loss: 2.2452
Epoch [23/100], Step [735/735], Loss: 2.2067
Epoch [24/100], Step [735/735], Loss: 2.2897
Epoch [25/100], Step [735/735], Loss: 2.5136
Epoch [26/100], Step [735/735], Loss: 2.2306
Epoch [27/100], Step [735/735], Loss: 2.5830
Epoch [28/100], Step [735/735], Loss: 2.0897
Epoch [29/100], Step [735/735], Loss: 1.8605
Epoch [30/100], Step [735/735], Loss: 0.8162
Epoch [31/100], Step [735/735], Loss: 1.2708
Epoch [32/100], Step [735/735], Loss: 2.8634
Epoch [33/100], Step [735/735], Loss: 1.6606
Epoch [34/100], Step [735/735], Loss: 1.9653
Epoch [35/100], Step [735/735], Loss: 0.6917
Epoch [36/100], Step [735/735], Loss: 2.4972
Epoch [37/100], Step [735/735], Loss: 1.3133
Epoch [38/100], Step [735/735], Loss: 1.5776
Epoch [39/100], Step [735/735], Loss: 1.5299
Epoch [40/100], Step [735/735], Loss: 1.4271
Epoch [41/100], Step [735/735], Loss: 2.4110
Epoch [42/100], Step [735/735], Loss: 0.8210
Epoch [43/100], Step [735/735], Loss: 2.0251
Epoch [44/100], Step [735/735], Loss: 1.7477
Epoch [45/100], Step [735/735], Loss: 1.2366
Epoch [46/100], Step [735/735], Loss: 0.8024
Epoch [47/100], Step [735/735], Loss: 0.6971
Epoch [48/100], Step [735/735], Loss: 1.4656
Epoch [49/100], Step [735/735], Loss: 0.9737
Epoch [50/100], Step [735/735], Loss: 1.7508
Epoch [51/100], Step [735/735], Loss: 0.5578
Epoch [52/100], Step [735/735], Loss: 2.0422
Epoch [53/100], Step [735/735], Loss: 1.4357
Epoch [54/100], Step [735/735], Loss: 0.7596
Epoch [55/100], Step [735/735], Loss: 0.9469
Epoch [56/100], Step [735/735], Loss: 0.2959
Epoch [57/100], Step [735/735], Loss: 1.0509
Epoch [58/100], Step [735/735], Loss: 0.0899
Epoch [59/100], Step [735/735], Loss: 1.1464
Epoch [60/100], Step [735/735], Loss: 0.3317
Epoch [61/100], Step [735/735], Loss: 0.4407
Epoch [62/100], Step [735/735], Loss: 0.1234
Epoch [63/100], Step [735/735], Loss: 0.5293
Epoch [64/100], Step [735/735], Loss: 0.2410
Epoch [65/100], Step [735/735], Loss: 0.5247
Epoch [66/100], Step [735/735], Loss: 1.6201
Epoch [67/100], Step [735/735], Loss: 0.6911
Epoch [68/100], Step [735/735], Loss: 0.5114
Epoch [69/100], Step [735/735], Loss: 2.1005
Epoch [70/100], Step [735/735], Loss: 0.3613
Epoch [71/100], Step [735/735], Loss: 0.6173
Epoch [72/100], Step [735/735], Loss: 1.2132
Epoch [73/100], Step [735/735], Loss: 1.6028
Epoch [74/100], Step [735/735], Loss: 0.4446
Epoch [75/100], Step [735/735], Loss: 0.0385
Epoch [76/100], Step [735/735], Loss: 0.4334
Epoch [77/100], Step [735/735], Loss: 0.7288
Epoch [78/100], Step [735/735], Loss: 0.3108
Epoch [79/100], Step [735/735], Loss: 0.0414
Epoch [80/100], Step [735/735], Loss: 0.9710
Epoch [81/100], Step [735/735], Loss: 1.6496
Epoch [82/100], Step [735/735], Loss: 0.2286
Epoch [83/100], Step [735/735], Loss: 0.3446
Epoch [84/100], Step [735/735], Loss: 1.5905
Epoch [85/100], Step [735/735], Loss: 0.0646
Epoch [86/100], Step [735/735], Loss: 0.3458
Epoch [87/100], Step [735/735], Loss: 0.5336
Epoch [88/100], Step [735/735], Loss: 0.6830
Epoch [89/100], Step [735/735], Loss: 0.2243
Epoch [90/100], Step [735/735], Loss: 0.2306
Epoch [91/100], Step [735/735], Loss: 0.3867
Epoch [92/100], Step [735/735], Loss: 1.0494
Epoch [93/100], Step [735/735], Loss: 0.4044
Epoch [94/100], Step [735/735], Loss: 0.0223
Epoch [95/100], Step [735/735], Loss: 0.4681
Epoch [96/100], Step [735/735], Loss: 0.6258
Epoch [97/100], Step [735/735], Loss: 0.3917
Epoch [98/100], Step [735/735], Loss: 0.1265
Epoch [99/100], Step [735/735], Loss: 0.0049
Epoch [100/100], Step [735/735], Loss: 0.1304
Finished TrainingRNN_LSTM_ep100
training time:
20230130_161124
RNN_LSTM_ep100
training time:
20230130_161301
Epoch [1/100], Step [735/735], Loss: 2.3204
RNN_LSTM_ep50
training time:
20230130_161353
Epoch [1/50], Step [735/735], Loss: 2.3016
Epoch [2/50], Step [735/735], Loss: 2.3156
Epoch [3/50], Step [735/735], Loss: 2.3010
Epoch [4/50], Step [735/735], Loss: 2.3083
Epoch [5/50], Step [735/735], Loss: 2.3171
Epoch [6/50], Step [735/735], Loss: 2.3195
Epoch [7/50], Step [735/735], Loss: 2.3142
Epoch [8/50], Step [735/735], Loss: 2.3057
Epoch [9/50], Step [735/735], Loss: 2.2926
Epoch [10/50], Step [735/735], Loss: 2.3191
Epoch [11/50], Step [735/735], Loss: 2.2891
Epoch [12/50], Step [735/735], Loss: 2.2871
Epoch [13/50], Step [735/735], Loss: 2.2934
Epoch [14/50], Step [735/735], Loss: 2.2972
Epoch [15/50], Step [735/735], Loss: 2.2840
Epoch [16/50], Step [735/735], Loss: 2.2875
Epoch [17/50], Step [735/735], Loss: 2.3084
Epoch [18/50], Step [735/735], Loss: 2.3233
Epoch [19/50], Step [735/735], Loss: 2.3050
Epoch [20/50], Step [735/735], Loss: 2.2943
Epoch [21/50], Step [735/735], Loss: 2.2781
Epoch [22/50], Step [735/735], Loss: 2.3114
Epoch [23/50], Step [735/735], Loss: 2.2796
Epoch [24/50], Step [735/735], Loss: 2.2849
Epoch [25/50], Step [735/735], Loss: 2.2807
Epoch [26/50], Step [735/735], Loss: 2.2739
Epoch [27/50], Step [735/735], Loss: 2.2690
Epoch [28/50], Step [735/735], Loss: 2.3213
Epoch [29/50], Step [735/735], Loss: 2.3133
Epoch [30/50], Step [735/735], Loss: 2.2976
Epoch [31/50], Step [735/735], Loss: 2.2867
Epoch [32/50], Step [735/735], Loss: 2.3031
Epoch [33/50], Step [735/735], Loss: 2.2873
Epoch [34/50], Step [735/735], Loss: 2.2860
Epoch [35/50], Step [735/735], Loss: 2.3340
Epoch [36/50], Step [735/735], Loss: 2.3054
Epoch [37/50], Step [735/735], Loss: 2.2649
Epoch [38/50], Step [735/735], Loss: 2.3074
Epoch [39/50], Step [735/735], Loss: 2.3030
Epoch [40/50], Step [735/735], Loss: 2.2816
Epoch [41/50], Step [735/735], Loss: 2.3120
Epoch [42/50], Step [735/735], Loss: 2.2734
Epoch [43/50], Step [735/735], Loss: 2.3117
Epoch [44/50], Step [735/735], Loss: 2.2745
Epoch [45/50], Step [735/735], Loss: 2.2846
Epoch [46/50], Step [735/735], Loss: 2.3171
Epoch [47/50], Step [735/735], Loss: 2.2970
Epoch [48/50], Step [735/735], Loss: 2.2695
Epoch [49/50], Step [735/735], Loss: 2.2979
Epoch [50/50], Step [735/735], Loss: 2.2808
Finished TrainingRNN_LSTM_ep50
training time:
20230202_125740
RNN_LSTM_ep50
training time:
20230208_171029
RNN_LSTM_ep50
training time:
20230210_100551
RNN2_ep50
training time:
20230210_132213
RNN2_ep50
training time:
20230210_132233
RNN2_ep50
training time:
20230210_132312
RNN2_ep50
training time:
20230210_132531
RNN2_ep50
training time:
20230210_132650
RNN2_ep50
training time:
20230210_132800
RNN_ep50
training time:
20230210_133432
RNN_ep50
training time:
20230210_133616
RNN_ep50
training time:
20230210_133653
RNN_ep50
training time:
20230210_133721
RNN_LSTM_ep50
training time:
20230210_133959
RNN_test_ep50
training time:
20230210_153032
RNN_test_ep50
training time:
20230210_153148
RNN_ep50
training time:
20230210_153303
RNN_test_ep50
training time:
20230210_153746
DNN_base_ep50
training time:
20230213_140242
DNN_base_ep50
training time:
20230213_140345
DNN_base_ep50
training time:
20230213_140425
DNN_base_ep50
training time:
20230213_140508
DNN_base_ep50
training time:
20230213_140529
RNN_ep50
training time:
20230214_130822
RNN_ep50
training time:
20230214_132017
RNN_ep50
training time:
20230214_132310
RNN_ep50
training time:
20230214_132338
Epoch [1/50], Step [76/76], Loss: 1.0055
Epoch [2/50], Step [76/76], Loss: 1.1293
Epoch [3/50], Step [76/76], Loss: 1.0852
Epoch [4/50], Step [76/76], Loss: 1.0657
Epoch [5/50], Step [76/76], Loss: 1.1453
Epoch [6/50], Step [76/76], Loss: 1.0381
Epoch [7/50], Step [76/76], Loss: 1.1536
Epoch [8/50], Step [76/76], Loss: 1.0505
Epoch [9/50], Step [76/76], Loss: 1.1288
Epoch [10/50], Step [76/76], Loss: 1.0724
Epoch [11/50], Step [76/76], Loss: 1.1000
Epoch [12/50], Step [76/76], Loss: 1.2253
Epoch [13/50], Step [76/76], Loss: 1.1244
Epoch [14/50], Step [76/76], Loss: 1.0761
Epoch [15/50], Step [76/76], Loss: 1.0123
Epoch [16/50], Step [76/76], Loss: 1.0748
Epoch [17/50], Step [76/76], Loss: 1.0871
Epoch [18/50], Step [76/76], Loss: 1.1017
Epoch [19/50], Step [76/76], Loss: 1.2045
Epoch [20/50], Step [76/76], Loss: 1.1926
Epoch [21/50], Step [76/76], Loss: 1.0489
Epoch [22/50], Step [76/76], Loss: 1.0491
Epoch [23/50], Step [76/76], Loss: 1.0274
Epoch [24/50], Step [76/76], Loss: 1.1271
Epoch [25/50], Step [76/76], Loss: 1.1023
Epoch [26/50], Step [76/76], Loss: 1.1568
Epoch [27/50], Step [76/76], Loss: 1.0905
Epoch [28/50], Step [76/76], Loss: 1.0564
Epoch [29/50], Step [76/76], Loss: 1.1457
Epoch [30/50], Step [76/76], Loss: 1.0877
Epoch [31/50], Step [76/76], Loss: 1.0077
Epoch [32/50], Step [76/76], Loss: 1.1567
Epoch [33/50], Step [76/76], Loss: 1.0336
Epoch [34/50], Step [76/76], Loss: 1.0683
Epoch [35/50], Step [76/76], Loss: 1.0267
Epoch [36/50], Step [76/76], Loss: 1.1195
Epoch [37/50], Step [76/76], Loss: 1.0037
Epoch [38/50], Step [76/76], Loss: 1.0911
Epoch [39/50], Step [76/76], Loss: 1.2085
Epoch [40/50], Step [76/76], Loss: 1.1310
Epoch [41/50], Step [76/76], Loss: 1.1237
Epoch [42/50], Step [76/76], Loss: 1.0664
Epoch [43/50], Step [76/76], Loss: 1.0998
Epoch [44/50], Step [76/76], Loss: 1.1598
Epoch [45/50], Step [76/76], Loss: 1.0486
Epoch [46/50], Step [76/76], Loss: 1.1091
Epoch [47/50], Step [76/76], Loss: 1.1144
Epoch [48/50], Step [76/76], Loss: 1.1041
Epoch [49/50], Step [76/76], Loss: 1.1307
Epoch [50/50], Step [76/76], Loss: 1.0797
Finished TrainingRNN_ep50
training time:
20230214_132511
Epoch [1/50], Step [76/76], Loss: 1.0472
Epoch [2/50], Step [76/76], Loss: 1.0505
Epoch [3/50], Step [76/76], Loss: 1.0600
Epoch [4/50], Step [76/76], Loss: 1.0875
Epoch [5/50], Step [76/76], Loss: 1.1262
Epoch [6/50], Step [76/76], Loss: 1.0805
Epoch [7/50], Step [76/76], Loss: 1.0575
Epoch [8/50], Step [76/76], Loss: 1.2092
Epoch [9/50], Step [76/76], Loss: 1.0462
Epoch [10/50], Step [76/76], Loss: 1.0351
Epoch [11/50], Step [76/76], Loss: 1.0144
Epoch [12/50], Step [76/76], Loss: 1.0967
Epoch [13/50], Step [76/76], Loss: 1.0433
Epoch [14/50], Step [76/76], Loss: 1.1524
Epoch [15/50], Step [76/76], Loss: 0.9850
Epoch [16/50], Step [76/76], Loss: 1.1256
Epoch [17/50], Step [76/76], Loss: 1.1144
Epoch [18/50], Step [76/76], Loss: 1.1320
Epoch [19/50], Step [76/76], Loss: 0.9982
Epoch [20/50], Step [76/76], Loss: 1.1307
Epoch [21/50], Step [76/76], Loss: 1.1364
Epoch [22/50], Step [76/76], Loss: 1.1688
Epoch [23/50], Step [76/76], Loss: 1.1142
Epoch [24/50], Step [76/76], Loss: 1.0748
Epoch [25/50], Step [76/76], Loss: 1.0664
Epoch [26/50], Step [76/76], Loss: 1.1106
Epoch [27/50], Step [76/76], Loss: 1.1546
Epoch [28/50], Step [76/76], Loss: 1.0852
Epoch [29/50], Step [76/76], Loss: 1.1371
Epoch [30/50], Step [76/76], Loss: 1.1044
Epoch [31/50], Step [76/76], Loss: 1.0344
Epoch [32/50], Step [76/76], Loss: 1.0891
Epoch [33/50], Step [76/76], Loss: 1.1319
Epoch [34/50], Step [76/76], Loss: 1.0633
Epoch [35/50], Step [76/76], Loss: 1.0787
Epoch [36/50], Step [76/76], Loss: 1.0660
Epoch [37/50], Step [76/76], Loss: 1.0960
Epoch [38/50], Step [76/76], Loss: 1.1571
Epoch [39/50], Step [76/76], Loss: 1.1117
Epoch [40/50], Step [76/76], Loss: 1.1095
Epoch [41/50], Step [76/76], Loss: 1.1206
Epoch [42/50], Step [76/76], Loss: 1.1265
Epoch [43/50], Step [76/76], Loss: 1.1057
Epoch [44/50], Step [76/76], Loss: 1.1921
Epoch [45/50], Step [76/76], Loss: 1.1228
Epoch [46/50], Step [76/76], Loss: 1.0803
Epoch [47/50], Step [76/76], Loss: 1.1161
Epoch [48/50], Step [76/76], Loss: 1.0804
Epoch [49/50], Step [76/76], Loss: 1.0830
Epoch [50/50], Step [76/76], Loss: 1.0630
Finished TrainingRNN_ep50
training time:
20230214_132801
Epoch [1/50], Step [76/76], Loss: 1.0376
Epoch [2/50], Step [76/76], Loss: 1.0948
Epoch [3/50], Step [76/76], Loss: 1.1450
Epoch [4/50], Step [76/76], Loss: 1.1054
Epoch [5/50], Step [76/76], Loss: 1.1106
Epoch [6/50], Step [76/76], Loss: 1.1773
Epoch [7/50], Step [76/76], Loss: 1.1227
Epoch [8/50], Step [76/76], Loss: 1.1492
Epoch [9/50], Step [76/76], Loss: 1.0736
Epoch [10/50], Step [76/76], Loss: 1.0810
Epoch [11/50], Step [76/76], Loss: 1.1335
Epoch [12/50], Step [76/76], Loss: 1.0371
Epoch [13/50], Step [76/76], Loss: 1.0591
Epoch [14/50], Step [76/76], Loss: 1.1454
Epoch [15/50], Step [76/76], Loss: 1.0681
Epoch [16/50], Step [76/76], Loss: 1.0857
Epoch [17/50], Step [76/76], Loss: 0.9913
Epoch [18/50], Step [76/76], Loss: 1.1069
Epoch [19/50], Step [76/76], Loss: 1.1513
Epoch [20/50], Step [76/76], Loss: 1.0610
Epoch [21/50], Step [76/76], Loss: 1.0150
Epoch [22/50], Step [76/76], Loss: 1.0503
Epoch [23/50], Step [76/76], Loss: 1.1271
Epoch [24/50], Step [76/76], Loss: 1.0895
Epoch [25/50], Step [76/76], Loss: 1.0472
Epoch [26/50], Step [76/76], Loss: 1.1410
Epoch [27/50], Step [76/76], Loss: 1.1115
Epoch [28/50], Step [76/76], Loss: 1.0938
Epoch [29/50], Step [76/76], Loss: 1.1258
Epoch [30/50], Step [76/76], Loss: 1.0293
Epoch [31/50], Step [76/76], Loss: 1.1332
Epoch [32/50], Step [76/76], Loss: 1.1394
Epoch [33/50], Step [76/76], Loss: 1.0657
Epoch [34/50], Step [76/76], Loss: 1.1361
Epoch [35/50], Step [76/76], Loss: 1.0607
Epoch [36/50], Step [76/76], Loss: 1.1031
Epoch [37/50], Step [76/76], Loss: 1.1328
Epoch [38/50], Step [76/76], Loss: 1.1086
Epoch [39/50], Step [76/76], Loss: 1.0559
Epoch [40/50], Step [76/76], Loss: 1.0843
Epoch [41/50], Step [76/76], Loss: 1.0744
Epoch [42/50], Step [76/76], Loss: 1.1103
Epoch [43/50], Step [76/76], Loss: 1.0765
Epoch [44/50], Step [76/76], Loss: 1.1720
Epoch [45/50], Step [76/76], Loss: 1.0689
Epoch [46/50], Step [76/76], Loss: 1.1130
Epoch [47/50], Step [76/76], Loss: 1.0875
Epoch [48/50], Step [76/76], Loss: 1.0812
Epoch [49/50], Step [76/76], Loss: 1.0777
Epoch [50/50], Step [76/76], Loss: 1.1511
Finished TrainingOptimConvNet2_ep50
training time:
20230214_133523
Epoch [1/50], Step [76/76], Loss: 1.1352
Epoch [2/50], Step [76/76], Loss: 1.1282
Epoch [3/50], Step [76/76], Loss: 1.1216
Epoch [4/50], Step [76/76], Loss: 1.1160
Epoch [5/50], Step [76/76], Loss: 1.0711
Epoch [6/50], Step [76/76], Loss: 1.1078
Epoch [7/50], Step [76/76], Loss: 1.1026
Epoch [8/50], Step [76/76], Loss: 1.0979
Epoch [9/50], Step [76/76], Loss: 1.0820
Epoch [10/50], Step [76/76], Loss: 1.0919
Epoch [11/50], Step [76/76], Loss: 1.0881
Epoch [12/50], Step [76/76], Loss: 1.1245
Epoch [13/50], Step [76/76], Loss: 1.0832
Epoch [14/50], Step [76/76], Loss: 1.0802
Epoch [15/50], Step [76/76], Loss: 1.0953
Epoch [16/50], Step [76/76], Loss: 1.1251
Epoch [17/50], Step [76/76], Loss: 1.0755
Epoch [18/50], Step [76/76], Loss: 1.0729
Epoch [19/50], Step [76/76], Loss: 1.0706
Epoch [20/50], Step [76/76], Loss: 1.1264
Epoch [21/50], Step [76/76], Loss: 1.0681
Epoch [22/50], Step [76/76], Loss: 1.1261
Epoch [23/50], Step [76/76], Loss: 1.1091
Epoch [24/50], Step [76/76], Loss: 1.0650
Epoch [25/50], Step [76/76], Loss: 1.1255
Epoch [26/50], Step [76/76], Loss: 1.1232
Epoch [27/50], Step [76/76], Loss: 1.0634
Epoch [28/50], Step [76/76], Loss: 1.0609
Epoch [29/50], Step [76/76], Loss: 1.0586
Epoch [30/50], Step [76/76], Loss: 1.1250
Epoch [31/50], Step [76/76], Loss: 1.1167
Epoch [32/50], Step [76/76], Loss: 1.1239
Epoch [33/50], Step [76/76], Loss: 1.0595
Epoch [34/50], Step [76/76], Loss: 1.1162
Epoch [35/50], Step [76/76], Loss: 1.0582
Epoch [36/50], Step [76/76], Loss: 1.0559
Epoch [37/50], Step [76/76], Loss: 1.1188
Epoch [38/50], Step [76/76], Loss: 1.0551
Epoch [39/50], Step [76/76], Loss: 1.1188
Epoch [40/50], Step [76/76], Loss: 1.0543
Epoch [41/50], Step [76/76], Loss: 1.0531
Epoch [42/50], Step [76/76], Loss: 1.1274
Epoch [43/50], Step [76/76], Loss: 1.1260
Epoch [44/50], Step [76/76], Loss: 1.0538
Epoch [45/50], Step [76/76], Loss: 1.0525
Epoch [46/50], Step [76/76], Loss: 1.0509
Epoch [47/50], Step [76/76], Loss: 1.1260
Epoch [48/50], Step [76/76], Loss: 1.0509
Epoch [49/50], Step [76/76], Loss: 1.0498
Epoch [50/50], Step [76/76], Loss: 1.1260
Finished TrainingOptimConvNet2_ep200
training time:
20230214_133653
Epoch [1/200], Step [76/76], Loss: 1.0759
Epoch [2/200], Step [76/76], Loss: 1.0730
Epoch [3/200], Step [76/76], Loss: 1.0703
Epoch [4/200], Step [76/76], Loss: 1.1458
Epoch [5/200], Step [76/76], Loss: 1.0867
Epoch [6/200], Step [76/76], Loss: 1.0876
Epoch [7/200], Step [76/76], Loss: 1.1420
Epoch [8/200], Step [76/76], Loss: 1.0679
Epoch [9/200], Step [76/76], Loss: 1.0654
Epoch [10/200], Step [76/76], Loss: 1.0633
Epoch [11/200], Step [76/76], Loss: 1.0973
Epoch [12/200], Step [76/76], Loss: 1.1378
Epoch [13/200], Step [76/76], Loss: 1.0625
Epoch [14/200], Step [76/76], Loss: 1.1359
Epoch [15/200], Step [76/76], Loss: 1.1030
Epoch [16/200], Step [76/76], Loss: 1.1024
Epoch [17/200], Step [76/76], Loss: 1.0616
Epoch [18/200], Step [76/76], Loss: 1.0604
Epoch [19/200], Step [76/76], Loss: 1.1337
Epoch [20/200], Step [76/76], Loss: 1.1067
Epoch [21/200], Step [76/76], Loss: 1.1322
Epoch [22/200], Step [76/76], Loss: 1.0595
Epoch [23/200], Step [76/76], Loss: 1.1091
Epoch [24/200], Step [76/76], Loss: 1.1087
Epoch [25/200], Step [76/76], Loss: 1.1300
Epoch [26/200], Step [76/76], Loss: 1.1282
Epoch [27/200], Step [76/76], Loss: 1.0600
Epoch [28/200], Step [76/76], Loss: 1.1268
Epoch [29/200], Step [76/76], Loss: 1.1251
Epoch [30/200], Step [76/76], Loss: 1.1143
Epoch [31/200], Step [76/76], Loss: 1.1240
Epoch [32/200], Step [76/76], Loss: 1.1231
Epoch [33/200], Step [76/76], Loss: 1.1156
Epoch [34/200], Step [76/76], Loss: 1.1226
Epoch [35/200], Step [76/76], Loss: 1.0592
Epoch [36/200], Step [76/76], Loss: 1.1168
Epoch [37/200], Step [76/76], Loss: 1.0587
Epoch [38/200], Step [76/76], Loss: 1.1171
Epoch [39/200], Step [76/76], Loss: 1.1237
Epoch [40/200], Step [76/76], Loss: 1.1226
Epoch [41/200], Step [76/76], Loss: 1.1213
Epoch [42/200], Step [76/76], Loss: 1.1202
Epoch [43/200], Step [76/76], Loss: 1.1198
Epoch [44/200], Step [76/76], Loss: 1.1186
Epoch [45/200], Step [76/76], Loss: 1.1172
Epoch [46/200], Step [76/76], Loss: 1.0589
Epoch [47/200], Step [76/76], Loss: 1.0570
Epoch [48/200], Step [76/76], Loss: 1.1229
Epoch [49/200], Step [76/76], Loss: 1.0561
Epoch [50/200], Step [76/76], Loss: 1.1199
Epoch [51/200], Step [76/76], Loss: 1.0547
Epoch [52/200], Step [76/76], Loss: 1.1198
Epoch [53/200], Step [76/76], Loss: 1.1247
Epoch [54/200], Step [76/76], Loss: 1.1195
Epoch [55/200], Step [76/76], Loss: 1.1231
Epoch [56/200], Step [76/76], Loss: 1.1200
Epoch [57/200], Step [76/76], Loss: 1.1188
Epoch [58/200], Step [76/76], Loss: 1.1183
Epoch [59/200], Step [76/76], Loss: 1.1237
Epoch [60/200], Step [76/76], Loss: 1.0568
Epoch [61/200], Step [76/76], Loss: 1.1229
Epoch [62/200], Step [76/76], Loss: 1.0558
Epoch [63/200], Step [76/76], Loss: 1.1223
Epoch [64/200], Step [76/76], Loss: 1.1216
Epoch [65/200], Step [76/76], Loss: 1.1220
Epoch [66/200], Step [76/76], Loss: 1.1207
Epoch [67/200], Step [76/76], Loss: 1.1198
Epoch [68/200], Step [76/76], Loss: 1.1218
Epoch [69/200], Step [76/76], Loss: 1.1196
Epoch [70/200], Step [76/76], Loss: 1.1193
Epoch [71/200], Step [76/76], Loss: 1.1180
Epoch [72/200], Step [76/76], Loss: 1.1229
Epoch [73/200], Step [76/76], Loss: 1.1194
Epoch [74/200], Step [76/76], Loss: 1.1177
Epoch [75/200], Step [76/76], Loss: 1.1225
Epoch [76/200], Step [76/76], Loss: 1.1180
Epoch [77/200], Step [76/76], Loss: 1.1223
Epoch [78/200], Step [76/76], Loss: 1.0579
Epoch [79/200], Step [76/76], Loss: 1.1188
Epoch [80/200], Step [76/76], Loss: 1.1173
Epoch [81/200], Step [76/76], Loss: 1.0573
Epoch [82/200], Step [76/76], Loss: 1.1192
Epoch [83/200], Step [76/76], Loss: 1.1176
Epoch [84/200], Step [76/76], Loss: 1.1167
Epoch [85/200], Step [76/76], Loss: 1.1159
Epoch [86/200], Step [76/76], Loss: 1.1249
Epoch [87/200], Step [76/76], Loss: 1.1156
Epoch [88/200], Step [76/76], Loss: 1.1239
Epoch [89/200], Step [76/76], Loss: 1.1233
Epoch [90/200], Step [76/76], Loss: 1.0584
Epoch [91/200], Step [76/76], Loss: 1.1221
Epoch [92/200], Step [76/76], Loss: 1.1208
Epoch [93/200], Step [76/76], Loss: 1.1201
Epoch [94/200], Step [76/76], Loss: 1.1193
Epoch [95/200], Step [76/76], Loss: 1.1207
Epoch [96/200], Step [76/76], Loss: 1.1198
Epoch [97/200], Step [76/76], Loss: 1.1180
Epoch [98/200], Step [76/76], Loss: 1.0581
Epoch [99/200], Step [76/76], Loss: 1.1215
Epoch [100/200], Step [76/76], Loss: 1.1194
Epoch [101/200], Step [76/76], Loss: 1.1223
Epoch [102/200], Step [76/76], Loss: 1.0572
Epoch [103/200], Step [76/76], Loss: 1.1208
Epoch [104/200], Step [76/76], Loss: 1.1205
Epoch [105/200], Step [76/76], Loss: 1.1205
Epoch [106/200], Step [76/76], Loss: 1.0565
Epoch [107/200], Step [76/76], Loss: 1.0544
Epoch [108/200], Step [76/76], Loss: 1.1215
Epoch [109/200], Step [76/76], Loss: 1.0533
Epoch [110/200], Step [76/76], Loss: 1.1239
Epoch [111/200], Step [76/76], Loss: 1.1214
Epoch [112/200], Step [76/76], Loss: 1.0532
Epoch [113/200], Step [76/76], Loss: 1.1211
Epoch [114/200], Step [76/76], Loss: 1.0524
Epoch [115/200], Step [76/76], Loss: 1.0525
Epoch [116/200], Step [76/76], Loss: 1.1248
Epoch [117/200], Step [76/76], Loss: 1.1238
Epoch [118/200], Step [76/76], Loss: 1.1229
Epoch [119/200], Step [76/76], Loss: 1.1217
Epoch [120/200], Step [76/76], Loss: 1.1197
Epoch [121/200], Step [76/76], Loss: 1.1235
Epoch [122/200], Step [76/76], Loss: 1.0540
Epoch [123/200], Step [76/76], Loss: 1.0528
Epoch [124/200], Step [76/76], Loss: 1.1232
Epoch [125/200], Step [76/76], Loss: 1.1218
Epoch [126/200], Step [76/76], Loss: 1.1204
Epoch [127/200], Step [76/76], Loss: 1.1196
Epoch [128/200], Step [76/76], Loss: 1.1183
Epoch [129/200], Step [76/76], Loss: 1.0539
Epoch [130/200], Step [76/76], Loss: 1.0524
Epoch [131/200], Step [76/76], Loss: 1.0513
Epoch [132/200], Step [76/76], Loss: 1.1209
Epoch [133/200], Step [76/76], Loss: 1.1192
Epoch [134/200], Step [76/76], Loss: 1.1192
Epoch [135/200], Step [76/76], Loss: 1.1171
Epoch [136/200], Step [76/76], Loss: 1.1274
Epoch [137/200], Step [76/76], Loss: 1.1177
Epoch [138/200], Step [76/76], Loss: 1.1256
Epoch [139/200], Step [76/76], Loss: 1.1178
Epoch [140/200], Step [76/76], Loss: 1.1253
Epoch [141/200], Step [76/76], Loss: 1.1235
Epoch [142/200], Step [76/76], Loss: 1.0549
Epoch [143/200], Step [76/76], Loss: 1.1190
Epoch [144/200], Step [76/76], Loss: 1.0544
Epoch [145/200], Step [76/76], Loss: 1.0532
Epoch [146/200], Step [76/76], Loss: 1.1197
Epoch [147/200], Step [76/76], Loss: 1.1186
Epoch [148/200], Step [76/76], Loss: 1.1253
Epoch [149/200], Step [76/76], Loss: 1.1248
Epoch [150/200], Step [76/76], Loss: 1.0537
Epoch [151/200], Step [76/76], Loss: 1.1234
Epoch [152/200], Step [76/76], Loss: 1.1212
Epoch [153/200], Step [76/76], Loss: 1.1242
Epoch [154/200], Step [76/76], Loss: 1.0531
Epoch [155/200], Step [76/76], Loss: 1.0527
Epoch [156/200], Step [76/76], Loss: 1.1212
Epoch [157/200], Step [76/76], Loss: 1.1203
Epoch [158/200], Step [76/76], Loss: 1.0525
Epoch [159/200], Step [76/76], Loss: 1.1265
Epoch [160/200], Step [76/76], Loss: 1.1251
Epoch [161/200], Step [76/76], Loss: 1.1210
Epoch [162/200], Step [76/76], Loss: 1.1239
Epoch [163/200], Step [76/76], Loss: 1.0542
Epoch [164/200], Step [76/76], Loss: 1.1238
Epoch [165/200], Step [76/76], Loss: 1.1223
Epoch [166/200], Step [76/76], Loss: 1.0525
Epoch [167/200], Step [76/76], Loss: 1.1230
Epoch [168/200], Step [76/76], Loss: 1.1221
Epoch [169/200], Step [76/76], Loss: 1.1213
Epoch [170/200], Step [76/76], Loss: 1.0524
Epoch [171/200], Step [76/76], Loss: 1.0509
Epoch [172/200], Step [76/76], Loss: 1.0497
Epoch [173/200], Step [76/76], Loss: 1.0489
Epoch [174/200], Step [76/76], Loss: 1.1245
Epoch [175/200], Step [76/76], Loss: 1.0485
Epoch [176/200], Step [76/76], Loss: 1.0473
Epoch [177/200], Step [76/76], Loss: 1.0469
Epoch [178/200], Step [76/76], Loss: 1.0471
Epoch [179/200], Step [76/76], Loss: 1.1246
Epoch [180/200], Step [76/76], Loss: 1.0463
Epoch [181/200], Step [76/76], Loss: 1.1243
Epoch [182/200], Step [76/76], Loss: 1.1239
Epoch [183/200], Step [76/76], Loss: 1.0476
Epoch [184/200], Step [76/76], Loss: 1.0461
Epoch [185/200], Step [76/76], Loss: 1.0448
Epoch [186/200], Step [76/76], Loss: 1.0444
Epoch [187/200], Step [76/76], Loss: 1.0440
Epoch [188/200], Step [76/76], Loss: 1.1258
Epoch [189/200], Step [76/76], Loss: 1.1243
Epoch [190/200], Step [76/76], Loss: 1.1220
Epoch [191/200], Step [76/76], Loss: 1.0476
Epoch [192/200], Step [76/76], Loss: 1.1290
Epoch [193/200], Step [76/76], Loss: 1.0468
Epoch [194/200], Step [76/76], Loss: 1.1238
Epoch [195/200], Step [76/76], Loss: 1.1230
Epoch [196/200], Step [76/76], Loss: 1.1207
Epoch [197/200], Step [76/76], Loss: 1.1196
Epoch [198/200], Step [76/76], Loss: 1.1277
Epoch [199/200], Step [76/76], Loss: 1.0506
Epoch [200/200], Step [76/76], Loss: 1.1192
Finished TrainingOptimConvNet2_ep200
training time:
20230214_134044
Epoch [1/200], Step [76/76], Loss: 1.1493
Epoch [2/200], Step [76/76], Loss: 1.0434
Epoch [3/200], Step [76/76], Loss: 1.0438
Epoch [4/200], Step [76/76], Loss: 1.1072
Epoch [5/200], Step [76/76], Loss: 1.1471
Epoch [6/200], Step [76/76], Loss: 1.1464
Epoch [7/200], Step [76/76], Loss: 1.1069
Epoch [8/200], Step [76/76], Loss: 1.1068
Epoch [9/200], Step [76/76], Loss: 1.1067
Epoch [10/200], Step [76/76], Loss: 1.1067
Epoch [11/200], Step [76/76], Loss: 1.1439
Epoch [12/200], Step [76/76], Loss: 1.1435
Epoch [13/200], Step [76/76], Loss: 1.0485
Epoch [14/200], Step [76/76], Loss: 1.1424
Epoch [15/200], Step [76/76], Loss: 1.1067
Epoch [16/200], Step [76/76], Loss: 1.1067
Epoch [17/200], Step [76/76], Loss: 1.1066
Epoch [18/200], Step [76/76], Loss: 1.1066
Epoch [19/200], Step [76/76], Loss: 1.1403
Epoch [20/200], Step [76/76], Loss: 1.1398
Epoch [21/200], Step [76/76], Loss: 1.0518
Epoch [22/200], Step [76/76], Loss: 1.1067
Epoch [23/200], Step [76/76], Loss: 1.1383
Epoch [24/200], Step [76/76], Loss: 1.0531
Epoch [25/200], Step [76/76], Loss: 1.1372
Epoch [26/200], Step [76/76], Loss: 1.0538
Epoch [27/200], Step [76/76], Loss: 1.1068
Epoch [28/200], Step [76/76], Loss: 1.1069
Epoch [29/200], Step [76/76], Loss: 1.1353
Epoch [30/200], Step [76/76], Loss: 1.1347
Epoch [31/200], Step [76/76], Loss: 1.1068
Epoch [32/200], Step [76/76], Loss: 1.1067
Epoch [33/200], Step [76/76], Loss: 1.0573
Epoch [34/200], Step [76/76], Loss: 1.1328
Epoch [35/200], Step [76/76], Loss: 1.1322
Epoch [36/200], Step [76/76], Loss: 1.0582
Epoch [37/200], Step [76/76], Loss: 1.0586
Epoch [38/200], Step [76/76], Loss: 1.1070
Epoch [39/200], Step [76/76], Loss: 1.1070
Epoch [40/200], Step [76/76], Loss: 1.1301
Epoch [41/200], Step [76/76], Loss: 1.1070
Epoch [42/200], Step [76/76], Loss: 1.1071
Epoch [43/200], Step [76/76], Loss: 1.1288
Epoch [44/200], Step [76/76], Loss: 1.1071
Epoch [45/200], Step [76/76], Loss: 1.0614
Epoch [46/200], Step [76/76], Loss: 1.0617
Epoch [47/200], Step [76/76], Loss: 1.1072
Epoch [48/200], Step [76/76], Loss: 1.0622
Epoch [49/200], Step [76/76], Loss: 1.1267
Epoch [50/200], Step [76/76], Loss: 1.1074
Epoch [51/200], Step [76/76], Loss: 1.1257
Epoch [52/200], Step [76/76], Loss: 1.0636
Epoch [53/200], Step [76/76], Loss: 1.1074
Epoch [54/200], Step [76/76], Loss: 1.1246
Epoch [55/200], Step [76/76], Loss: 1.1075
Epoch [56/200], Step [76/76], Loss: 1.0650
Epoch [57/200], Step [76/76], Loss: 1.1076
Epoch [58/200], Step [76/76], Loss: 1.1233
Epoch [59/200], Step [76/76], Loss: 1.0658
Epoch [60/200], Step [76/76], Loss: 1.1076
Epoch [61/200], Step [76/76], Loss: 1.1077
Epoch [62/200], Step [76/76], Loss: 1.1219
Epoch [63/200], Step [76/76], Loss: 1.1214
Epoch [64/200], Step [76/76], Loss: 1.0672
Epoch [65/200], Step [76/76], Loss: 1.1207
Epoch [66/200], Step [76/76], Loss: 1.0681
Epoch [67/200], Step [76/76], Loss: 1.1081
Epoch [68/200], Step [76/76], Loss: 1.1198
Epoch [69/200], Step [76/76], Loss: 1.1082
Epoch [70/200], Step [76/76], Loss: 1.0688
Epoch [71/200], Step [76/76], Loss: 1.1083
Epoch [72/200], Step [76/76], Loss: 1.1184
Epoch [73/200], Step [76/76], Loss: 1.1180
Epoch [74/200], Step [76/76], Loss: 1.1175
Epoch [75/200], Step [76/76], Loss: 1.1170
Epoch [76/200], Step [76/76], Loss: 1.0707
Epoch [77/200], Step [76/76], Loss: 1.0711
Epoch [78/200], Step [76/76], Loss: 1.1088
Epoch [79/200], Step [76/76], Loss: 1.1087
Epoch [80/200], Step [76/76], Loss: 1.1157
Epoch [81/200], Step [76/76], Loss: 1.0723
Epoch [82/200], Step [76/76], Loss: 1.1150
OptimConvNet2_ep200
training time:
20230214_134119
Epoch [1/200], Step [76/76], Loss: 1.1251
Epoch [2/200], Step [76/76], Loss: 1.0472
Epoch [3/200], Step [76/76], Loss: 1.0477
Epoch [4/200], Step [76/76], Loss: 1.0482
Epoch [5/200], Step [76/76], Loss: 1.1247
Epoch [6/200], Step [76/76], Loss: 1.1244
Epoch [7/200], Step [76/76], Loss: 1.0497
Epoch [8/200], Step [76/76], Loss: 1.1236
Epoch [9/200], Step [76/76], Loss: 1.0506
Epoch [10/200], Step [76/76], Loss: 1.0509
Epoch [11/200], Step [76/76], Loss: 1.0514
Epoch [12/200], Step [76/76], Loss: 1.1215
Epoch [13/200], Step [76/76], Loss: 1.0523
Epoch [14/200], Step [76/76], Loss: 1.0526
Epoch [15/200], Step [76/76], Loss: 1.1245
Epoch [16/200], Step [76/76], Loss: 1.0535
Epoch [17/200], Step [76/76], Loss: 1.1195
Epoch [18/200], Step [76/76], Loss: 1.0543
Epoch [19/200], Step [76/76], Loss: 1.1184
Epoch [20/200], Step [76/76], Loss: 1.1177
Epoch [21/200], Step [76/76], Loss: 1.0556
Epoch [22/200], Step [76/76], Loss: 1.1248
Epoch [23/200], Step [76/76], Loss: 1.1246
Epoch [24/200], Step [76/76], Loss: 1.1242
Epoch [25/200], Step [76/76], Loss: 1.1157
Epoch [26/200], Step [76/76], Loss: 1.1150
Epoch [27/200], Step [76/76], Loss: 1.0583
Epoch [28/200], Step [76/76], Loss: 1.1243
Epoch [29/200], Step [76/76], Loss: 1.1240
Epoch [30/200], Step [76/76], Loss: 1.0598
Epoch [31/200], Step [76/76], Loss: 1.1238
Epoch [32/200], Step [76/76], Loss: 1.0607
Epoch [33/200], Step [76/76], Loss: 1.1123
Epoch [34/200], Step [76/76], Loss: 1.0616
Epoch [35/200], Step [76/76], Loss: 1.1237
Epoch [36/200], Step [76/76], Loss: 1.0623
Epoch [37/200], Step [76/76], Loss: 1.0626
Epoch [38/200], Step [76/76], Loss: 1.0629
Epoch [39/200], Step [76/76], Loss: 1.1238
Epoch [40/200], Step [76/76], Loss: 1.1097
Epoch [41/200], Step [76/76], Loss: 1.1237
Epoch [42/200], Step [76/76], Loss: 1.0645
Epoch [43/200], Step [76/76], Loss: 1.1084
Epoch [44/200], Step [76/76], Loss: 1.0652
Epoch [45/200], Step [76/76], Loss: 1.1075
Epoch [46/200], Step [76/76], Loss: 1.0659
Epoch [47/200], Step [76/76], Loss: 1.1239
Epoch [48/200], Step [76/76], Loss: 1.1238
Epoch [49/200], Step [76/76], Loss: 1.1060
Epoch [50/200], Step [76/76], Loss: 1.1056
Epoch [51/200], Step [76/76], Loss: 1.1050
Epoch [52/200], Step [76/76], Loss: 1.1239
Epoch [53/200], Step [76/76], Loss: 1.1043
Epoch [54/200], Step [76/76], Loss: 1.1041
Epoch [55/200], Step [76/76], Loss: 1.1034
Epoch [56/200], Step [76/76], Loss: 1.1241
Epoch [57/200], Step [76/76], Loss: 1.1237
Epoch [58/200], Step [76/76], Loss: 1.1234
Epoch [59/200], Step [76/76], Loss: 1.1023
Epoch [60/200], Step [76/76], Loss: 1.0713
Epoch [61/200], Step [76/76], Loss: 1.0715
Epoch [62/200], Step [76/76], Loss: 1.0719
Epoch [63/200], Step [76/76], Loss: 1.1236
Epoch [64/200], Step [76/76], Loss: 1.0724
Epoch [65/200], Step [76/76], Loss: 1.1004
Epoch [66/200], Step [76/76], Loss: 1.0728
Epoch [67/200], Step [76/76], Loss: 1.0731
Epoch [68/200], Step [76/76], Loss: 1.1238
Epoch [69/200], Step [76/76], Loss: 1.1235
Epoch [70/200], Step [76/76], Loss: 1.0990
Epoch [71/200], Step [76/76], Loss: 1.0744
Epoch [72/200], Step [76/76], Loss: 1.1235
Epoch [73/200], Step [76/76], Loss: 1.0750
Epoch [74/200], Step [76/76], Loss: 1.0979
Epoch [75/200], Step [76/76], Loss: 1.0975
Epoch [76/200], Step [76/76], Loss: 1.0758
Epoch [77/200], Step [76/76], Loss: 1.0762
Epoch [78/200], Step [76/76], Loss: 1.0965
Epoch [79/200], Step [76/76], Loss: 1.1238
Epoch [80/200], Step [76/76], Loss: 1.1236
Epoch [81/200], Step [76/76], Loss: 1.1235
Epoch [82/200], Step [76/76], Loss: 1.0774
Epoch [83/200], Step [76/76], Loss: 1.0954
Epoch [84/200], Step [76/76], Loss: 1.0782
Epoch [85/200], Step [76/76], Loss: 1.0946
Epoch [86/200], Step [76/76], Loss: 1.1234
Epoch [87/200], Step [76/76], Loss: 1.1232
Epoch [88/200], Step [76/76], Loss: 1.0793
Epoch [89/200], Step [76/76], Loss: 1.0794
Epoch [90/200], Step [76/76], Loss: 1.0935
Epoch [91/200], Step [76/76], Loss: 1.0931
Epoch [92/200], Step [76/76], Loss: 1.0801
Epoch [93/200], Step [76/76], Loss: 1.0923
Epoch [94/200], Step [76/76], Loss: 1.1236
Epoch [95/200], Step [76/76], Loss: 1.1236
Epoch [96/200], Step [76/76], Loss: 1.0919
Epoch [97/200], Step [76/76], Loss: 1.1232
Epoch [98/200], Step [76/76], Loss: 1.0911
Epoch [99/200], Step [76/76], Loss: 1.1231
Epoch [100/200], Step [76/76], Loss: 1.1230
Epoch [101/200], Step [76/76], Loss: 1.0829
Epoch [102/200], Step [76/76], Loss: 1.1228
Epoch [103/200], Step [76/76], Loss: 1.1227
Epoch [104/200], Step [76/76], Loss: 1.0839
Epoch [105/200], Step [76/76], Loss: 1.0900
Epoch [106/200], Step [76/76], Loss: 1.1228
Epoch [107/200], Step [76/76], Loss: 1.1225
Epoch [108/200], Step [76/76], Loss: 1.0891
Epoch [109/200], Step [76/76], Loss: 1.1224
Epoch [110/200], Step [76/76], Loss: 1.0886
Epoch [111/200], Step [76/76], Loss: 1.0882
Epoch [112/200], Step [76/76], Loss: 1.0861
Epoch [113/200], Step [76/76], Loss: 1.0856
Epoch [114/200], Step [76/76], Loss: 1.0861
Epoch [115/200], Step [76/76], Loss: 1.0873
Epoch [116/200], Step [76/76], Loss: 1.0861
Epoch [117/200], Step [76/76], Loss: 1.0866
Epoch [118/200], Step [76/76], Loss: 1.1231
Epoch [119/200], Step [76/76], Loss: 1.0870
Epoch [120/200], Step [76/76], Loss: 1.1231
Epoch [121/200], Step [76/76], Loss: 1.0857
Epoch [122/200], Step [76/76], Loss: 1.0853
Epoch [123/200], Step [76/76], Loss: 1.0881
Epoch [124/200], Step [76/76], Loss: 1.1232
Epoch [125/200], Step [76/76], Loss: 1.0886
Epoch [126/200], Step [76/76], Loss: 1.0887
Epoch [127/200], Step [76/76], Loss: 1.1233
Epoch [128/200], Step [76/76], Loss: 1.1231
Epoch [129/200], Step [76/76], Loss: 1.0839
Epoch [130/200], Step [76/76], Loss: 1.1229
Epoch [131/200], Step [76/76], Loss: 1.1228
Epoch [132/200], Step [76/76], Loss: 1.0902
Epoch [133/200], Step [76/76], Loss: 1.0831
Epoch [134/200], Step [76/76], Loss: 1.1229
Epoch [135/200], Step [76/76], Loss: 1.1227
Epoch [136/200], Step [76/76], Loss: 1.0826
Epoch [137/200], Step [76/76], Loss: 1.0912
Epoch [138/200], Step [76/76], Loss: 1.0912
Epoch [139/200], Step [76/76], Loss: 1.1230
Epoch [140/200], Step [76/76], Loss: 1.0817
Epoch [141/200], Step [76/76], Loss: 1.0813
Epoch [142/200], Step [76/76], Loss: 1.1229
Epoch [143/200], Step [76/76], Loss: 1.0925
Epoch [144/200], Step [76/76], Loss: 1.0925
Epoch [145/200], Step [76/76], Loss: 1.1231
Epoch [146/200], Step [76/76], Loss: 1.0930
Epoch [147/200], Step [76/76], Loss: 1.0802
Epoch [148/200], Step [76/76], Loss: 1.1230
Epoch [149/200], Step [76/76], Loss: 1.0931
Epoch [150/200], Step [76/76], Loss: 1.0798
Epoch [151/200], Step [76/76], Loss: 1.0793
Epoch [152/200], Step [76/76], Loss: 1.0942
Epoch [153/200], Step [76/76], Loss: 1.1235
Epoch [154/200], Step [76/76], Loss: 1.0943
Epoch [155/200], Step [76/76], Loss: 1.1232
Epoch [156/200], Step [76/76], Loss: 1.0946
Epoch [157/200], Step [76/76], Loss: 1.0946
Epoch [158/200], Step [76/76], Loss: 1.0947
Epoch [159/200], Step [76/76], Loss: 1.0946
Epoch [160/200], Step [76/76], Loss: 1.0780
Epoch [161/200], Step [76/76], Loss: 1.0950
Epoch [162/200], Step [76/76], Loss: 1.0774
Epoch [163/200], Step [76/76], Loss: 1.1238
Epoch [164/200], Step [76/76], Loss: 1.0771
Epoch [165/200], Step [76/76], Loss: 1.0957
Epoch [166/200], Step [76/76], Loss: 1.0956
Epoch [167/200], Step [76/76], Loss: 1.0961
Epoch [168/200], Step [76/76], Loss: 1.1239
Epoch [169/200], Step [76/76], Loss: 1.1238
Epoch [170/200], Step [76/76], Loss: 1.0965
Epoch [171/200], Step [76/76], Loss: 1.1236
Epoch [172/200], Step [76/76], Loss: 1.1233
Epoch [173/200], Step [76/76], Loss: 1.1231
Epoch [174/200], Step [76/76], Loss: 1.0974
Epoch [175/200], Step [76/76], Loss: 1.0758
Epoch [176/200], Step [76/76], Loss: 1.0974
Epoch [177/200], Step [76/76], Loss: 1.0976
Epoch [178/200], Step [76/76], Loss: 1.0752
Epoch [179/200], Step [76/76], Loss: 1.0980
Epoch [180/200], Step [76/76], Loss: 1.0747
Epoch [181/200], Step [76/76], Loss: 1.1238
Epoch [182/200], Step [76/76], Loss: 1.0981
Epoch [183/200], Step [76/76], Loss: 1.1236
Epoch [184/200], Step [76/76], Loss: 1.1234
Epoch [185/200], Step [76/76], Loss: 1.0992
Epoch [186/200], Step [76/76], Loss: 1.0739
Epoch [187/200], Step [76/76], Loss: 1.1233
Epoch [188/200], Step [76/76], Loss: 1.1231
Epoch [189/200], Step [76/76], Loss: 1.1230
Epoch [190/200], Step [76/76], Loss: 1.0733
Epoch [191/200], Step [76/76], Loss: 1.0730
Epoch [192/200], Step [76/76], Loss: 1.1004
Epoch [193/200], Step [76/76], Loss: 1.1230
Epoch [194/200], Step [76/76], Loss: 1.1228
Epoch [195/200], Step [76/76], Loss: 1.1227
Epoch [196/200], Step [76/76], Loss: 1.1012
Epoch [197/200], Step [76/76], Loss: 1.1010
Epoch [198/200], Step [76/76], Loss: 1.0723
Epoch [199/200], Step [76/76], Loss: 1.1231
Epoch [200/200], Step [76/76], Loss: 1.1015
Finished Training